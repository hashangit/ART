<!DOCTYPE html>
<html lang="en" class="scroll-smooth">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ART Framework: Comprehensive Developer Guide</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-light.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/typescript.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/javascript.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/json.min.js"></script>

    <style>
        /* Custom Styles */
        body {
            font-family: 'Inter', sans-serif;
            background-color: #f8fafc; /* Tailwind gray-50 */
            color: #374151; /* Tailwind gray-700 */
        }

        /* Style code blocks */
        pre code.hljs {
            display: block;
            overflow-x: auto;
            padding: 1.5em;
            border-radius: 0.5rem;
            background: #f3f4f6; /* Tailwind gray-100 */
            color: #1f2937; /* Tailwind gray-800 */
            font-size: 0.9em;
            line-height: 1.6;
        }
        /* Inline code */
        code:not(pre > code) {
             background-color: #e5e7eb; /* Tailwind gray-200 */
             padding: 0.1em 0.3em;
             border-radius: 0.25rem;
             font-size: 0.9em;
        }

        /* Ensure mermaid diagram is centered and responsive */
        .mermaid {
            display: block;
            margin: 2rem auto; /* Increased margin */
            max-width: 100%;
            text-align: center;
        }
        .mermaid svg {
             max-width: 100%;
             height: auto;
        }

        /* Sidebar active link style */
        .sidebar-link.active {
            background-color: #e0f2fe; /* Tailwind sky-100 */
            color: #0c4a6e; /* Tailwind sky-800 */
            font-weight: 600;
        }

        /* Smooth scroll */
        html {
            scroll-behavior: smooth;
        }

        /* Simple fade-in animation */
        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(10px); }
            to { opacity: 1; transform: translateY(0); }
        }
        .fade-in-section {
            animation: fadeIn 0.6s ease-out forwards;
        }

        /* Details/Summary styling for Developer Notes */
        details {
            background-color: #f9fafb; /* Tailwind gray-50 */
            border: 1px solid #e5e7eb; /* Tailwind gray-200 */
            border-radius: 0.375rem; /* rounded-md */
            padding: 0.75rem 1rem; /* px-4 py-3 */
            margin-top: 0.5rem; /* mt-2 */
        }
        summary {
            font-weight: 500; /* medium */
            color: #4b5563; /* gray-600 */
            cursor: pointer;
            list-style-position: inside; /* Keeps marker aligned */
        }
         details[open] > summary {
             margin-bottom: 0.5rem; /* mb-2 */
         }
        details > *:not(summary) {
            font-size: 0.9em;
            color: #4b5563; /* gray-600 */
            line-height: 1.6;
        }
        details ul {
            margin-top: 0.5rem;
            padding-left: 1rem;
        }
         details ul li {
             margin-bottom: 0.25rem;
         }

        /* Responsive adjustments */
        @media (max-width: 768px) {
            .sidebar {
                position: static;
                width: 100%;
                height: auto;
                margin-bottom: 1rem;
                border-right: none;
                border-bottom: 1px solid #e5e7eb;
            }
            .main-content {
                margin-left: 0;
                padding-top: 1rem;
            }
            .sidebar ul {
                display: flex;
                flex-wrap: wrap;
                justify-content: center;
            }
             .sidebar li {
                margin-right: 0.5rem;
                margin-bottom: 0.5rem;
            }
        }

    </style>
</head>
<body class="text-gray-800">

    <div class="flex flex-col md:flex-row min-h-screen">
        <aside class="sidebar w-full md:w-64 lg:w-72 bg-white border-r border-gray-200 p-4 md:p-6 md:sticky md:top-0 md:h-screen overflow-y-auto">
            <h1 class="text-xl lg:text-2xl font-bold text-sky-700 mb-6">ART Framework</h1>
            <nav>
                <ul class="space-y-2" id="sidebar-nav">
                    <li>
                        <a href="#introduction" class="sidebar-link block px-4 py-2 rounded-md text-gray-700 hover:bg-gray-100 transition duration-150">1. Introduction</a>
                        <ul class="ml-4 mt-1 space-y-1 text-sm">
                            <li><a href="#usage-complexity" class="block px-3 py-1 rounded text-gray-600 hover:bg-gray-100 transition duration-150">1.1 Usage Complexity</a></li>
                        </ul>
                    </li>
                    <li>
                        <a href="#architecture" class="sidebar-link block px-4 py-2 rounded-md text-gray-700 hover:bg-gray-100 transition duration-150">2. Architecture</a>
                         <ul class="ml-4 mt-1 space-y-1 text-sm">
                            <li><a href="#streaming-architecture" class="block px-3 py-1 rounded text-gray-600 hover:bg-gray-100 transition duration-150">2.1 Streaming Architecture</a></li>
                            <li><a href="#prompt-architecture" class="block px-3 py-1 rounded text-gray-600 hover:bg-gray-100 transition duration-150">2.2 Prompt Architecture</a></li>
                            <li><a href="#state-management-architecture" class="block px-3 py-1 rounded text-gray-600 hover:bg-gray-100 transition duration-150">2.3 State Management</a></li>
                            <li><a href="#provider-manager-architecture" class="block px-3 py-1 rounded text-gray-600 hover:bg-gray-100 transition duration-150">2.4 Provider Manager</a></li>
                        </ul>
                    </li>
                    <li>
                        <a href="#scenario-1" class="sidebar-link block px-4 py-2 rounded-md text-gray-700 hover:bg-gray-100 transition duration-150">3. Scenario 1: React Chatbot</a>
                        <ul class="ml-4 mt-1 space-y-1 text-sm">
                            <li><a href="#scenario-1-imports" class="block px-3 py-1 rounded text-gray-600 hover:bg-gray-100 transition duration-150">3.1 Imports</a></li>
                            <li><a href="#scenario-1-component" class="block px-3 py-1 rounded text-gray-600 hover:bg-gray-100 transition duration-150">3.2 Component</a></li>
                             <li><a href="#scenario-1-workflow" class="block px-3 py-1 rounded text-gray-600 hover:bg-gray-100 transition duration-150">3.3 Internal Workflow</a></li>
                        </ul>
                    </li>
                    <li>
                        <a href="#scenario-2" class="sidebar-link block px-4 py-2 rounded-md text-gray-700 hover:bg-gray-100 transition duration-150">4. Scenario 2: Custom Tool</a>
                         <ul class="ml-4 mt-1 space-y-1 text-sm">
                            <li><a href="#scenario-2-simple-explanation" class="block px-3 py-1 rounded text-gray-600 hover:bg-gray-100 transition duration-150">4.0 Simplified Explanation</a></li>
                            <li><a href="#scenario-2-imports" class="block px-3 py-1 rounded text-gray-600 hover:bg-gray-100 transition duration-150">4.1 Imports</a></li>
                            <li><a href="#scenario-2-implementation" class="block px-3 py-1 rounded text-gray-600 hover:bg-gray-100 transition duration-150">4.2 Implementation</a></li>
                            <li><a href="#scenario-2-integration" class="block px-3 py-1 rounded text-gray-600 hover:bg-gray-100 transition duration-150">4.3 Integration</a></li>
                        </ul>
                    </li>
                     <li>
                        <a href="#scenario-3" class="sidebar-link block px-4 py-2 rounded-md text-gray-700 hover:bg-gray-100 transition duration-150">5. Scenario 3: Custom Provider</a>
                         <ul class="ml-4 mt-1 space-y-1 text-sm">
                            <li><a href="#scenario-3-simple-explanation" class="block px-3 py-1 rounded text-gray-600 hover:bg-gray-100 transition duration-150">5.0 Simplified Explanation</a></li>
                            <li><a href="#scenario-3-imports" class="block px-3 py-1 rounded text-gray-600 hover:bg-gray-100 transition duration-150">5.1 Imports</a></li>
                            <li><a href="#scenario-3-implementation" class="block px-3 py-1 rounded text-gray-600 hover:bg-gray-100 transition duration-150">5.2 Implementation</a></li>
                            <li><a href="#scenario-3-integration" class="block px-3 py-1 rounded text-gray-600 hover:bg-gray-100 transition duration-150">5.3 Integration</a></li>
                        </ul>
                    </li>
                     <li>
                        <a href="#scenario-4" class="sidebar-link block px-4 py-2 rounded-md text-gray-700 hover:bg-gray-100 transition duration-150">6. Scenario 4: Custom Storage</a>
                         <ul class="ml-4 mt-1 space-y-1 text-sm">
                            <li><a href="#scenario-4-simple-explanation" class="block px-3 py-1 rounded text-gray-600 hover:bg-gray-100 transition duration-150">6.0 Simplified Explanation</a></li>
                            <li><a href="#scenario-4-imports" class="block px-3 py-1 rounded text-gray-600 hover:bg-gray-100 transition duration-150">6.1 Imports</a></li>
                            <li><a href="#scenario-4-implementation" class="block px-3 py-1 rounded text-gray-600 hover:bg-gray-100 transition duration-150">6.2 Implementation</a></li>
                            <li><a href="#scenario-4-integration" class="block px-3 py-1 rounded text-gray-600 hover:bg-gray-100 transition duration-150">6.3 Integration</a></li>
                        </ul>
                    </li>
                    <li>
                        <a href="#scenario-5" class="sidebar-link block px-4 py-2 rounded-md text-gray-700 hover:bg-gray-100 transition duration-150">7. Scenario 5: Custom Agent</a>
                         <ul class="ml-4 mt-1 space-y-1 text-sm">
                            <li><a href="#scenario-5-imports" class="block px-3 py-1 rounded text-gray-600 hover:bg-gray-100 transition duration-150">7.1 Imports</a></li>
                            <li><a href="#scenario-5-implementation" class="block px-3 py-1 rounded text-gray-600 hover:bg-gray-100 transition duration-150">7.2 Implementation</a></li>
                            <li><a href="#scenario-5-integration" class="block px-3 py-1 rounded text-gray-600 hover:bg-gray-100 transition duration-150">7.3 Integration</a></li>
                        </ul>
                    </li>
                    <li><a href="#conclusion" class="sidebar-link block px-4 py-2 rounded-md text-gray-700 hover:bg-gray-100 transition duration-150">8. Conclusion</a></li>
                </ul>
            </nav>
        </aside>

        <main class="main-content flex-1 p-6 md:p-10 lg:p-12 overflow-y-auto">

            <section id="introduction" class="mb-16 p-6 bg-white rounded-lg shadow-lg fade-in-section">
                <h2 class="text-2xl md:text-3xl font-semibold mb-6 border-b pb-3 text-sky-700">1. Introduction: What is ART?</h2>
                <p class="mb-5 text-gray-700 leading-relaxed">
                    ART (Agent Reasoning & Tooling) is a JavaScript/TypeScript framework designed specifically for building intelligent, AI-powered agents that run <strong>directly in the user's web browser</strong>. Think of it as a toolkit that helps you create applications like sophisticated chatbots, research assistants, or automated helpers without needing a separate server for the core AI logic.
                </p>
                <h3 class="text-xl font-medium mb-3 text-gray-800">Core Goals:</h3>
                <ul class="list-disc list-inside mb-5 space-y-2 text-gray-700 leading-relaxed">
                    <li><strong>Client-Side First:</strong> Runs entirely in the browser, making web-native AI apps possible.</li>
                    <li><strong>Modular:</strong> Built like LEGO bricks – different parts (like memory, reasoning engine, tools) can be swapped or added.</li>
                    <li><strong>Flexible:</strong> Adaptable to different AI models, tools, and agent behaviors.</li>
                    <li><strong>Decoupled:</strong> Components work together through defined contracts (interfaces), not direct dependencies, making the system easier to manage and extend.</li>
                </ul>
                <h3 class="text-xl font-medium mb-3 text-gray-800">Who is this guide for?</h3>
                <p class="mb-5 text-gray-700 leading-relaxed">
                    This guide is for web developers who want to build applications using Large Language Models (LLMs) directly in the browser. We'll cover everything from basic setup to advanced customization, using both technical terms and simpler explanations.
                </p>

                 <div id="usage-complexity" class="mt-10">
                     <h3 class="text-xl font-medium mb-4 text-gray-800">1.1. Usage Complexity Levels</h3>
                    <p class="mb-4 text-gray-700 leading-relaxed">Developers can engage with ART at different levels of complexity, depending on their needs:</p>
                    <ul class="list-disc list-inside space-y-4 text-gray-700 leading-relaxed">
                        <li>
                            <strong>Level 1: Simple Usage (Using Built-ins)</strong>
                            <ul class="list-circle list-inside ml-6 space-y-1 text-sm mt-1">
                                <li><strong>Focus:</strong> Configuration.</li>
                                <li><strong>Activities:</strong> Select from ART's pre-built adapters (storage, reasoning), use the default agent pattern (<code>PESAgent</code>), and potentially include built-in tools. Initialize via <code>createArtInstance</code> and use <code>art.process()</code>.</li>
                                <li><strong>Goal:</strong> Quickly set up a functional agent using standard components.</li>
                            </ul>
                        </li>
                        <li>
                            <strong>Level 2: Intermediate Usage (Extending with Custom Tools/Adapters)</strong>
                             <ul class="list-circle list-inside ml-6 space-y-1 text-sm mt-1">
                                <li><strong>Focus:</strong> Extension & Integration.</li>
                                <li><strong>Activities:</strong> Includes Simple Usage activities, PLUS implementing custom <code>IToolExecutor</code> interfaces to add specific capabilities (e.g., interacting with your backend, using specific libraries) or custom <code>ProviderAdapter</code>/<code>StorageAdapter</code> interfaces for unsupported services/storage.</li>
                                <li><strong>Goal:</strong> Tailor the agent's capabilities and integrations while leveraging the core framework's orchestration.</li>
                            </ul>
                        </li>
                        <li>
                            <strong>Level 3: Advanced Usage (Implementing Custom Agent Patterns)</strong>
                             <ul class="list-circle list-inside ml-6 space-y-1 text-sm mt-1">
                                <li><strong>Focus:</strong> Core Logic Customization.</li>
                                <li><strong>Activities:</strong> Includes Intermediate Usage activities, PLUS implementing a custom <code>IAgentCore</code> interface. This involves defining a completely new reasoning loop or modifying an existing one significantly, including logic for determining the `RuntimeProviderConfig` for each LLM call. Requires a deep understanding of how all internal ART components interact.</li>
                                <li><strong>Goal:</strong> Gain maximum control over the agent's behavior, reasoning process, and dynamic LLM selection logic.</li>
                            </ul>
                        </li>
                    </ul>
                </div>
            </section>

            <section id="architecture" class="mb-16 p-6 bg-white rounded-lg shadow-lg fade-in-section">
                <h2 class="text-2xl md:text-3xl font-semibold mb-6 border-b pb-3 text-sky-700">2. Understanding ART's Architecture: The 3 Nodes</h2>
                <p class="mb-5 text-gray-700 leading-relaxed">
                    Imagine ART as having three main layers or "nodes" that work together:
                </p>
                <div class="mermaid mb-8">
flowchart LR
    A["Node 1: Developer Interface\n(Your Code & Config)"] --> B["Node 2: ART Core Orchestration\n(The Framework's Brain)"]
    B --> C["Node 3: External Dependencies & Interactions\n(The Outside World)"]
    C --> B
                </div>

                <div class="space-y-8">
                    <div>
                        <h3 class="text-xl font-medium mb-3 text-gray-800">Node 1: Developer Interface (Your Code & Config)</h3>
                        <ul class="list-disc list-inside space-y-2 text-gray-700 leading-relaxed">
                            <li><strong>What it is:</strong> This is where you, the developer, interact with ART. You write the code to set up, configure, and control the agent.</li>
                            <li><strong>What you do here:</strong> Choose which AI model to use (like GPT-4 or Gemini), decide how the agent should remember things (in memory or browser storage), select which tools it can use, pick the agent's thinking style (its "pattern"), and tell the agent when to start processing a user's request.</li>
                            <li><strong>Key ART parts involved:</strong> <code>createArtInstance</code> (the function to start ART), configuration objects, <code>ArtInstance</code> (the main object you interact with), <code>art.process()</code> (the command to make the agent think).</li>
                        </ul>
                    </div>
                    <div>
                        <h3 class="text-xl font-medium mb-3 text-gray-800">Node 2: ART Core Orchestration (The Framework's Brain)</h3>
                         <ul class="list-disc list-inside space-y-2 text-gray-700 leading-relaxed">
                            <li><strong>What it is:</strong> This is the internal engine of ART, set up based on your configuration in Node 1. It manages the entire process of understanding a request, using tools, and generating a response.</li>
                            <li><strong>What it does:</strong> Follows the chosen agent pattern (like "Plan-Execute-Synthesize" or "ReAct"), manages conversation history, keeps track of the agent's state, prepares instructions (prompts) for the AI model using blueprints and context, understands the AI's responses (including streaming tokens), coordinates tool usage, logs important events (observations), and broadcasts real-time updates to the UI.</li>
                            <li><strong>Key ART parts involved:</strong> The specific Agent Core implementation (`PESAgent`, `ReActAgent`), Managers (`StateManager`, `ConversationManager`, `ObservationManager`), Systems (`ToolSystem`, `UISystem`), Reasoning Components (`ReasoningEngine`, `PromptManager`, `OutputParser`). You usually don't interact with these directly after setup unless you're doing advanced customization or consuming UI sockets.</li>
                        </ul>
                    </div>
                    <div>
                        <h3 class="text-xl font-medium mb-3 text-gray-800">Node 3: External Dependencies & Interactions (The Outside World)</h3>
                         <ul class="list-disc list-inside space-y-2 text-gray-700 leading-relaxed">
                            <li><strong>What it is:</strong> This node represents where the ART engine connects to resources outside its core orchestration logic. These are the pluggable pieces configured in Node 1, whose instances are often managed by Node 2.</li>
                            <li><strong>What it does:</strong> Makes the actual calls to the LLM provider APIs or local services (using adapter instances provided by the `ProviderManager`), executes tool logic (which might involve calling other web services or using browser features), and persists/retrieves data from the chosen storage mechanism (using the configured `StorageAdapter`).</li>
                            <li><strong>Key Elements (Interfaces & Implementations):** Adapters (`ProviderAdapter` for LLMs, `StorageAdapter` for memory/storage), Tool Implementations (`IToolExecutor`). The actual `ProviderAdapter` instances are created and managed (pooled, cached, evicted) by the `ProviderManager` in Node 2.</li>
                        </ul>
                    </div>
                </div>

                 <div id="streaming-architecture" class="mt-10">
                    <h3 class="text-xl font-medium mb-4 text-gray-800">2.1. Core Concept: Real-time Streaming Architecture</h3>
                    <p class="mb-4 text-gray-700 leading-relaxed">To provide a more responsive and interactive user experience, ART incorporates a real-time streaming architecture for handling LLM responses. Instead of waiting for the entire response, the UI can receive and display tokens as soon as the LLM generates them.</p>

                    <h4 class="text-lg font-medium mt-6 mb-3 text-gray-800">Key Components:</h4>
                    <ul class="list-disc list-inside space-y-3 mb-6 text-gray-700 leading-relaxed">
                        <li>
                            <strong><code>ReasoningEngine.call</code> returning <code>AsyncIterable&lt;StreamEvent&gt;</code>:</strong>
                            <ul class="list-circle list-inside ml-6 space-y-1 text-sm mt-1">
                                <li><strong>Non-Developer Explanation:</strong> Instead of the agent's "brain" waiting for the LLM's complete answer, it now gets a "conveyor belt" (<code>AsyncIterable</code>). Pieces of the answer (<code>StreamEvent</code> objects) arrive on the belt one by one as the LLM thinks and writes.</li>
                                <li><strong>Developer Notes:</strong> The core <code>ReasoningEngine</code> interface's <code>call</code> method now returns a <code>Promise</code> resolving to an <code>AsyncIterable</code>. This iterable yields <code>StreamEvent</code> objects, allowing the consuming code (typically the <code>AgentCore</code>) to process tokens, metadata, errors, and end signals asynchronously as they arrive from the <code>ProviderAdapter</code>.</li>
                            </ul>
                        </li>
                         <li>
                            <strong><code>StreamEvent</code> Interface:</strong>
                            <ul class="list-circle list-inside ml-6 space-y-1 text-sm mt-1">
                                <li><strong>Non-Developer Explanation:</strong> Each item on the conveyor belt has a label (<code>StreamEvent</code>) saying what it is: a piece of text (<code>TOKEN</code>), statistics (<code>METADATA</code>), an error (<code>ERROR</code>), or the end signal (<code>END</code>). Text tokens also have a sub-label (<code>tokenType</code>) indicating if it's part of the LLM's internal "thinking" process or the final "response".</li>
                                <li><strong>Developer Notes:</strong> This interface (detailed below) standardizes the data flowing from the LLM stream. The <code>type</code> field is crucial for routing, and the <code>tokenType</code> field enables differentiating intermediate reasoning steps from the final output meant for the user. Adapters are responsible for correctly populating these fields based on provider-specific stream formats and the <code>callContext</code> option.</li>
                            </ul>
                        </li>
                         <li>
                            <strong><code>LLMStreamSocket</code> (<code>UISystem</code>):</strong>
                            <ul class="list-circle list-inside ml-6 space-y-1 text-sm mt-1">
                                <li><strong>Non-Developer Explanation:</strong> ART uses an announcement system (sockets) for different parts to communicate, especially with the UI. A new, dedicated channel (<code>LLMStreamSocket</code>) was added specifically for broadcasting the live stream events (tokens, metadata, errors, end signals) from the LLM conveyor belt to any UI components listening.</li>
                                <li><strong>Developer Notes:</strong> Accessed via <code>artInstance.uiSystem.getLLMStreamSocket()</code>. The <code>AgentCore</code> consumes the <code>AsyncIterable</code> from the <code>ReasoningEngine</code> and pushes each <code>StreamEvent</code> to this socket. UI components subscribe to this socket to receive real-time updates, decoupling the UI from the stream source and providing a consistent subscription pattern (<code>socket.subscribe(...)</code>).</li>
                            </ul>
                        </li>
                    </ul>

                     <h4 class="text-lg font-medium mt-6 mb-3 text-gray-800">Flow Overview:</h4>
                    <ol class="list-decimal list-inside space-y-3 mb-6 text-gray-700 leading-relaxed">
                        <li><strong>UI/App:</strong> Calls <code>artInstance.process(props)</code>.</li>
                        <li><strong>Agent Core (<code>PESAgent</code>, etc.):** Determines `RuntimeProviderConfig`, creates `CallOptions`, calls `reasoningEngine.call(prompt, callOptions)`.</li>
                        <li><strong>Reasoning Engine:** Gets managed adapter instance from `ProviderManager` based on `callOptions.providerConfig`. Calls adapter's `call` method.</li>
                        <li><strong>Adapter (e.g., `OpenAIAdapter`):** Makes a streaming request to the LLM provider API. Receives stream chunks, parses them, determines `tokenType`, and `yield`s `StreamEvent` objects via the `AsyncIterable`.</li>
                        <li><strong>Reasoning Engine:** Returns the `AsyncIterable` (wrapped in a releasing generator) to the Agent Core.</li>
                        <li><strong>Agent Core:** Consumes the `AsyncIterable` using `for await...of`.</li>
                        <li><strong>Agent Core:** For each `StreamEvent`:
                            <ul class="list-disc list-inside ml-6 space-y-1 text-sm mt-1">
                                <li>Pushes the event to `uiSystem.getLLMStreamSocket().notify(event)`.</li>
                                <li>If it's a final response `TOKEN`, appends it to an internal buffer.</li>
                                <li>If it's `METADATA`, `ERROR`, or `END`, records it via `ObservationManager`.</li>
                                <li>Aggregates `METADATA`.</li>
                            </ul>
                        </li>
                        <li><strong>UI:** Receives `StreamEvent`s via its `LLMStreamSocket` subscription and updates the display in real-time.</li>
                        <li><strong>Agent Core (After Stream Ends):** Constructs the final `ConversationMessage` from the buffer, saves it via `ConversationManager`, and returns the `AgentFinalResponse` containing the final message and aggregated `ExecutionMetadata` (including `llmMetadata`).</li>
                        <li><strong>UI:** (Optional but recommended) Receives the final `ConversationMessage` via `ConversationSocket` subscription and replaces the temporary streamed message with the final, persisted one to ensure consistency.</li>
                    </ol>

                     <h4 class="text-lg font-medium mt-6 mb-3 text-gray-800">Thinking vs. Response Tokens (<code>tokenType</code>):</h4>
                     <ul class="list-disc list-inside space-y-2 mb-6 text-gray-700 leading-relaxed">
                         <li>The <code>StreamEvent.tokenType</code> field allows distinguishing between tokens generated during intermediate reasoning steps (e.g., <code>AGENT_THOUGHT_LLM_RESPONSE</code>) and tokens forming the final user-facing answer (e.g., <code>FINAL_SYNTHESIS_LLM_RESPONSE</code>).</li>
                         <li>Adapters determine the <code>LLM_THINKING</code> vs <code>LLM_RESPONSE</code> part based on provider-specific markers (if available).</li>
                         <li>The Agent Core provides the <code>AGENT_THOUGHT</code> vs <code>FINAL_SYNTHESIS</code> context via <code>CallOptions.callContext</code>.</li>
                         <li>The UI can use <code>tokenType</code> to visually differentiate these tokens (e.g., showing thinking steps faded or in a separate area).</li>
                     </ul>

                     <h4 class="text-lg font-medium mt-6 mb-3 text-gray-800">Metadata Delivery (<code>LLMMetadata</code>):</h4>
                     <ul class="list-disc list-inside space-y-2 mb-6 text-gray-700 leading-relaxed">
                          <li>Detailed LLM statistics (token counts, timing) are packaged into <code>LLMMetadata</code> objects.</li>
                          <li>Adapters yield these as <code>METADATA</code> <code>StreamEvent</code>s (either during the stream if the provider supports it, or after the stream ends based on the final response/usage info).</li>
                          <li>These events are broadcast via <code>LLMStreamSocket</code> for potential real-time display.</li>
                          <li>They are also logged as discrete observations (`LLM_STREAM_METADATA`) via `ObservationManager`.</li>
                          <li>Finally, the metadata from all relevant LLM calls within an execution cycle is aggregated and included in the `AgentFinalResponse.metadata.llmMetadata` field.</li>
                     </ul>

                    <div id="streaming-interfaces">
                        <h4 class="text-lg font-medium mt-6 mb-4 text-gray-800">Streaming-Related Interfaces and Types</h4>
                        <p class="mb-4 text-gray-700 leading-relaxed">To support real-time streaming and detailed metadata, the following interfaces and types were introduced or modified:</p>
                        <div class="space-y-4">
                            <div>
                                <p><strong><code>StreamEvent</code> Interface:</strong></p>
<pre><code class="language-typescript">// Non-Developer: Defines the "label" attached to each piece coming from the LLM stream, saying what it is.
// Developer Notes: Represents a single event from the LLM stream iterator. Should be defined in src/types/index.ts.
interface StreamEvent {
  type: 'TOKEN' | 'METADATA' | 'ERROR' | 'END'; // Kind of event
  data: any; // The actual content (string for TOKEN, LLMMetadata for METADATA, Error for ERROR)
  tokenType?: 'LLM_THINKING' | 'LLM_RESPONSE' | 'AGENT_THOUGHT_LLM_THINKING' | 'AGENT_THOUGHT_LLM_RESPONSE' | 'FINAL_SYNTHESIS_LLM_THINKING' | 'FINAL_SYNTHESIS_LLM_RESPONSE'; // Specific kind of token
  threadId: string; // Links event to the conversation thread. Essential for routing and context.
  traceId: string; // Links event to the specific agent.process() call. Essential for debugging and correlation.
  sessionId?: string; // Links event to a specific UI tab/window. Recommended for multi-session scenarios.
}
</code></pre>
                            </div>
                             <div>
                                <p><strong><code>LLMMetadata</code> Interface:</strong></p>
<pre><code class="language-typescript">// Non-Developer: Defines what kind of statistics we want to capture about the LLM's work (like token counts).
// Developer Notes: Structure for holding parsed metadata from LLM responses/streams. Should be defined in src/types/index.ts.
interface LLMMetadata {
  inputTokens?: number;
  outputTokens?: number;
  thinkingTokens?: number; // If available from provider
  timeToFirstTokenMs?: number;
  totalGenerationTimeMs?: number;
  stopReason?: string; // e.g., 'stop_sequence', 'max_tokens', 'end_turn'
  providerRawUsage?: any; // Optional raw usage data from provider for extensibility
  // Developer Note: Include traceId if this object might be stored or passed independently.
  traceId?: string;
}
</code></pre>
                            </div>
                             <div>
                                <p><strong><code>CallOptions</code> Interface:</strong></p>
<pre><code class="language-typescript">// Non-Developer: Adds switches to turn streaming on/off and tells the LLM call whether it's for an internal "thought" or the final answer.
// Developer Notes: Extends options passed to ReasoningEngine.call. Defined in src/types/index.ts.
interface CallOptions {
  threadId: string;
  traceId?: string;
  sessionId?: string;
  userId?: string;
  stream?: boolean; // Request streaming response. Adapters MUST check this.
  callContext?: 'AGENT_THOUGHT' | 'FINAL_SYNTHESIS' | string; // Provides context for tokenType determination by the adapter. Agent Core MUST provide this.
  providerConfig: RuntimeProviderConfig; // Carries the specific target + config for this call. Agent Core MUST provide this.
  // Other potential options like stopSequences, etc. should be IN providerConfig.adapterOptions
}
</code></pre>
                            </div>
                            <div>
                                <p><strong><code>ExecutionMetadata</code> Interface:</strong></p>
<pre><code class="language-typescript">// Non-Developer: Adds a place to store the final LLM statistics associated with the agent's overall response.
// Developer Notes: Extends the metadata returned by agent.process(). Defined in src/types/index.ts.
interface ExecutionMetadata {
  // ... existing fields
  llmMetadata?: LLMMetadata; // Aggregated metadata from LLM calls in the execution. Agent Core MUST populate this.
}
</code></pre>
                            </div>
                             <div>
                                <p><strong><code>ObservationType</code> Enum:</strong></p>
<pre><code class="language-typescript">// Non-Developer: Adds new categories to the agent's logbook specifically for important streaming events.
// Developer Notes: New enum values for ObservationManager.record(). Defined in src/types/index.ts.
enum ObservationType {
  // ... existing types
  LLM_STREAM_START = 'LLM_STREAM_START', // Optional: Logged by Agent Core when iterator consumption begins.
  LLM_STREAM_METADATA = 'LLM_STREAM_METADATA', // Logged by Agent Core upon receiving METADATA event. Content should be LLMMetadata.
  LLM_STREAM_END = 'LLM_STREAM_END', // Logged by Agent Core upon receiving END event.
  LLM_STREAM_ERROR = 'LLM_STREAM_ERROR', // Logged by Agent Core upon receiving ERROR event. Content should be Error object or message.
}
</code></pre>
                            </div>
                        </div>
                    </div>
                </div>

                 <div id="prompt-architecture" class="mt-10">
                     <h3 class="text-xl font-medium mb-4 text-gray-800">2.2. Core Concept: Prompt Management Architecture</h3>
                    <p class="mb-4 text-gray-700 leading-relaxed">ART's prompt management system has been refactored to provide greater flexibility, decouple agent patterns from the core framework, and give developers more control over prompt content.</p>

                    <h4 class="text-lg font-medium mt-6 mb-3 text-gray-800">Rationale:</h4>
                     <ul class="list-disc list-inside space-y-2 mb-6 text-gray-700 leading-relaxed">
                        <li><strong>Support Custom Agent Patterns:</strong> Decouple core prompt assembly from specific agent patterns (like PES) to allow developers to create and integrate arbitrary agent flows without modifying the ART framework.</li>
                        <li><strong>Developer Control over Content:</strong> Enable developers to define and control key prompt content elements, such as system prompts (via `ThreadConfig` or dynamic context), tool presentation (via descriptions/schemas in their tool definitions, interpreted by agent blueprints), and custom data relevant to their specific agent logic (via `PromptContext`).</li>
                        <li><strong>Provider Agnosticism:</strong> Achieve true decoupling between agent logic/prompt structure and provider-specific API requirements by introducing a standardized intermediate format (`ArtStandardPrompt`).</li>
                        <li><strong>Clear Responsibilities:</strong> Establish clear boundaries: Agent Patterns define structure (blueprints) and provide context; `PromptManager` assembles into the standard format; Adapters translate the standard format to the provider API.</li>
                        <li><strong>Alignment:</strong> Align with established framework best practices that favor configuration, standardization, and template-driven customization over hardcoded, monolithic logic.</li>
                    </ul>

                    <h4 class="text-lg font-medium mt-6 mb-3 text-gray-800">Proposed Architecture Overview:</h4>
                    <div class="mermaid mb-8">
graph TD
    subgraph AgentLogic["Agent Pattern Logic"]
        A[Agent Step Logic] -->|Decides prompt needed| B(Retrieves Blueprint/Template)
        A -->|Gathers data| C{PromptContext}
        C -->|Includes| Q(Query)
        C -->|Includes| SP(System Prompt from Config)
        C -->|Includes| H(History from Storage)
        C -->|Includes| TS(Tool Schemas from Registry)
        C -->|Includes| TR(Tool Results if applicable)
        C -->|Includes| CD(Custom Data)
        A -->|Calls| PM(PromptManager.assemblePrompt)
        B -->|blueprint| PM
        C -->|context| PM
    end

    subgraph ARTCore["ART Core"]
        PM -->|Uses Mustache.js| ASP(ArtStandardPrompt)
        PM -->|Returns| A
        ASP -->|Passed to| RE(ReasoningEngine)
        RE -->|Forwards| PA(ProviderAdapter)
        PA -->|Translates| APIFormat(Provider API Payload)
    end

    PA -->|Sends| ExtAPI[External LLM API]
                    </div>

                    <h4 class="text-lg font-medium mt-6 mb-3 text-gray-800">Key Concepts:</h4>
                    <ul class="list-disc list-inside space-y-3 mb-6 text-gray-700 leading-relaxed">
                        <li><strong>`ArtStandardPrompt` Format:</strong> A canonical, provider-agnostic message array format (e.g., `[{ role: 'system' | 'user' | 'assistant' | 'tool_request' | 'tool_result', content: string | object }]`). This replaces the previous `FormattedPrompt` type alias.</li>
                        <li><strong>Agent Pattern Responsibility:</strong> Defines prompt "blueprints" (templates targeting `ArtStandardPrompt`) and gathers all necessary `PromptContext` data (query, system prompt string, history, tool schemas, tool results, custom data). Blueprints are considered intrinsic to the agent pattern's logic and are not selected or modified via `ThreadConfig`. Built-in agents define default system prompt strings, which can be overridden by `ThreadConfig`.</li>
                        <li><strong>Core `PromptManager`:** A stateless assembler with an `assemblePrompt(blueprint: string | object, context: PromptContext): Promise&lt;ArtStandardPrompt&gt;` method. It uses an internal templating engine (like Mustache.js) to produce the `ArtStandardPrompt` object based on the agent-provided blueprint and context.</li>
                        <li><strong>Core `ReasoningEngine`:** Receives the `ArtStandardPrompt` from `PromptManager` and passes it directly to the selected `ProviderAdapter`.</li>
                        <li><strong>Provider Adapters:** Each adapter is responsible for **translating** the received `ArtStandardPrompt` object into the specific API format required by the target LLM provider (e.g., mapping roles, structuring content, handling tool calls/results according to that API's specification).</li>
                    </ul>
                    <p class="mb-4 text-gray-700 leading-relaxed">This architecture ensures that developers can customize prompt behavior significantly by controlling the inputs (blueprints, configuration, context) without needing to modify the core `PromptManager` assembly logic or the provider-specific `Adapter` translation logic.</p>

                     <h4 class="text-lg font-medium mt-6 mb-3 text-gray-800">Example Data Flow (`PromptContext` -> `ArtStandardPrompt`):</h4>
                    <ol class="list-decimal list-inside space-y-3 text-gray-700 leading-relaxed">
                        <li><em>Agent Logic (e.g., PES Planning):</em> Gathers `query`, `history`, `availableTools`, `systemPrompt` (retrieved via `stateManager.getThreadConfigValue` or agent default) into a `PromptContext` object. Retrieves its specific `planningBlueprint` (Mustache template string).</li>
                        <li><em>Agent Logic Calls:</em> `promptManager.assemblePrompt(planningBlueprint, planningContext)`.</li>
                        <li><em>`PromptManager` Execution:</em> Uses Mustache.js to render the `planningBlueprint` using data from `planningContext`. The output is a JSON string representing the `ArtStandardPrompt` array structure defined in the blueprint. `PromptManager` parses this JSON string into the final `ArtStandardPrompt` object.</li>
                        <li><em>`PromptManager` Output:</em> Returns the assembled `ArtStandardPrompt` (an array of `ArtStandardMessage` objects).</li>
                        <li><em>Agent Logic:</em> Determines the `RuntimeProviderConfig` (e.g., from `ThreadConfig`). Creates `CallOptions` including the `providerConfig`. Calls `reasoningEngine.call(assembledPrompt, callOptions)`.</li>
                        <li><em>`ReasoningEngine` Execution:</em> Calls `providerManager.getAdapter(callOptions.providerConfig)` to get a `ManagedAdapterAccessor`.</li>
                        <li><em>`ReasoningEngine` Execution:</em> Calls `accessor.adapter.call(assembledPrompt, callOptions)` which translates the prompt and interacts with the LLM API.</li>
                        <li><em>`ReasoningEngine` Execution (Post-Call/Stream):</em> Calls `accessor.release()` to return the adapter instance slot to the `ProviderManager`.</li>
                    </ol>
                </div>

                 <div id="state-management-architecture" class="mt-10">
                    <h3 class="text-xl font-medium mb-4 text-gray-800">2.3. Core Concept: State Management Architecture</h3>
                    <p class="mb-4 text-gray-700 leading-relaxed">ART's state management system is responsible for keeping track of important information beyond just the conversation history. This includes thread-specific configurations and the agent's internal state, which helps maintain context and manage complex tasks across turns.</p>

                    <h4 class="text-lg font-medium mt-6 mb-3 text-gray-800">Components of State Management:</h4>
                    <p class="mb-4 text-gray-700 leading-relaxed">ART's state management involves a few key components working together:</p>
                    <ul class="list-disc list-inside space-y-3 mb-6 text-gray-700 leading-relaxed">
                        <li><strong>The State Manager (`StateManager`):</strong> This is the primary interface within the ART core that agent implementations interact with. Its role is to handle the *logic* of state management. It knows *what* state is needed for a particular conversation or task and *when* to load or save it. It provides methods like `loadThreadContext(threadId)` to retrieve configuration (`ThreadConfig`) and state (`AgentState`) for a specific thread, and `saveStateIfModified(threadId)` to persist changes. The `ThreadConfig` is particularly important as it's the standard place for applications to store thread-specific settings, such as the desired LLM provider and model (`RuntimeProviderConfig`) for that conversation.</li>
                        <li><strong>The State Repository (`IStateRepository`):</strong> This component acts as a librarian between the `StateManager` and the actual storage mechanism. It defines *how* to find and organize the specific state information the `StateManager` asks for. It has methods like `getState(threadId)` or `saveState(threadId, state)`. However, the librarian doesn't handle the physical storage details; it delegates these operations to the `StorageAdapter`.</li>
                        <li><strong>The Storage Adapter (`StorageAdapter`):</strong> This is the layer responsible for the *actual saving and loading* of state data to and from a physical storage medium. It knows *how* to interact with the specific storage backend being used (e.g., IndexedDB, a database, a custom API). The `IStateRepository` tells the `StorageAdapter` *what* data to store or retrieve (e.g., "save this piece of state data with this ID in the 'state' collection"), and the `StorageAdapter` handles the *how* (e.g., "write this JSON object to the 'state' table in Supabase").</li>
                    </ul>

                     <h4 class="text-lg font-medium mt-6 mb-3 text-gray-800">Extensibility through the Storage Adapter:</h4>
                     <p class="mb-4 text-gray-700 leading-relaxed">A key aspect of ART's state management flexibility for developers using the npm package is the ability to provide a custom `StorageAdapter`. While the `StateManager` and `IStateRepository` interfaces and their default implementations are part of the core framework, the `IStateRepository` is designed to work with *any* component that implements the `StorageAdapter` interface.</p>
                     <p class="mb-4 text-gray-700 leading-relaxed">This means you, as the application developer, have full control over where and how the state is persisted by implementing your own `StorageAdapter` that connects to your desired storage backend.</p>

                      <h4 class="text-lg font-medium mt-6 mb-3 text-gray-800">How Custom Storage Adapters Enable Flexible State Management:</h4>
                      <p class="mb-4 text-gray-700 leading-relaxed">By creating a custom `StorageAdapter`, you can integrate ART's state management with virtually any storage solution. This allows for:</p>
                      <ul class="list-disc list-inside space-y-2 mb-6 text-gray-700 leading-relaxed">
                            <li><strong>Persistent State:</strong> Using databases (like Supabase, PostgreSQL, etc.) or other persistent storage mechanisms to ensure conversation settings and agent state are saved across application sessions.</li>
                            <li><strong>Caching:</strong> Implementing caching layers (like using `InMemoryStorageAdapter` as a cache in front of a persistent backend) to improve performance by quickly accessing frequently used state data.</li>
                            <li><strong>Custom Logic:</strong> Adding any custom logic needed for your specific storage requirements within your adapter, such as data transformation, encryption, or integration with specific APIs.</li>
                      </ul>
                      <p class="mb-4 text-gray-700 leading-relaxed">When you initialize ART using `createArtInstance`, you provide your custom `StorageAdapter` instance in the configuration. The ART factory will then ensure that the core state management components use your adapter for all state persistence and loading operations.</p>

                    <h4 class="text-lg font-medium mt-6 mb-4 text-gray-800">How to Create and Use Your Custom Storage Adapter:</h4>
                    <ol class="list-decimal list-inside space-y-3 text-gray-700 leading-relaxed">
                         <li><strong>Create Your Adapter File(s):</strong> Create new file(s) in your application's project, perhaps in a folder like `storage-adapters` or `data`. For example, `supabase-adapter.ts` and `caching-adapter.ts`.</li>
                         <li><strong>Import Necessary ART Components:** Inside your adapter files, import the required types and interfaces from `art-framework`. Key imports for a storage adapter include:
                             <ul class="list-disc list-inside ml-6 space-y-1 text-sm mt-1">
                                 <li>`StorageAdapter`: The interface your adapter class must implement.</li>
                                 <li>`FilterOptions`: The type defining options for querying data.</li>
                                 <li>You might also need types for the data you are storing (e.g., `ConversationMessage`, `AgentState`, `Observation`) if you want type safety within your adapter, although the `StorageAdapter` interface uses generic types (`<T>`).</li>
                             </ul>
                         </li>
                         <li><strong>Implement Your Adapter Class(es):** Create the class(es) that implement the `StorageAdapter` interface.
                             <ul class="list-disc list-inside ml-6 space-y-1 text-sm mt-1">
                                 <li>For a backend-specific adapter (like a `SupabaseAdapter`), implement the `get`, `set`, `delete`, and `query` methods using the client library for your chosen backend to interact with your database or storage service. Implement `init` if you need to establish the connection asynchronously.</li>
                                 <li>For a caching adapter (like a `CachingStorageAdapter`), implement the `get`, `set`, `delete`, and `query` methods by coordinating calls to the injected primary and cache adapters (e.g., check cache on `get`, write to both on `set`).</li>
                             </ul>
                         </li>
                         <li><strong>Import and Pass to `createArtInstance`:** In the file where you initialize ART, import your custom adapter class(es). In the configuration object passed to `createArtInstance`, create instances of your custom adapters and pass the top-level adapter (e.g., your `CachingStorageAdapter`) in the `storage` part:
<pre><code class="language-typescript">// Assuming SupabaseAdapter constructor takes options like URL and Key
const supabaseAdapter = new SupabaseAdapter({ url: 'YOUR_SUPABASE_URL', apiKey: 'YOUR_SUPABASE_API_KEY' });
const inMemoryAdapter = new InMemoryStorageAdapter(); // Use the built-in in-memory adapter

// Instantiate your caching adapter with the primary and cache adapters
const cachingAdapter = new CachingStorageAdapter(supabaseAdapter, inMemoryAdapter);

// Define the ProviderManagerConfig
const providerConfig: ProviderManagerConfig = {
  availableProviders: [
    {
      name: 'openai', // Unique identifier for this provider configuration
      adapter: OpenAIAdapter, // The adapter class
      // isLocal: false (default)
    },
    // Add other providers like Ollama, Anthropic, etc. here
  ],
  // Optional global limits
  // maxParallelApiInstancesPerProvider: 5,
  // apiInstanceIdleTimeoutSeconds: 300,
};

const config = {
  storage: cachingAdapter, // Pass the caching adapter instance
  providers: providerConfig, // Pass the ProviderManagerConfig
  // ... other config (agentCore, tools)
};

const art = await createArtInstance(config);
</code></pre>
                        </li>
                    </ol>
                    <p class="mt-4 text-gray-700 leading-relaxed">By following these steps, you can seamlessly integrate your custom storage solution(s) with ART's state management system without modifying the framework's core code, providing the flexibility to handle various persistence and caching requirements.</p>
                </div>

                <div id="provider-manager-architecture" class="mt-10">
                    <h3 class="text-xl font-medium mb-4 text-gray-800">2.4. Core Concept: Provider Manager Architecture</h3>
                     <p class="mb-4 text-gray-700 leading-relaxed">ART's provider manager system enables flexible runtime selection and management of multiple LLM providers within a single ART instance. This architecture allows applications to seamlessly switch between different providers (like OpenAI, Anthropic, or local models) based on user choice, task requirements, or other runtime conditions.</p>

                     <h4 class="text-lg font-medium mt-6 mb-3 text-gray-800">Core Goals:</h4>
                     <ul class="list-disc list-inside space-y-2 mb-6 text-gray-700 leading-relaxed">
                        <li><strong>Dynamic Provider Selection:</strong> Configure multiple potential LLM providers at initialization, but select specific providers and models at runtime.</li>
                        <li><strong>Resource Management:</strong> Control concurrent API usage, manage instance lifecycles, and enforce provider-specific constraints.</li>
                        <li><strong>Separation of Concerns:</strong> Clearly separate provider configuration (available providers) from runtime selection (which provider to use for a specific call).</li>
                     </ul>

                     <h4 class="text-lg font-medium mt-6 mb-3 text-gray-800">Key Components:</h4>
                     <div class="mermaid mb-8">
graph TD
    subgraph DeveloperSetup ["Node 1: Developer Interface"]
        Config["createArtInstance(config)"] --> PM_Config{ProviderManagerConfig}
        PM_Config --> AvailProvider1["Available: OpenAI\nAdapter Class, Limits"]
        PM_Config --> AvailProviderN["Available: Ollama\nAdapter Class, isLocal=true"]
        AppLogic["Application Logic"] -- Manages --> RuntimeCfg(RuntimeProviderConfig)
    end

    subgraph ARTCore ["Node 2: ART Core Orchestration"]
        Factory["AgentFactory"] -->|Creates| ProvMgr(ProviderManager)
        RE[ReasoningEngine] -->|Requests Instance| ProvMgr
        ProvMgr -- Manages --> InstancePool["Instance Pool/Cache"]
        ProvMgr -- Returns --> Accessor["ManagedAdapterAccessor\n(adapter + release())"]
        RE -- Uses --> Accessor
        AgentCore["IAgentCore"] -->|Creates| RuntimeCfg
        AgentCore -->|Calls with RuntimeCfg| RE
    end

    subgraph ExternalConnections ["Node 3: External Providers"]
        Accessor --> API["API Providers\n(OpenAI, Anthropic)"]
        Accessor --> Local["Local Providers\n(Ollama)"]
    end

    style ProvMgr fill:#f9d,stroke:#333,stroke-width:2px
    style RuntimeCfg fill:#lightgreen,stroke:#333,stroke-width:1px
    style Accessor fill:#lightblue,stroke:#333,stroke-width:1px

                     </div>

                     <h4 class="text-lg font-medium mt-6 mb-3 text-gray-800">1. Provider Registration (Initialization Time):</h4>
                     <ul class="list-disc list-inside space-y-2 mb-6 text-gray-700 leading-relaxed">
                        <li>
                            <strong><code>ProviderManagerConfig</code>:</strong> Defines available provider adapters and global management rules:
<pre><code class="language-typescript">interface ProviderManagerConfig {
    availableProviders: {
        name: string;          // e.g., 'openai', 'anthropic', 'ollama_local'
        adapter: new (...args) => ProviderAdapter; // The adapter class
        isLocal?: boolean;     // Determines singleton vs pooling behavior
    }[];
    maxParallelApiInstancesPerProvider?: number; // Default: 5
    apiInstanceIdleTimeoutSeconds?: number;      // Default: 300
}
</code></pre>
                        </li>
                         <li>
                            <strong>Example Configuration:</strong>
<pre><code class="language-typescript">const config = {
    providers: {
        availableProviders: [
            {
                name: 'openai',
                adapter: OpenAIAdapter,
                // Default API provider behavior
            },
            {
                name: 'ollama_local',
                adapter: OllamaAdapter,
                isLocal: true  // Enforces singleton behavior
            }
        ],
        maxParallelApiInstancesPerProvider: 3
    }
    // ... other ART config
};
</code></pre>
                        </li>
                    </ul>

                     <h4 class="text-lg font-medium mt-6 mb-3 text-gray-800">2. Runtime Provider Selection:</h4>
                    <ul class="list-disc list-inside space-y-2 mb-6 text-gray-700 leading-relaxed">
                        <li>
                            <strong><code>RuntimeProviderConfig</code>:</strong> Specifies the desired provider and settings for a specific LLM call:
<pre><code class="language-typescript">interface RuntimeProviderConfig {
    providerName: string;    // Must match a registered name
    modelId: string;         // e.g., 'gpt-4o', 'llama3:latest'
    adapterOptions: {        // Provider-specific options passed to adapter constructor
        apiKey?: string;
        baseUrl?: string;
        temperature?: number; // Can often be set here or via LLM params
        // ... other options
    };
}
</code></pre>
                        </li>
                        <li>
                            The application typically stores this configuration in <code>ThreadConfig</code> via <code>StateManager</code>:
<pre><code class="language-typescript">await stateManager.setThreadConfigValue(threadId, 'runtimeProviderConfig', {
    providerName: 'openai',
    modelId: 'gpt-4o',
    adapterOptions: {
        apiKey: process.env.OPENAI_API_KEY
    }
});
</code></pre>
                        </li>
                    </ul>

                    <h4 class="text-lg font-medium mt-6 mb-3 text-gray-800">3. Instance Management Rules:</h4>
                    <ul class="list-disc list-inside space-y-2 mb-6 text-gray-700 leading-relaxed">
                        <li><strong>API Providers (isLocal: false):</strong>
                            <ul class="list-circle list-inside ml-6 space-y-1 text-sm mt-1">
                                <li>Multiple instances allowed up to `maxParallelApiInstancesPerProvider` per provider.</li>
                                <li>Instances are pooled and reused when configurations match.</li>
                                <li>Idle instances are evicted after `apiInstanceIdleTimeoutSeconds`.</li>
                                <li>Requests queue when provider limit is reached.</li>
                            </ul>
                        </li>
                        <li><strong>Local Providers (isLocal: true):</strong>
                            <ul class="list-circle list-inside ml-6 space-y-1 text-sm mt-1">
                                <li>Only ONE local provider instance can be active across all providers.</li>
                                <li>No idle timeouts - instances persist until replaced.</li>
                                <li>Requesting a different local provider while one is active throws an error.</li>
                            </ul>
                        </li>
                    </ul>

                    <h4 class="text-lg font-medium mt-6 mb-3 text-gray-800">4. Workflow Example:</h4>
                    <ol class="list-decimal list-inside space-y-3 mb-6 text-gray-700 leading-relaxed">
                        <li>
                            <strong>Application Prepares:</strong>
<pre><code class="language-typescript">// In your React component or application logic
const runtimeConfig: RuntimeProviderConfig = {
    providerName: 'openai',
    modelId: 'gpt-4o',
    adapterOptions: {
        apiKey: apiKey,
        temperature: 0.7
    }
};
await art.stateManager.setThreadConfigValue(threadId, 'runtimeProviderConfig', runtimeConfig);
</code></pre>
                        </li>
                        <li>
                             <strong>Agent Core Acts:</strong>
<pre><code class="language-typescript">// Inside PESAgent or custom IAgentCore implementation
async process(props: AgentProps) {
    const context = await this.deps.stateManager.loadThreadContext(threadId);
    const runtimeConfig = context.config.runtimeProviderConfig; // Load config for the thread

    // Create CallOptions with the runtime config
    const callOptions: CallOptions = {
        providerConfig: runtimeConfig, // Pass the specific config for this call
        threadId: props.threadId,
        traceId: /* ... */,
        stream: true,
        // ... other options
    };

    // ReasoningEngine will use this config to get the right adapter
    const responseStream = await this.deps.reasoningEngine.call(prompt, callOptions);
    // ... continue processing
}
</code></pre>
                        </li>
                        <li>
                            <strong>Provider Manager Handles Request:</strong>
<pre><code class="language-typescript">// Inside ProviderManagerImpl
async getAdapter(config: RuntimeProviderConfig): Promise<ManagedAdapterAccessor> {
    // Check if config is for a local provider
    if (this.isLocalProvider(config.providerName)) {
        // Enforce local provider singleton rules
        await this.enforceLocalProviderConstraints(config);
    } else {
        // Check API provider limits & queue if needed
        await this.enforceApiProviderLimits(config);
    }

    // Get or create adapter instance (passing config.adapterOptions to constructor)
    const instance = await this.getOrCreateInstance(config);

    // Return accessor with release function
    return {
        adapter: instance.adapter,
        release: () => this.releaseInstance(instance)
    };
}
</code></pre>
                        </li>
                        <li>
                            <strong>ReasoningEngine Uses Adapter:</strong>
<pre><code class="language-typescript">// Inside ReasoningEngineImpl
async call(prompt: ArtStandardPrompt, options: CallOptions): Promise<AsyncIterable<StreamEvent>> {
    // Get managed adapter using the provided config
    const accessor = await this.providerManager.getAdapter(options.providerConfig);

    try {
        // Create releasing generator that ensures adapter is released
        return {
            [Symbol.asyncIterator]: async function*() {
                try {
                    // Call the specific adapter instance's call method
                    const stream = await accessor.adapter.call(prompt, options);
                    for await (const event of stream) {
                        yield event;
                    }
                } finally {
                    // Always release the adapter back to the manager
                    accessor.release();
                }
            }
        };
    } catch (error) {
        // Release on error during the call itself
        accessor.release();
        throw error;
    }
}
</code></pre>
                        </li>
                    </ol>

                    <h4 class="text-lg font-medium mt-6 mb-3 text-gray-800">5. Error Scenarios:</h4>
                     <ul class="list-disc list-inside space-y-2 mb-6 text-gray-700 leading-relaxed">
                        <li>
                            <strong>Local Provider Conflict:</strong>
<pre><code class="language-typescript">// Trying to use Ollama while another local provider is active
throw new LocalProviderConflictError(
    `Cannot use Ollama while LMStudio is active. Only one local provider can be active at a time.`
);
</code></pre>
                        </li>
                         <li>
                            <strong>API Limit Reached:</strong>
<pre><code class="language-typescript">// All OpenAI slots in use (hitting maxParallelApiInstancesPerProvider)
// Request will be queued, or throw error if queueing disabled
throw new ProviderLimitError(
    `Maximum parallel instances (3) reached for OpenAI. Try again later.`
);
</code></pre>
                        </li>
                    </ul>
                    <p class="mb-4 text-gray-700 leading-relaxed">This architecture enables applications to provide flexible LLM provider selection while maintaining control over resource usage and enforcing necessary constraints. The separation between initialization-time configuration (`ProviderManagerConfig`) and runtime selection (`RuntimeProviderConfig`) allows for dynamic provider switching without sacrificing stability or control.</p>
                </div>

            </section>

            <section id="scenario-1" class="mb-16 p-6 bg-white rounded-lg shadow-lg fade-in-section">
                <h2 class="text-2xl md:text-3xl font-semibold mb-6 border-b pb-3 text-sky-700">3. Scenario 1: Building a Feature-Rich React Chatbot (Simple Usage)</h2>
                <p class="mb-5 text-gray-700 leading-relaxed">
                    Let's build a chatbot component for a React website using only ART's built-in features. We'll aim to showcase several core ART capabilities.
                </p>
                <p class="mb-8 text-gray-700 leading-relaxed"><strong>Goal:</strong> A chat interface where users can talk to an AI powered by OpenAI, with conversation history saved in the browser, and some real-time feedback using ART's observation system.</p>

                <div id="scenario-1-imports" class="mb-10">
                    <h3 class="text-xl font-medium mb-4 text-gray-800">3.1. Necessary Imports & Explanations</h3>
<pre><code class="language-typescript">// src/components/ArtChatbot.tsx
import React, { useState, useEffect, useRef, useCallback } from 'react';

// --- ART Core Imports ---
import {
  // The main factory function to initialize ART
  createArtInstance,
  // Type definition for the initialized ART object
  ArtInstance,
  // Type for the properties needed to call the agent's process method
  AgentProps,
  // Type for the final response object from the agent
  AgentFinalResponse,
  // Type representing a single message in the conversation
  ConversationMessage,
  // Enum defining message roles (USER, ASSISTANT, SYSTEM, TOOL)
  MessageRole,
  // Type representing an internal event/observation within ART
  Observation,
  // Enum defining different types of observations (PROCESS_START, LLM_REQUEST, etc.)
  ObservationType,
  // The default Plan-Execute-Synthesize agent pattern implementation
  PESAgent,
  // Interfaces for core components (needed for type hints, less for direct use here)
  StorageAdapter, ProviderAdapter, ReasoningEngine, IToolExecutor, IAgentCore,
  StateManager, ConversationManager, ToolRegistry, ObservationManager, UISystem,
  // New/Updated types for streaming and metadata
  StreamEvent, LLMMetadata, ExecutionMetadata,
  // Types for Provider Manager configuration
  ProviderManagerConfig, RuntimeProviderConfig
} from 'art-framework'; // Assuming 'art-framework' is the installed package name

// --- ART Adapter Imports (Developer Choices) ---
import {
  // Storage adapter that uses the browser's IndexedDB for persistence
  IndexedDBStorageAdapter,
  // Storage adapter that uses temporary browser memory (data lost on refresh)
  // InMemoryStorageAdapter, // Alternative, uncomment if preferred
} from 'art-framework'; // Adapters are usually exported from the main package too

import {
  // Reasoning provider adapter for OpenAI models (GPT-3.5, GPT-4, etc.)
  OpenAIAdapter,
  // Reasoning provider adapter for Google Gemini models
  // GeminiAdapter, // Alternative, uncomment if preferred
  // Reasoning provider adapter for Anthropic Claude models
  // AnthropicAdapter,
} from 'art-framework';

// --- ART Built-in Tool Imports (Optional) ---
import {
  // A simple tool that can evaluate mathematical expressions
  CalculatorTool
} from 'art-framework';
</code></pre>
                    <h4 class="text-lg font-medium mt-6 mb-4 text-gray-800">Explanation of Imports:</h4>
                    <div class="space-y-4 text-gray-700 leading-relaxed">
                        <div>
                            <p><strong><code>createArtInstance</code></strong></p>
                            <p class="mb-2">This is the main function you use to start the ART framework. Think of it as the "ignition key" – you give it instructions (configuration), and it builds and starts the ART engine for you.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                An asynchronous factory function (`async function createArtInstance(config: AgentFactoryConfig): Promise&lt;ArtInstance&gt;`). Takes `config` (object conforming to the `AgentFactoryConfig` interface: `{ storage: StorageAdapter, providers: ProviderManagerConfig, tools?: IToolExecutor[], agentCore?: new (deps: any) => IAgentCore, logger?: { level?: LogLevel } }`). Uses `AgentFactory` internally to instantiate and inject dependencies for all core components (Managers, Systems, Repositories, Adapters, ProviderManager). Returns a `Promise` resolving to the fully initialized `ArtInstance` object. Typically called once at application setup.
                            </details>
                        </div>
                         <div>
                            <p><strong><code>ArtInstance</code></strong></p>
                            <p class="mb-2">This describes the main control panel you get after starting ART. It's the object that lets you interact with the initialized framework, primarily by telling it to process user messages.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                A TypeScript interface defining the public API returned by <code>createArtInstance</code>. Key properties:
                                <ul class="list-disc list-inside ml-4">
                                    <li><code>process(props: AgentProps): Promise&lt;AgentFinalResponse&gt;</code>: The core method to run the agent's reasoning cycle.</li>
                                    <li><code>conversationManager: ConversationManager</code>: Access methods like <code>getMessages</code>, <code>addMessages</code>.</li>
                                    <li><code>stateManager: StateManager</code>: Access methods like <code>loadThreadContext</code>, <code>setThreadConfigValue</code>, `getThreadConfigValue`, `getAgentState`, `setAgentState`, `isToolEnabled`.</li>
                                    <li><code>toolRegistry: ToolRegistry</code>: Access methods like <code>registerTool</code>, <code>getToolExecutor</code>, <code>getAvailableTools</code>.</li>
                                    <li><code>observationManager: ObservationManager</code>: Access methods like <code>record</code>, <code>getObservations</code>.</li>
                                    <li><code>uiSystem: UISystem</code>: Access methods like <code>getObservationSocket</code>, <code>getConversationSocket</code>, and <code>getLLMStreamSocket</code> (for streaming) to get subscription interfaces.</li>
                                </ul>
                            </details>
                        </div>
                         <div>
                            <p><strong><code>AgentProps</code></strong></p>
                            <p class="mb-2">Describes the information you need to give the agent each time you want it to respond (your message and which chat it belongs to).</p>
                            <details>
                                <summary>Developer Notes</summary>
                                Interface for the input object to <code>ArtInstance.process()</code>.
                                <ul class="list-disc list-inside ml-4">
                                    <li>Required: <code>query: string</code> (user input), <code>threadId: string</code> (conversation ID).</li>
                                    <li>Optional: `configOverrides?: Partial<ThreadConfig>` (can include `runtimeProviderConfig` to override the thread's default provider for this call), `executionContext?: Record<string, any>`, `userId?: string`, `sessionId?: string`.</li>
                                </ul>
                            </details>
                        </div>
                         <div>
                            <p><strong><code>AgentFinalResponse</code></strong></p>
                            <p class="mb-2">Describes the information the agent gives back after processing your request (its reply and some tracking info).</p>
                            <details>
                                <summary>Developer Notes</summary>
                                Interface for the output object from <code>ArtInstance.process()</code>.
                                <ul class="list-disc list-inside ml-4">
                                    <li>Core Properties: <code>responseText: string</code>, <code>responseId: string</code>, <code>threadId: string</code>, <code>traceId: string</code>.</li>
                                    <li>Optional/Contextual: `toolResults?: ToolResult[]`.</li>
                                    <li>`metadata: ExecutionMetadata`: Contains detailed metadata about the execution cycle, including aggregated LLM statistics (`llmMetadata`).</li>
                                </ul>
                            </details>
                        </div>
                         <div>
                            <p><strong><code>ConversationMessage</code></strong></p>
                            <p class="mb-2">How each chat bubble's information (who sent it, what it says, when) is organized.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                Interface representing a message. Properties: <code>id: string</code>, <code>role: MessageRole</code>, <code>content: string</code>, <code>timestamp: number</code>, <code>threadId: string</code>, <code>metadata?: Record&lt;string, any&gt;</code>. Used by <code>ConversationManager</code> (via <code>StorageAdapter</code>) and often directly in UI rendering logic.
                            </details>
                        </div>
                         <div>
                            <p><strong><code>MessageRole</code></strong></p>
                            <p class="mb-2">Labels to know if a message is from the User, the AI Assistant, the System (e.g., errors, info), or a Tool (results).</p>
                            <details>
                                <summary>Developer Notes</summary>
                                TypeScript enum: <code>USER</code>, <code>ASSISTANT</code>, <code>SYSTEM</code>, <code>TOOL</code>. Crucial for structuring prompts for the LLM (differentiating user input from previous AI responses) and for UI display logic.
                            </details>
                        </div>
                         <div>
                            <p><strong><code>Observation</code></strong></p>
                            <p class="mb-2">A notification about something happening inside the agent's brain while it's working (like "Thinking..." or "Using calculator...").</p>
                            <details>
                                <summary>Developer Notes</summary>
                                Interface for internal events. Properties: <code>id: string</code>, <code>timestamp: number</code>, <code>threadId: string</code>, <code>traceId?: string</code>, <code>type: ObservationType</code>, <code>content: any</code>, <code>metadata?: Record&lt;string, any&gt;</code>. Emitted by <code>ObservationManager</code> and broadcast via <code>UISystem</code>'s <code>ObservationSocket</code>. Useful for real-time UI updates (status indicators) and debugging.
                            </details>
                        </div>
                         <div>
                            <p><strong><code>ObservationType</code></strong></p>
                            <p class="mb-2">Labels for the different types of internal notifications (like "Started thinking", "Asking the AI", "Finished using a tool").</p>
                            <details>
                                <summary>Developer Notes</summary>
                                TypeScript enum listing event types (e.g., <code>PROCESS_START</code>, <code>LLM_REQUEST</code>, <code>LLM_RESPONSE</code>, <code>TOOL_START</code>, <code>TOOL_END</code>, <code>PLANNING_OUTPUT</code>, <code>SYNTHESIS_OUTPUT</code>, <code>PROCESS_END</code>, <code>REACT_STEP</code>, <code>thought</code>, <code>action</code>, <code>observation`). Includes new types for discrete streaming events: `LLM_STREAM_START`, `LLM_STREAM_METADATA`, `LLM_STREAM_END`, `LLM_STREAM_ERROR`. Used to categorize `Observation` events and filter subscriptions on the `ObservationSocket`.
                            </details>
                        </div>
                         <div>
                            <p><strong><code>StreamEvent</code></strong></p>
                            <p class="mb-2">Represents a single piece of information arriving from the LLM's real-time stream (like a word, statistics, or an end signal).</p>
                            <details>
                                <summary>Developer Notes</summary>
                                Interface defining the structure of events yielded by the `ReasoningEngine.call` async iterable. Properties: <code>type</code> ('TOKEN', 'METADATA', 'ERROR', 'END'), <code>data</code> (content), <code>tokenType</code> (classification like 'LLM_THINKING', 'FINAL_SYNTHESIS_LLM_RESPONSE'), <code>threadId</code>, <code>traceId</code>, <code>sessionId</code>. Consumed by the Agent Core and pushed to the <code>LLMStreamSocket</code>.
                            </details>
                        </div>
                          <div>
                            <p><strong><code>LLMMetadata</code></strong></p>
                            <p class="mb-2">A structured way to hold detailed statistics about an LLM call (token counts, timing, etc.).</p>
                            <details>
                                <summary>Developer Notes</summary>
                                Interface defining the structure for LLM statistics. Properties: <code>inputTokens?</code>, <code>outputTokens?</code>, <code>thinkingTokens?</code>, <code>timeToFirstTokenMs?</code>, <code>totalGenerationTimeMs?</code>, <code>stopReason?</code>, <code>providerRawUsage?</code>, <code>traceId?</code>. Delivered via <code>StreamEvent</code> (type 'METADATA') and aggregated into <code>ExecutionMetadata.llmMetadata</code>.
                            </details>
                        </div>
                         <div>
                            <p><strong><code>PESAgent</code></strong></p>
                            <p class="mb-2">The specific "thinking style" the agent will use by default (Plan -> Use Tools -> Answer).</p>
                            <details>
                                <summary>Developer Notes</summary>
                                Concrete class implementing <code>IAgentCore</code>. Instantiated by <code>AgentFactory</code> if specified in <code>config.agentCore</code> or if `agentCore` is omitted. Receives dependencies (`StateManager`, `ReasoningEngine`, `ToolSystem`, `UISystem`, etc.) in its constructor. Its `process` method orchestrates the Plan-Execute-Synthesize flow, determines the `RuntimeProviderConfig` for each LLM call, interacts with the injected dependencies, and handles the consumption and processing of the `AsyncIterable<StreamEvent>` from the `ReasoningEngine`.
                            </details>
                        </div>
                         <div>
                            <p><strong><code>PromptManager</code></strong></p>
                            <p class="mb-2">This component is now a stateless assembler that takes a prompt "blueprint" (template) and a <code>PromptContext</code> object to generate a standardized <code>ArtStandardPrompt</code> (an array of messages). It no longer has hardcoded prompt logic tied to specific agent patterns.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                The `PromptManager` interface now has an `assemblePrompt(blueprint: string | object, context: PromptContext): Promise&lt;ArtStandardPrompt&gt;` method. Agent implementations are responsible for providing the appropriate blueprint and gathering the necessary data into the `PromptContext`.
                            </details>
                        </div>
                        <div>
                            <p><strong><code>ReasoningEngine</code></strong></p>
                            <p class="mb-2">This component orchestrates the interaction with the selected LLM provider via the `ProviderManager`. Its `call` method takes the assembled prompt and `CallOptions` (which includes the `RuntimeProviderConfig`) and returns a `Promise<AsyncIterable<StreamEvent>>`.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                The `ReasoningEngine` uses the `ProviderManager` to get a managed instance of the correct `ProviderAdapter` based on `CallOptions.providerConfig`. It then calls the adapter's `call` method and returns the resulting stream, wrapped in logic to ensure the adapter instance is released back to the `ProviderManager`.
                            </details>
                        </div>
                         <div>
                            <p><strong><code>UISystem</code></strong></p>
                            <p class="mb-2">This system provides access to communication channels for the UI. It now includes a dedicated <code>LLMStreamSocket</code> for broadcasting real-time <code>StreamEvent</code>s from the LLM.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                The <code>UISystem</code> interface includes a <code>getLLMStreamSocket(): LLMStreamSocket</code> method. UI components subscribe to this socket to receive and display streaming tokens and other stream events.
                            </details>
                        </div>
                         <div>
                            <p><strong><code>IndexedDBStorageAdapter</code> / <code>InMemoryStorageAdapter</code></strong></p>
                            <p class="mb-2">How the agent remembers the conversation. <code>IndexedDB</code> is like saving to a file (remembers after closing), <code>InMemory</code> is like writing on a whiteboard (erased when closed).</p>
                            <details>
                                <summary>Developer Notes</summary>
                                Concrete classes implementing <code>StorageAdapter</code> (`get`, `set`, `delete`, `query`). Passed directly in `config.storage` during `createArtInstance`. Used by internal Repositories. `IndexedDB` provides persistence across browser sessions; `InMemory` does not.
                            </details>
                        </div>
                         <div>
                            <p><strong><code>OpenAIAdapter</code> / <code>GeminiAdapter</code> / <code>AnthropicAdapter</code></strong></p>
                            <p class="mb-2">The specific translator the agent uses to talk to a particular AI brain (like OpenAI's GPT, Google's Gemini, or Anthropic's Claude).</p>
                            <details>
                                <summary>Developer Notes</summary>
                                Concrete classes implementing <code>ProviderAdapter</code>. Registered by *class* in `config.providers.availableProviders`. The `ProviderManager` instantiates these adapters at runtime, passing `RuntimeProviderConfig.adapterOptions` to their constructor. The `call` method returns `Promise<AsyncIterable<StreamEvent>>` to support streaming. Adapters must check `options.stream` and `options.callContext`.
                            </details>
                        </div>
                         <div>
                            <p><strong><code>CalculatorTool</code></strong></p>
                            <p class="mb-2">A specific skill the agent can use, like a pocket calculator.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                Concrete class implementing <code>IToolExecutor</code>. Provides <code>schema</code> (`name`, `description`, `inputSchema`) and `execute(input, context)`. Instances are passed in `config.tools`. Registered with `ToolRegistry` and executed by `ToolSystem` when planned by the `IAgentCore`.
                            </details>
                        </div>
                         <div>
                            <p><strong><code>ProviderManagerConfig</code> / <code>RuntimeProviderConfig</code></strong></p>
                            <p class="mb-2">Configuration types for managing and selecting LLM providers.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                <code>ProviderManagerConfig</code> is passed to `createArtInstance` to register available provider adapter *classes*. <code>RuntimeProviderConfig</code> is determined by the application/agent at runtime (often stored in `ThreadConfig` or passed via `AgentProps.configOverrides`) and passed in `CallOptions` to specify which provider/model/options to use for a specific LLM call.
                            </details>
                        </div>
                    </div>
                </div>

                <div id="scenario-1-component" class="mb-8">
                     <h3 class="text-xl font-medium mb-4 text-gray-800">3.2. React Component Implementation</h3>
<pre><code class="language-typescript">// src/components/ArtChatbot.tsx
import React, { useState, useEffect, useRef, useCallback } from 'react';
import {
  createArtInstance, ArtInstance, AgentProps, AgentFinalResponse,
  ConversationMessage, MessageRole, Observation, ObservationType, PESAgent,
  StreamEvent, // Import StreamEvent for socket subscription
  ProviderManagerConfig, RuntimeProviderConfig // Import provider config types
} from 'art-framework';
import { IndexedDBStorageAdapter } from 'art-framework'; // Or InMemoryStorageAdapter
import { OpenAIAdapter } from 'art-framework'; // Or GeminiAdapter, etc.
import { CalculatorTool } from 'art-framework';

// Basic CSS (add this to a corresponding CSS file or use styled-components/tailwind)
/*
.chatbot-container { max-width: 600px; margin: auto; border: 1px solid #ccc; border-radius: 8px; display: flex; flex-direction: column; height: 70vh; }
.message-list { flex-grow: 1; overflow-y: auto; padding: 10px; display: flex; flex-direction: column; }
.message { margin-bottom: 10px; padding: 8px 12px; border-radius: 15px; max-width: 80%; word-wrap: break-word; }
.message.USER { background-color: #dcf8c6; align-self: flex-end; border-bottom-right-radius: 0; }
.message.ASSISTANT { background-color: #f1f0f0; align-self: flex-start; border-bottom-left-radius: 0; }
.message.SYSTEM, .message.TOOL { background-color: #e0e0e0; font-style: italic; font-size: 0.9em; align-self: center; text-align: center; }
.message.ASSISTANT.streaming { background-color: #e6f7ff; /* Lighter blue for streaming */ }
.input-area { display: flex; padding: 10px; border-top: 1px solid #ccc; }
.input-area input { flex-grow: 1; padding: 10px; border: 1px solid #ccc; border-radius: 20px; margin-right: 10px; }
.input-area button { padding: 10px 15px; border: none; background-color: #007bff; color: white; border-radius: 20px; cursor: pointer; }
.input-area button:disabled { background-color: #aaa; cursor: not-allowed; }
.status-indicator { padding: 5px 10px; font-size: 0.8em; color: #666; text-align: center; height: 20px; }
*/

// Helper to generate temporary IDs
const tempId = () => `temp-${Date.now()}-${Math.random().toString(16).slice(2)}`;

const ArtChatbot: React.FC = () => {
  const [messages, setMessages] = useState&lt;ConversationMessage[]&gt;([]);
  const [input, setInput] = useState('');
  const [isLoading, setIsLoading] = useState(false);
  const [status, setStatus] = useState&lt;string&gt;('Initializing...'); // For observation feedback
  const artInstanceRef = useRef&lt;ArtInstance | null&gt;(null);
  const messageListRef = useRef&lt;HTMLDivElement>(null); // For auto-scrolling
  const threadId = 'web-chatbot-thread-1'; // Consistent ID for this chat instance
  const streamingMessageIdRef = useRef<string | null>(null); // Track the ID of the message being streamed

  // --- Auto-scrolling ---
  useEffect(() => {
    if (messageListRef.current) {
      messageListRef.current.scrollTop = messageListRef.current.scrollHeight;
    }
  }, [messages]);

  // --- ART Initialization ---
  useEffect(() => {
    let isMounted = true; // Prevent state updates on unmounted component
    let unsubObservation: (() => void) | null = null;
    let unsubConversation: (() => void) | null = null;
    let unsubStream: (() => void) | null = null; // Subscription for LLM stream

    const initializeArt = async () => {
      if (!artInstanceRef.current) {
        try {
          setStatus('Initializing ART Engine...');

          // Define Provider Configuration
          const providerConfig: ProviderManagerConfig = {
              availableProviders: [
                {
                  name: 'openai', // Name used in RuntimeProviderConfig
                  adapter: OpenAIAdapter, // Pass the adapter class
                }
                // Add other providers like GeminiAdapter, AnthropicAdapter here if needed
              ]
              // Optional global limits can be set here
          };

          // Define ART Factory Configuration
          const config = {
            storage: new IndexedDBStorageAdapter({ dbName: 'artWebChatHistory' }), // Use IndexedDB adapter directly
            providers: providerConfig, // Pass the provider config
            agentCore: PESAgent, // Explicitly using the default
            tools: [new CalculatorTool()] // Include the calculator
          };

          const instance = await createArtInstance(config);
          if (!isMounted) return; // Check if component unmounted during async init

          artInstanceRef.current = instance;
          setStatus('Loading history...');
          await loadMessages(); // Load history after successful init

          // --- Subscribe to Observations (UI Feedback) ---
          setStatus('Connecting observers...');
          const observationSocket = instance.uiSystem.getObservationSocket();
          unsubObservation = observationSocket.subscribe(
            (observation: Observation) => {
              if (observation.threadId === threadId && isMounted) {
                // Simple status updates based on observations
                let newStatus = status;
                switch (observation.type) {
                  case ObservationType.LLM_REQUEST: newStatus = 'Asking AI...'; break;
                  case ObservationType.TOOL_START: newStatus = `Using ${observation.metadata?.toolName}...`; break;
                  case ObservationType.TOOL_END: newStatus = 'Tool finished.'; break;
                  case ObservationType.PROCESS_START: newStatus = 'Processing request...'; break;
                  case ObservationType.PROCESS_END:
                      newStatus = 'Ready.';
                      streamingMessageIdRef.current = null; // Clear streaming ID on process end
                      break;
                  // Add new stream observations if needed for status
                  case ObservationType.LLM_STREAM_START: newStatus = 'Receiving response...'; break;
                  case ObservationType.LLM_STREAM_END: newStatus = 'Response received.'; break;
                  case ObservationType.LLM_STREAM_ERROR: newStatus = 'Stream error.'; break;
                }
                 // Avoid overwriting status if currently processing
                 if (!isLoading) {
                     setStatus(newStatus);
                 }
              }
            },
            undefined, // Subscribe to all relevant types
            { threadId: threadId } // Filter for this specific chat thread
          );

           // --- Subscribe to Conversation (Update UI with Final Messages) ---
           const conversationSocket = instance.uiSystem.getConversationSocket();
           unsubConversation = conversationSocket.subscribe(
             (message: ConversationMessage) => {
               if (message.threadId === threadId && isMounted) {
                 console.log("Received final message via socket:", message);
                 setMessages(prev => {
                   // Replace the temporary streaming message with the final one
                   const existingIndex = prev.findIndex(m => m.id === streamingMessageIdRef.current && m.role === MessageRole.ASSISTANT);
                   if (existingIndex > -1) {
                     const updatedMessages = [...prev];
                     updatedMessages[existingIndex] = message;
                     return updatedMessages;
                   } else if (!prev.some(m => m.id === message.id)) {
                     // Add if not already present (e.g., user message from another client)
                     return [...prev, message].sort((a, b) => a.timestamp - b.timestamp);
                   }
                   return prev;
                 });
                  streamingMessageIdRef.current = null; // Clear after receiving final message
               }
             },
             undefined, // No role filter
             { threadId: threadId }
           );


          // --- Subscribe to LLM Stream (Real-time Token Updates) ---
          const streamSocket = instance.uiSystem.getLLMStreamSocket();
          unsubStream = streamSocket.subscribe(
            (event: StreamEvent) => {
              if (event.threadId === threadId && isMounted) {
                 // Focus on displaying tokens for the final synthesis stage
                if (event.type === 'TOKEN' && event.tokenType === 'FINAL_SYNTHESIS_LLM_RESPONSE') {
                  setMessages(prev => {
                    const currentStreamingId = streamingMessageIdRef.current;
                    if (!currentStreamingId) {
                      // Start of a new streaming message
                      const newStreamingMessage: ConversationMessage = {
                        id: tempId(), // Use temporary ID
                        role: MessageRole.ASSISTANT,
                        content: event.data,
                        timestamp: Date.now(),
                        threadId: threadId,
                        metadata: { streaming: true } // Mark as streaming
                      };
                      streamingMessageIdRef.current = newStreamingMessage.id;
                      return [...prev, newStreamingMessage];
                    } else {
                      // Append token to existing streaming message
                      return prev.map(msg =>
                        msg.id === currentStreamingId
                          ? { ...msg, content: msg.content + event.data }
                          : msg
                      );
                    }
                  });
                } else if (event.type === 'END') {
                    // Handled by PROCESS_END observation or ConversationSocket update
                    // streamingMessageIdRef.current = null; // Clear streaming ID
                } else if (event.type === 'METADATA') {
                    console.log("Stream Metadata:", event.data);
                    // Optionally display metadata
                } else if (event.type === 'ERROR') {
                    console.error("Stream Error:", event.data);
                    setStatus('Stream Error');
                    // Optionally display error message in UI
                    setMessages(prev => [...prev, {
                         id: tempId(), role: MessageRole.SYSTEM, content: `Stream Error: ${event.data.message || event.data}`,
                         timestamp: Date.now(), threadId: threadId
                    }]);
                     streamingMessageIdRef.current = null; // Clear streaming ID on error
                }
              }
            },
            { threadId: threadId } // Filter stream events for this thread
          );


          if (isMounted) setStatus('Ready.');

        } catch (error) {
          console.error("Failed to initialize ART:", error);
          if (isMounted) setStatus(`Initialization Error: ${error instanceof Error ? error.message : 'Unknown error'}`);
        }
      }
    };

    initializeArt();

    // Cleanup function
    return () => {
      isMounted = false;
      console.log("Cleaning up ART subscriptions...");
      if (unsubObservation) unsubObservation();
      if (unsubConversation) unsubConversation();
      if (unsubStream) unsubStream(); // Unsubscribe from stream socket
    };
  }, [threadId]); // Rerun if threadId changes (it doesn't in this example)

  // --- Load Messages ---
  const loadMessages = useCallback(async () => {
    if (!artInstanceRef.current) return;
    try {
      // Use a local loading flag for history fetch to avoid interfering with process loading
      // setIsLoading(true); // Removed this, rely on status indicator
      const history = await artInstanceRef.current.conversationManager.getMessages(threadId, { limit: 100 });
      setMessages(history.sort((a, b) => a.timestamp - b.timestamp)); // Sort oldest to newest
    } catch (error) {
      console.error("Failed to load messages:", error);
      setStatus('Error loading history.');
    } finally {
      // setIsLoading(false); // Removed this
    }
  }, [threadId]);

  // --- Handle Sending ---
  const handleSend = useCallback(async () => {
    if (!input.trim() || !artInstanceRef.current || isLoading) return;

    const userMessage: ConversationMessage = {
      id: `user-${Date.now()}`, // Consider using UUIDs for production
      role: MessageRole.USER,
      content: input,
      timestamp: Date.now(),
      threadId: threadId,
    };

    // Add user message optimistically
    setMessages(prev => [...prev, userMessage]);
    // Clear streaming ref in case a previous stream was interrupted
    streamingMessageIdRef.current = null;

    const currentInput = input; // Capture input before clearing
    setInput('');
    setIsLoading(true);
    setStatus('Processing request...'); // Initial status, will be updated by observations

    try {
       // Define the runtime provider configuration for this specific call
       const runtimeConfig: RuntimeProviderConfig = {
         providerName: 'openai', // Must match the name registered in createArtInstance config.providers
         modelId: 'gpt-4o', // Specify the desired model for this call
         adapterOptions: {
           apiKey: import.meta.env.VITE_OPENAI_API_KEY || 'YOUR_OPENAI_API_KEY', // Provide API key at runtime
           // Add other OpenAI specific options if needed, e.g., temperature
           // temperature: 0.7
         }
       };

      // Prepare the properties for the agent process call
      // Pass the runtimeConfig via configOverrides. The AgentCore will extract this
      // and pass it within CallOptions to the ReasoningEngine.
      const props: AgentProps = {
        query: currentInput,
        threadId: threadId,
        configOverrides: {
          runtimeProviderConfig: runtimeConfig
        }
      };

      // process() now resolves AFTER the stream is complete and the final message is saved.
      // The UI updates come via the LLMStreamSocket and ConversationSocket.
      const response: AgentFinalResponse = await artInstanceRef.current.process(props);

      console.log("ART process completed. Final Response:", response);
      // Final message display is handled by ConversationSocket subscription

    } catch (error) {
      console.error("Error processing message:", error);
      const errorMessage: ConversationMessage = {
        id: `error-${Date.now()}`,
        role: MessageRole.SYSTEM,
        content: `Error: ${error instanceof Error ? error.message : 'Failed to get response'}`,
        timestamp: Date.now(),
        threadId: threadId,
      };
      setMessages(prev => [...prev, errorMessage]);
      setStatus('Error occurred.');
       streamingMessageIdRef.current = null; // Clear streaming ID on error
    } finally {
      setIsLoading(false); // Set loading false only after process() finishes
      // Status should update to 'Ready.' via PROCESS_END observation if successful
    }
  }, [input, isLoading, threadId]);

  // --- Render Component ---
  return (
    &lt;div className="chatbot-container"&gt;
      &lt;div className="message-list" ref={messageListRef}&gt;
        {messages.map((msg) => (
          &lt;div key={msg.id} className={`message ${msg.role} ${msg.metadata?.streaming ? 'streaming' : ''}`}&gt;
            {/* Simple rendering, consider markdown parsing for content */}
            &lt;pre style={{ whiteSpace: 'pre-wrap', margin: 0, fontFamily: 'inherit' }}&gt;{msg.content}&lt;/pre&gt;
          &lt;/div&gt;
        ))}
      &lt;/div&gt;
      &lt;div className="status-indicator"&gt;{isLoading ? status : 'Ready.'}&lt;/div&gt; {/* Show status while loading */}
      &lt;div className="input-area"&gt;
        &lt;input
          type="text"
          value={input}
          onChange={(e) => setInput(e.target.value)}
          onKeyPress={(e) => e.key === 'Enter' && !isLoading && handleSend()}
          disabled={isLoading || !artInstanceRef.current}
          placeholder={artInstanceRef.current ? "Ask something..." : "Initializing..."}
        /&gt;
        &lt;button onClick={handleSend} disabled={isLoading || !artInstanceRef.current || !input.trim()}&gt;
          {isLoading ? '...' : 'Send'}
        &lt;/button&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  );
};

export default ArtChatbot;

</code></pre>
                    <h4 class="text-lg font-medium mt-6 mb-4 text-gray-800">Explanation of Features Used:</h4>
                    <ol class="list-decimal list-inside space-y-3 text-gray-700 leading-relaxed">
                        <li><strong>Initialization (`createArtInstance`):** Sets up ART with a configured `StorageAdapter` (IndexedDB), registers available `ProviderAdapter` *classes* (OpenAIAdapter) via `ProviderManagerConfig`, specifies the default `agentCore` (PESAgent), and includes a built-in `CalculatorTool`.</li>
                        <li><strong>Conversation Management (`conversationManager.getMessages`):** Loads previous messages from storage when the component mounts, providing history. Messages are saved implicitly by the `PESAgent` after `process` completes.</li>
                        <li><strong>State Management (`StateManager`):** Used internally by ART to load thread configuration (like the `RuntimeProviderConfig`) and potentially save agent state between turns.</li>
                        <li><strong>Provider Management (`ProviderManager`):** Manages the lifecycle of `ProviderAdapter` instances based on the configuration provided during initialization.</li>
                        <li><strong>Reasoning (`PESAgent`, `ReasoningEngine`):** Handles the core logic of understanding the query, planning, and synthesizing the response. The `PESAgent` determines the appropriate `RuntimeProviderConfig` for each LLM call (using overrides or thread defaults), passes it via `CallOptions` to the `ReasoningEngine`. The `ReasoningEngine` gets the correct adapter instance from the `ProviderManager` and handles the streaming interaction.</li>
                        <li><strong>Tools (`CalculatorTool`, `ToolSystem`):** The calculator is available. If the user asks "What is 5*5?", the `PESAgent` should plan to use it, the `ToolSystem` will execute it, and the result will inform the final answer.</li>
                        <li><strong>Storage (`IndexedDBStorageAdapter`):** Ensures conversation history persists even if the user closes and reopens the browser tab.</li>
                        <li><strong>Observations & UI Sockets (`uiSystem`):** The UI subscribes to the `ObservationSocket` for discrete events, the `LLMStreamSocket` for real-time `StreamEvent`s (tokens, metadata, errors, end signals) from the LLM, and the `ConversationSocket` to receive the final, persisted message after streaming is complete.</li>
                    </ol>
                     <p class="mt-6 text-gray-700 leading-relaxed">
                        This component provides a solid foundation, demonstrating the core ART features working together in a practical application, including the new streaming and enhanced observation capabilities.
                    </p>
                </div>

                 <div id="scenario-1-workflow" class="mt-10">
                    <h3 class="text-xl font-medium mb-4 text-gray-800">3.3. Detailed Internal Workflow: `art.process()` with PESAgent</h3>
                    <p class="mb-4 text-gray-700 leading-relaxed">When you call `art.process()` using the default Plan-Execute-Synthesize (PES) agent, a sequence of steps occurs internally to understand your request, potentially use tools, and generate a response. Here’s a breakdown, with both technical details and simpler explanations:</p>
                    <ol class="list-decimal list-inside space-y-4 text-gray-700 leading-relaxed">
                        <li>
                            <strong>Call Received:</strong> <code>PESAgent.process(props)</code> starts.
                            <p class="text-sm text-gray-600 ml-4 italic">In simple terms: The agent receives your query and gets ready to work.</p>
                        </li>
                        <li>
                            <strong>Record Start:</strong> Log <code>PROCESS_START</code> observation.
                            <p class="text-sm text-gray-600 ml-4 italic">In simple terms: The agent makes a note that it has started processing a new request.</p>
                        </li>
                        <li>
                            <strong>Load Context:</strong> Fetch <code>ThreadConfig</code> and <code>AgentState</code> via <code>StateManager</code>. Apply any `configOverrides` from `props`.
                            <p class="text-sm text-gray-600 ml-4 italic">In simple terms: The agent retrieves settings and memory for this conversation, potentially using temporary overrides for this specific request.</p>
                        </li>
                        <li>
                            <strong>Load History:</strong> Fetch recent <code>ConversationMessage</code>s via <code>ConversationManager</code>.
                            <p class="text-sm text-gray-600 ml-4 italic">In simple terms: The agent looks up the recent chat history.</p>
                        </li>
                        <li>
                            <strong>Get Available Tools:</strong> Fetch enabled <code>ToolSchema</code>s via <code>ToolRegistry</code> (using `StateManager` to check permissions).
                            <p class="text-sm text-gray-600 ml-4 italic">In simple terms: The agent checks which tools it can use.</p>
                        </li>
                        <li>
                            <strong>Create Planning Prompt:</strong> Use <code>PromptManager.assemblePrompt(planningBlueprint, planningContext)</code> to combine query, history, system prompt (from `ThreadConfig`), and tool schemas into a standardized <code>ArtStandardPrompt</code> asking the LLM to plan.
                            <p class="text-sm text-gray-600 ml-4 italic">In simple terms: The agent prepares instructions (using a template) asking the AI brain to figure out a plan, including whether tools are needed.</p>
                        </li>
                        <li>
                             <strong>Execute Planning LLM Call & Stream Processing:</strong>
                             <ul class="list-disc list-inside ml-6 space-y-1 text-sm mt-1">
                                 <li>Determine `RuntimeProviderConfig` (from `props.configOverrides` or thread defaults).</li>
                                 <li>Create `CallOptions` including the `providerConfig`, `stream: true`, `callContext: 'AGENT_THOUGHT'`.</li>
                                 <li>Log `LLM_REQUEST` observation.</li>
                                 <li>Call `ReasoningEngine.call(planningPrompt, callOptions)`.</li>
                                 <li>`ReasoningEngine` requests the adapter instance from `ProviderManager`.</li>
                                 <li>`ReasoningEngine` calls the adapter, gets the `AsyncIterable<StreamEvent>`.</li>
                                 <li>Consume the returned `AsyncIterable<StreamEvent>`.</li>
                                 <li>For each `StreamEvent`:
                                     <ul class="list-circle list-inside ml-6 space-y-1 text-xs mt-1">
                                         <li>Push event to `LLMStreamSocket`.</li>
                                         <li>Buffer `TOKEN` data.</li>
                                         <li>Log `LLM_STREAM_METADATA`, `LLM_STREAM_ERROR`, `LLM_STREAM_END` observations.</li>
                                         <li>Aggregate `METADATA`.</li>
                                     </ul>
                                 </li>
                                 <li>`ReasoningEngine` (via generator `finally`) releases the adapter instance back to `ProviderManager`.</li>
                                 <li>Log `LLM_RESPONSE` observation (with aggregated content & metadata) after stream ends.</li>
                             </ul>
                             <p class="text-sm text-gray-600 ml-4 italic mt-1">In simple terms: The agent figures out which AI connection to use, sends the planning instructions, receives the plan piece by piece, broadcasts these pieces to the UI, logs events, collects stats, and ensures the AI connection is released.</p>
                         </li>
                        <li>
                            <strong>Parse Planning Output:</strong> Use <code>OutputParser</code> to extract intent, plan description, and <code>ParsedToolCall[]</code> from the *aggregated* planning LLM response content.
                            <p class="text-sm text-gray-600 ml-4 italic">In simple terms: The agent reads the complete plan (assembled from the streamed tokens) and understands which tools to use.</p>
                        </li>
                        <li>
                            <strong>Record Plan:</strong> Log <code>PLANNING_OUTPUT</code> observation.
                            <p class="text-sm text-gray-600 ml-4 italic">In simple terms: The agent notes down the plan.</p>
                        </li>
                        <li>
                            <strong>Execute Tools (if <code>ParsedToolCall[]</code> is not empty):</strong>
                            <ul class="list-disc list-inside ml-6 space-y-1 text-sm mt-1">
                                <li>Call <code>ToolSystem.executeTools()</code>.</li>
                                <li><code>ToolSystem</code> validates, logs <code>TOOL_START</code>, calls <code>executor.execute()</code>, logs <code>TOOL_END</code>.</li>
                                <li>Log <code>TOOL_EXECUTION_COMPLETE</code> observation.</li>
                            </ul>
                             <p class="text-sm text-gray-600 ml-4 italic mt-1">In simple terms: If needed, the agent runs the tools specified in the plan and records the results.</p>
                        </li>
                        <li>
                            <strong>Create Synthesis Prompt:</strong> Use <code>PromptManager.assemblePrompt(synthesisBlueprint, synthesisContext)</code> to combine original query, plan, tool results, history, and system prompt into an <code>ArtStandardPrompt</code> asking for the final response.
                            <p class="text-sm text-gray-600 ml-4 italic">In simple terms: The agent prepares final instructions (using another template) asking the AI brain to write the actual answer you will see, including any tool results.</p>
                        </li>
                        <li>
                            <strong>Execute Synthesis LLM Call & Stream Processing:</strong>
                            <ul class="list-disc list-inside ml-6 space-y-1 text-sm mt-1">
                                 <li>Determine `RuntimeProviderConfig` (e.g., from `AgentProps.configOverrides` or thread defaults - same as planning step).</li>
                                 <li>Create `CallOptions` including the `providerConfig`, `stream: true`, `callContext: 'FINAL_SYNTHESIS'`.</li>
                                 <li>Log `LLM_REQUEST`.</li>
                                 <li>Call `ReasoningEngine.call(synthesisPrompt, callOptions)`.</li>
                                 <li>`ReasoningEngine` gets adapter from `ProviderManager`.</li>
                                 <li>`ReasoningEngine` calls adapter, gets stream.</li>
                                 <li>Consume the returned `AsyncIterable<StreamEvent>`.</li>
                                 <li>For each `StreamEvent`:
                                     <ul class="list-circle list-inside ml-6 space-y-1 text-xs mt-1">
                                         <li>Push event to `LLMStreamSocket` (only `FINAL_SYNTHESIS_LLM_RESPONSE` tokens are typically displayed by UI).</li>
                                         <li>Buffer `TOKEN` data for the final response.</li>
                                         <li>Log `LLM_STREAM_...` observations.</li>
                                         <li>Aggregate `METADATA`.</li>
                                     </ul>
                                 </li>
                                 <li>`ReasoningEngine` releases adapter instance.</li>
                                 <li>Log `LLM_RESPONSE` observation after stream ends.</li>
                             </ul>
                            <p class="text-sm text-gray-600 ml-4 italic mt-1">In simple terms: The agent figures out which AI connection to use again, sends the final instructions, gets the answer piece by piece, broadcasts it to the UI, logs events, collects stats, and releases the connection.</p>
                        </li>
                        <li>
                            <strong>Parse Synthesis Output:</strong> Use <code>OutputParser</code> to extract the final <code>responseText</code> from the *aggregated* synthesis LLM response content.
                            <p class="text-sm text-gray-600 ml-4 italic">In simple terms: The agent extracts the complete final chat message from the streamed tokens.</p>
                        </li>
                        <li>
                            <strong>Record Synthesis:</strong> Log <code>SYNTHESIS_OUTPUT</code> observation.
                            <p class="text-sm text-gray-600 ml-4 italic">In simple terms: The agent notes down the final answer.</p>
                        </li>
                        <li>
                            <strong>Save History:</strong> Persist user query and final AI response via <code>ConversationManager</code>.
                            <p class="text-sm text-gray-600 ml-4 italic">In simple terms: The agent saves the final exchange to the chat history. (This triggers the `ConversationSocket` notification to the UI).</p>
                        </li>
                        <li>
                            <strong>Save State:</strong> Persist any changes to <code>AgentState</code> via <code>StateManager</code>.
                            <p class="text-sm text-gray-600 ml-4 italic">In simple terms: The agent saves any changes to its internal memory.</p>
                        </li>
                        <li>
                            <strong>Record End:</strong> Log <code>PROCESS_END</code> observation.
                            <p class="text-sm text-gray-600 ml-4 italic">In simple terms: The agent notes it has finished.</p>
                        </li>
                        <li>
                            <strong>Return Result:</strong> Return <code>AgentFinalResponse</code> object, including the final <code>responseText</code> and aggregated `ExecutionMetadata.llmMetadata`.
                            <p class="text-sm text-gray-600 ml-4 italic">In simple terms: The agent sends the final response object (containing the text and stats) back to the application code.</p>
                        </li>
                    </ol>
                </div>
            </section>

            <section id="scenario-2" class="mb-16 p-6 bg-white rounded-lg shadow-lg fade-in-section">
                <h2 class="text-2xl md:text-3xl font-semibold mb-6 border-b pb-3 text-sky-700">4. Scenario 2: Adding a Custom Tool (Intermediate Usage)</h2>
                <p class="mb-5 text-gray-700 leading-relaxed">
                    Now, let's extend our chatbot by adding a custom tool that provides current information like date, time, and approximate location/locale.
                </p>
                 <p class="mb-8 text-gray-700 leading-relaxed"><strong>Goal:</strong> Create a <code>CurrentInfoTool</code> and integrate it into the ART configuration.</p>

                 <div id="scenario-2-simple-explanation" class="mb-10 p-4 bg-blue-50 border border-blue-200 text-blue-800 rounded-md text-sm leading-relaxed">
                     <h3 class="text-lg font-medium mb-2 text-blue-900">4.0 Simplified Explanation for Developers</h3>
                     <p>Imagine ART is like a highly capable smart assistant you've hired for your application. This assistant comes with some built-in abilities (like a calculator), but its real power is that you can easily teach it *new* skills that you create yourself.</p>
                     <ol class="list-decimal list-inside mt-3 space-y-1">
                         <li><strong>Creating Your Custom Skill (Your Tool):</strong> You, as the developer, define the new skill you want the assistant to have. This involves writing the code for that skill (your custom tool) and describing what it does and what information it needs to work. ART provides a standard way to define these skills (an "interface" called `IToolExecutor`). You just need to make sure your skill follows this standard format so the ART assistant can understand it.</li>
                         <li><strong>Giving the Skill to the Assistant:</strong> When you set up the ART assistant for your application (using the `createArtInstance` function from the ART package), you provide it with a configuration. This configuration is like giving the assistant its instructions and resources. Crucially, this configuration includes a list of *all* the skills you want the assistant to have. You add your newly created custom skill to this list, along with any of ART's built-in skills you want to use.</li>
                         <li><strong>The Assistant Learns Your Skill:</strong> When `createArtInstance` runs, the ART framework reads your configuration. It sees the list of skills you provided and adds them to its internal "skill library" (the `ToolRegistry`).</li>
                     </ol>
                     <p class="mt-3">So, to integrate your custom tool without modifying the ART framework's source code:</p>
                     <ul class="list-disc list-inside mt-2 space-y-1">
                         <li>You create your custom tool's code in a file within your application's project structure (like a `tools` folder).</li>
                         <li>Inside that file, you define a class for your tool and make sure it follows the rules defined by ART's `IToolExecutor` interface.</li>
                         <li>In the part of your application where you set up ART (where you call `createArtInstance`), you import the custom tool class you just created.</li>
                         <li>When you call `createArtInstance`, you pass a configuration object. Within this object, there's a `tools` array. You create a new instance of your custom tool class (`new YourCustomTool()`) and include it in this array.</li>
                     </ul>
                     <p class="mt-3">By doing this, you're effectively handing your custom skill to the ART assistant during its setup. ART then knows about your tool and how to use it when needed, all without you having to touch the core ART framework code itself.</p>
                 </div>

                <div id="scenario-2-imports" class="mb-10">
                     <h3 class="text-xl font-medium mb-4 text-gray-800">4.1. Necessary Imports & Explanations</h3>
                     <p class="mb-4 text-gray-700 leading-relaxed">In addition to the imports from Scenario 1, you'll need these specifically for creating a tool:</p>
<pre><code class="language-typescript">// --- ART Tool Creation Imports ---
import {
  // The interface that every tool must implement
  IToolExecutor,
  // The type defining the tool's description, name, and input/output schemas
  ToolSchema,
  // The type defining the structure of the result returned by a tool's execute method
  ToolResult,
  // The type providing context (like threadId, traceId) to the tool's execute method
  ExecutionContext
} from 'art-framework';
</code></pre>
                    <h4 class="text-lg font-medium mt-6 mb-4 text-gray-800">Explanation of Tool Imports:</h4>
                     <div class="space-y-4 text-gray-700 leading-relaxed">
                         <div>
                            <p><strong><code>IToolExecutor</code></strong></p>
                            <p class="mb-2">The blueprint or set of rules your custom skill needs to follow so ART knows how to use it.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                The core interface for creating custom tools. Your tool class must implement this. Key requirements:
                                <ul class="list-disc list-inside ml-4">
                                    <li>Implement a readonly <code>schema</code> property of type <code>ToolSchema</code>.</li>
                                    <li>Implement an <code>async execute(input: any, context: ExecutionContext): Promise&lt;ToolResult&gt;</code> method. This method receives validated <code>input</code> (based on <code>schema.inputSchema</code>) and the <code>context</code> object. It should perform the tool's action and return a <code>Promise</code> resolving to a <code>ToolResult</code>.</li>
                                </ul>
                            </details>
                        </div>
                         <div>
                            <p><strong><code>ToolSchema</code></strong></p>
                            <p class="mb-2">The tool's "instruction manual" for the AI – its name, what it does, and what information it needs to run.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                Interface defining the tool's metadata, used by both the LLM (via prompts) and the <code>ToolSystem</code>. Properties:
                                <ul class="list-disc list-inside ml-4">
                                    <li><code>name: string</code>: The unique function name the LLM will use to call the tool (e.g., "get_current_weather"). Use snake_case.</li>
                                    <li><code>description: string</code>: Detailed explanation for the LLM about the tool's purpose, capabilities, and when it should be used. Crucial for effective tool selection by the LLM.</li>
                                    <li><code>inputSchema: object</code>: A standard JSON Schema object describing the expected structure, types (string, number, boolean, object, array), required fields, and descriptions for the <code>input</code> argument of the <code>execute</code> method. Used by <code>ToolSystem</code> to validate arguments before execution.</li>
                                </ul>
                            </details>
                        </div>
                         <div>
                            <p><strong><code>ToolResult</code></strong></p>
                            <p class="mb-2">The format for the tool's answer – whether it worked, and either the result or an error message.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                Interface defining the object returned by <code>IToolExecutor.execute</code>. Properties:
                                <ul class="list-disc list-inside ml-4">
                                    <li><code>status: 'success' | 'error'</code>: Must indicate the outcome.</li>
                                    <li><code>output?: any</code>: Required if <code>status</code> is 'success'. Contains the result data. Aim for JSON-serializable data (strings, numbers, booleans, arrays, plain objects) so the LLM can easily understand and incorporate it into its response.</li>
                                    <li><code>error?: string</code>: Required if <code>status</code> is 'error'. Provides a descriptive error message for logging and potentially for the LLM to understand the failure.</li>
                                </ul>
                            </details>
                        </div>
                         <div>
                            <p><strong><code>ExecutionContext</code></strong></p>
                            <p class="mb-2">Extra information passed to your tool when it runs, like which chat it's running for, useful for tracking or context-specific logic.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                Interface for the context object passed as the second argument to <code>IToolExecutor.execute</code>. Provides runtime context. Properties:
                                <ul class="list-disc list-inside ml-4">
                                    <li><code>threadId: string</code>: The ID of the conversation thread this execution belongs to.</li>
                                    <li><code>traceId?: string</code>: The ID tracing the entire <code>ArtInstance.process</code> call, useful for correlating logs across multiple steps and tool calls within a single user request.</li>
                                    <li>May contain other properties passed down from the <code>AgentProps</code> or added by the <code>IAgentCore</code> implementation.</li>
                                </ul>
                            </details>
                        </div>
                    </div>
                </div>

                <div id="scenario-2-implementation" class="mb-10">
                    <h3 class="text-xl font-medium mb-4 text-gray-800">4.2. Implementing the <code>CurrentInfoTool</code></h3>
<pre><code class="language-typescript">// src/tools/CurrentInfoTool.ts (or define within the component file for simplicity)

import { IToolExecutor, ToolSchema, ToolResult, ExecutionContext } from 'art-framework';

export class CurrentInfoTool implements IToolExecutor {
  readonly schema: ToolSchema = {
    name: "get_current_info",
    description: "Provides the current date, time, approximate user location (requires permission), and browser language/locale.",
    inputSchema: { // No specific input needed for this tool
      type: "object",
      properties: {},
    }
  };

  async execute(input: any, context: ExecutionContext): Promise&lt;ToolResult&gt; {
    console.log(`Executing CurrentInfoTool, Trace ID: ${context.traceId}`);
    try {
      const now = new Date();
      const dateTimeInfo = {
        date: now.toLocaleDateString(),
        time: now.toLocaleTimeString(),
        timezoneOffset: now.getTimezoneOffset(), // In minutes from UTC
        isoString: now.toISOString(),
      };

      let locationInfo: any = { status: 'permission_denied_or_unavailable' };
      try {
        // Use browser Geolocation API - Requires HTTPS and user permission
        if ('geolocation' in navigator) {
          locationInfo = await new Promise((resolve, reject) => {
            navigator.geolocation.getCurrentPosition(
              (position) => {
                resolve({
                  status: 'success',
                  latitude: position.coords.latitude,
                  longitude: position.coords.longitude,
                  accuracy: position.coords.accuracy, // In meters
                });
              },
              (error) => {
                // Handle errors (PERMISSION_DENIED, POSITION_UNAVAILABLE, TIMEOUT)
                resolve({ status: 'error', code: error.code, message: error.message });
              },
              { timeout: 5000 } // Set a timeout
            );
          });
        }
      } catch (geoError) {
         console.warn("Geolocation API error:", geoError);
         // Error already captured in the promise resolution
      }


      const localeInfo = {
        language: navigator.language, // e.g., "en-US"
        languages: navigator.languages, // Array of preferred languages
      };

      // Note: Getting local currency reliably client-side is complex.
      // We'll just include the locale as a hint.

      return {
        status: "success",
        output: {
          dateTime: dateTimeInfo,
          location: locationInfo,
          locale: localeInfo,
        }
      };
    } catch (error) {
      console.error("CurrentInfoTool Error:", error);
      return { status: "error", error: error instanceof Error ? error.message : "Unknown error fetching current info" };
    }
  }
}
</code></pre>
                    <h4 class="text-lg font-medium mt-6 mb-4 text-gray-800">Explanation:</h4>
                    <ol class="list-decimal list-inside space-y-3 text-gray-700 leading-relaxed">
                        <li><strong>Implement <code>IToolExecutor</code>:</strong> The class declares it follows the tool contract.</li>
                        <li><strong>Define <code>schema</code>:</strong> Provides the name (<code>get_current_info</code>), description, and specifies no required input.</li>
                        <li><strong>Implement <code>execute</code>:</strong>
                            <ul class="list-disc list-inside ml-6 space-y-2 mt-2">
                                <li>Gets the current date/time using the <code>Date</code> object.</li>
                                <li>Attempts to get the location using the browser's <code>navigator.geolocation</code> API. This is asynchronous and requires user permission (and usually HTTPS). It handles success and error cases gracefully.</li>
                                <li>Gets browser language/locale using <code>navigator.language(s)</code>.</li>
                                <li>Bundles all collected information into the <code>output</code> field of a successful <code>ToolResult</code>.</li>
                                <li>Includes error handling for unexpected issues.</li>
                            </ul>
                        </li>
                    </ol>
                </div>

                <div id="scenario-2-integration" class="mb-8">
                    <h3 class="text-xl font-medium mb-4 text-gray-800">4.3. Integrating the Tool into the Chatbot</h3>
                    <p class="mb-4 text-gray-700 leading-relaxed">Modify the ART configuration within the <code>ArtChatbot</code> component's <code>useEffect</code> hook:</p>
<pre><code class="language-typescript">// Inside the useEffect hook in ArtChatbot.tsx

import { CurrentInfoTool } from './tools/CurrentInfoTool'; // Adjust path if needed
import { OpenAIAdapter } from 'art-framework'; // Import adapter class
import { PESAgent } from 'art-framework'; // Import agent core
import { CalculatorTool } from 'art-framework'; // Import built-in tool
import { IndexedDBStorageAdapter } from 'art-framework'; // Import storage adapter

// ... inside initializeArt function ...
          const config = {
            storage: new IndexedDBStorageAdapter({ dbName: 'artWebChatHistory' }), // Pass storage adapter instance
            providers: { // New ProviderManager configuration
              availableProviders: [
                {
                  name: 'openai', // Identifier for this provider setup
                  adapter: OpenAIAdapter, // Pass the adapter CLASS
                }
              ]
            },
            agentCore: PESAgent, // Specify agent core class
            tools: [
                new CalculatorTool(),
                new CurrentInfoTool() // Add an instance of the new tool
            ]
          };

          const instance = await createArtInstance(config);
// ... rest of the initialization ...
</code></pre>
                    <h4 class="text-lg font-medium mt-6 mb-4 text-gray-800">How it Works Now:</h4>
                    <ul class="list-disc list-inside space-y-3 text-gray-700 leading-relaxed">
                         <li><strong>Node 1 (Developer Interface):** You've defined the <code>CurrentInfoTool</code> and told ART about it by adding `new CurrentInfoTool()` to the `tools` array in the configuration. You've also registered available LLM provider adapter *classes* (like `OpenAIAdapter`) in the `providers` configuration.</li>
                        <li><strong>Node 2 (Core Orchestration):** When the user asks something like "What time is it?", the `PESAgent` gathers `PromptContext` (including the query, history, and tool schemas like `get_current_info`). It uses `PromptManager` to create the `ArtStandardPrompt`. It determines the `RuntimeProviderConfig` (specifying provider name, model, API key, etc., likely from `AgentProps.configOverrides` or `ThreadConfig`), creates `CallOptions`, and calls `ReasoningEngine`. The `ReasoningEngine` uses the `RuntimeProviderConfig` to request the appropriate adapter instance (e.g., `OpenAIAdapter`) from the `ProviderManager` and sends the prompt via the adapter's `call` method. The `OutputParser` parses the LLM's response stream. If the LLM plans to use `get_current_info`, the `ToolSystem` finds and executes your tool. The results are used in the synthesis step, which again involves `PromptManager`, determining `RuntimeProviderConfig`, `ReasoningEngine`, `ProviderManager`, and the LLM adapter to generate the final response stream.</li>
                        <li><strong>Node 3 (External Connections):** The `CurrentInfoTool` interacts with browser APIs (`Date`, `navigator.geolocation`, `navigator.language`). The specific `ProviderAdapter` instance (instantiated and managed by `ProviderManager` based on the `RuntimeProviderConfig`) handles communication with the external LLM API (e.g., OpenAI), including processing streaming responses.</li>
                    </ul>
                </div>
            </section>

            <section id="scenario-3" class="mb-16 p-6 bg-white rounded-lg shadow-lg fade-in-section">
                <h2 class="text-2xl md:text-3xl font-semibold mb-6 border-b pb-3 text-sky-700">5. Scenario 3: Adding a Custom Provider Adapter (Anthropic Example)</h2>
                <p class="mb-5 text-gray-700 leading-relaxed">
                    Sometimes, you might want to connect ART to an LLM provider that isn't supported out-of-the-box, like Anthropic's Claude models, or perhaps use a proxy or a self-hosted model with a unique API. This requires creating a custom Provider Adapter.
                </p>
                 <p class="mb-8 text-gray-700 leading-relaxed"><strong>Goal:</strong> Implement a functional <code>AnthropicAdapter</code> using the Anthropic Messages API, supporting streaming.</p>

                 <div id="scenario-3-simple-explanation" class="mb-10 p-4 bg-blue-50 border border-blue-200 text-blue-800 rounded-md text-sm leading-relaxed">
                     <h3 class="text-lg font-medium mb-2 text-blue-900">5.0 Simplified Explanation for Developers</h3>
                     <p>Think of ART as that smart assistant again. This assistant needs to talk to different "AI brains" (Large Language Models like GPT, Gemini, Claude, Ollama, etc.) to get its work done. But each AI brain speaks a slightly different language (their API format).</p>
                     <ol class="list-decimal list-inside mt-3 space-y-1">
                         <li><strong>Creating a Translator (Your Custom Provider Adapter):</strong> If you want the ART assistant to be able to talk to a *new* AI brain it doesn't already know, you need to create a special "translator" for that specific AI brain. This translator is what we call a **Provider Adapter**. Your job is to write the code for this translator. ART provides a standard blueprint (the `ProviderAdapter` interface) that your translator must follow. This blueprint ensures that your translator knows how to:
                             <ul class="list-disc list-inside ml-6 text-xs space-y-1">
                                 <li>Receive instructions from the ART assistant in a standard format (the `ArtStandardPrompt`).</li>
                                 <li>Translate those standard instructions into the specific language the new AI brain understands (its API format).</li>
                                 <li>Send the translated request to the new AI brain.</li>
                                 <li>Receive the response back from the new AI brain (including handling streaming responses).</li>
                                 <li>Translate the AI brain's response back into a standard format that the ART assistant can understand (`AsyncIterable<StreamEvent>`).</li>
                             </ul>
                         </li>
                         <li><strong>Registering the Translator:** When you set up the ART assistant using `createArtInstance`, you provide a configuration (`ProviderManagerConfig`) that lists all the available translators (Provider Adapters) the assistant *could* use. You add your custom adapter *class* to this list, giving it a unique name (e.g., 'anthropic').</li>
                         <li><strong>The Assistant Selects and Uses Your Translator:** When the ART assistant needs to talk to an AI brain (when the `ReasoningEngine` is called), the application logic (usually the `AgentCore`) decides *which* translator to use for that specific call (based on user choice, task requirements, etc., often stored in `ThreadConfig`). It tells the `ProviderManager` the name of the desired translator (e.g., 'anthropic') and any specific options (like API key, model). The `ProviderManager` then creates an instance of your custom adapter class (using the options provided) and gives it to the `ReasoningEngine` to make the call.</li>
                     </ol>
                      <p class="mt-3">So, in simple terms:</p>
                      <p>You create a custom Provider Adapter that acts as a translator for a specific LLM API, making sure it follows ART's standard `ProviderAdapter` blueprint. Then, when you initialize ART in your application, you tell it about your custom adapter *class*. When your application needs to use that specific LLM, it provides the necessary runtime details (like API key and model), and ART's internal `ProviderManager` handles creating and using an instance of your translator. You don't need to change any of ART's core files; you just provide your new component during the setup process.</p>
                      <p class="mt-2">This allows you to connect ART to virtually any LLM provider by writing a single translator for that provider, without altering the core framework.</p>
                     <h4 class="text-lg font-medium mt-4 mb-2 text-blue-900">How to Create and Use Your Custom Adapter:</h4>
                     <ol class="list-decimal list-inside mt-2 space-y-1">
                         <li><strong>Create Your Adapter File:</strong> Create a new file in your application's project, perhaps in a folder like `llm-adapters` or `providers`. For example, `anthropic-adapter.ts`.</li>
                         <li><strong>Import Necessary ART Components:** Inside your adapter file, import the required types and interfaces from `art-framework`. Key imports include:
                             <ul class="list-disc list-inside ml-6 text-xs space-y-1">
                                 <li>`ProviderAdapter`: The interface your adapter class must implement.</li>
                                 <li>`ArtStandardPrompt`: The input format your adapter's `call` method will receive.</li>
                                 <li>`CallOptions`: Contains options for the LLM call (like `stream` and `callContext`).</li>
                                 <li>`StreamEvent`: The format for events yielded by your adapter's `call` method when streaming.</li>
                                 <li>`LLMMetadata`: The format for metadata events.</li>
                             </ul>
                         </li>
                         <li><strong>Implement Your Adapter Class:** Create a class that implements the `ProviderAdapter` interface. This class will contain the logic to:
                              <ul class="list-disc list-inside ml-6 text-xs space-y-1">
                                 <li>Receive the `ArtStandardPrompt` and `CallOptions` in its `call` method.</li>
                                 <li>Translate the `ArtStandardPrompt` into the specific API request format for your LLM provider (e.g., Anthropic).</li>
                                 <li>Make the API call (using `fetch` or a library), handling both non-streaming and streaming responses.</li>
                                 <li>If streaming, parse the provider's stream chunks and yield `StreamEvent` objects (`TOKEN`, `METADATA`, `ERROR`, `END`), ensuring correct `tokenType` based on `callContext` and provider markers.</li>
                                 <li>If not streaming, make the call, parse the full response, and yield a minimal sequence of `StreamEvent`s.</li>
                                 <li>Extract and include `LLMMetadata` in `METADATA` events.</li>
                                 <li>The constructor will receive options (like API keys, base URLs) when the `ProviderManager` instantiates the adapter at runtime, based on the `RuntimeProviderConfig`.</li>
                             </ul>
                         </li>
                         <li><strong>Import and Register in `createArtInstance`:** In the file where you initialize ART, import your custom adapter class. In the configuration object passed to `createArtInstance`, include your adapter class in the `providers.availableProviders` array within the `ProviderManagerConfig`:
<pre><code class="language-typescript">import { createArtInstance, IndexedDBStorageAdapter } from 'art-framework';
import { ProviderManagerConfig, AgentFactoryConfig } from 'art-framework';
import { AnthropicAdapter } from './llm-adapters/anthropic-adapter'; // Import your custom adapter class

const config = {
  storage: { type: 'indexedDB', dbName: 'myAppHistory' },
  providers: {
    availableProviders: [
      {
        name: 'anthropic', // Unique identifier for this provider configuration
        adapter: AnthropicAdapter, // Pass the adapter CLASS
        // isLocal: true // Indicate if it's a local provider (optional, defaults to false)
      },
      // You could also register built-in adapters here, e.g., OpenAIAdapter
    ],
    // Optional global limits
    // maxParallelApiInstancesPerProvider: 5,
    // apiInstanceIdleTimeoutSeconds: 300,
  },
  // ... other config (agentCore, tools)
};

const art = await createArtInstance(config);
</code></pre>
                         </li>
                     </ol>
                     <p class="mt-3">By following these steps, you can seamlessly register your custom LLM provider adapter with ART, allowing the `ProviderManager` to instantiate and use it at runtime.</p>
                 </div>

                 <div id="scenario-3-imports" class="mb-10">
                     <h3 class="text-xl font-medium mb-4 text-gray-800">5.1. Necessary Imports & Explanations</h3>
<pre><code class="language-typescript">// --- ART Provider Adapter Creation Imports ---
import {
  // The base interface for LLM provider adapters
  ProviderAdapter,
  // The core interface for making LLM calls (ProviderAdapter extends this) - Now returns AsyncIterable<StreamEvent>
  ReasoningEngine,
  // Type for standardized prompts (array of messages) ART uses internally
  ArtStandardPrompt,
  ArtStandardMessage,
  // Type for options passed to the LLM call (model params, streaming flags, context, etc.)
  CallOptions,
  // Type for conversation messages (may be part of ArtStandardPrompt context)
  ConversationMessage,
  // Enum for message roles
  MessageRole,
  // Types for streaming output
  StreamEvent,
  LLMMetadata,
  // Type for runtime provider config passed in CallOptions
  RuntimeProviderConfig
} from 'art-framework';

// --- Potentially types from Anthropic SDK if used, or define manually ---
// Example manual types for Anthropic Messages API
interface AnthropicMessage {
  role: 'user' | 'assistant';
  // Content can be string or complex array for tool use/results
  content: string | Array<{ type: 'text', text: string } | { type: 'tool_use', id: string, name: string, input: any } | { type: 'tool_result', tool_use_id: string, content: string, is_error?: boolean }>;
}
interface AnthropicRequestBody {
  model: string;
  messages: AnthropicMessage[];
  system?: string;
  max_tokens: number;
  temperature?: number;
  stop_sequences?: string[];
  stream?: boolean; // Added for streaming
  // Potentially add tool definitions if using Anthropic's native tool support
  // tools?: Array<{ name: string, description: string, input_schema: object }>;
}
// Type for non-streaming or final aggregated response
interface AnthropicResponse {
  content: Array<{ type: 'text', text: string } | { type: 'tool_use', id: string, name: string, input: any }>;
  stop_reason?: string;
  usage?: { input_tokens: number, output_tokens: number };
  // ... other fields
}
</code></pre>
                    <h4 class="text-lg font-medium mt-6 mb-4 text-gray-800">Explanation of Provider Adapter Imports:</h4>
                     <div class="space-y-4 text-gray-700 leading-relaxed">
                         <div>
                             <p><strong><code>ProviderAdapter</code></strong></p>
                             <p class="mb-2">The blueprint for creating a translator between ART's general way of thinking about AI models and the specific way a particular AI provider's API works (like Anthropic).</p>
                             <details>
                                 <summary>Developer Notes</summary>
                                 The interface your custom LLM adapter class must implement. It extends <code>ReasoningEngine</code>, meaning it must primarily implement the <code>call</code> method (which now returns `Promise<AsyncIterable<StreamEvent>>`). It also requires a <code>readonly providerName: string</code> property to identify the adapter (e.g., 'anthropic').
                             </details>
                         </div>
                         <div>
                             <p><strong><code>ReasoningEngine</code></strong></p>
                             <p class="mb-2">Defines the basic capability of making a call to an AI model with a prompt. `ProviderAdapter` builds upon this.</p>
                             <details>
                                 <summary>Developer Notes</summary>
                                 The base interface defining the core `async call(prompt: ArtStandardPrompt, options: CallOptions): Promise<AsyncIterable<StreamEvent>>` method signature (updated for streaming and standardized prompt). Your `ProviderAdapter` implementation provides the concrete logic for this method, typically returning an async generator function.
                             </details>
                         </div>
                         <div>
                             <p><strong><code>ArtStandardPrompt</code> / <code>ArtStandardMessage</code></strong></p>
                             <p class="mb-2">Represents the standardized, provider-agnostic instructions prepared for the AI model by the `PromptManager`. This is now an array of `ArtStandardMessage` objects.</p>
                             <details>
                                 <summary>Developer Notes</summary>
                                 Your adapter's <code>call</code> method receives this standard format (`ArtStandardPrompt`) from the `ReasoningEngine` and is responsible for translating it into the specific message structure and format required by the target LLM provider's API (e.g., mapping roles, handling content types, structuring tool calls/results).
                             </details>
                         </div>
                         <div>
                             <p><strong><code>CallOptions</code></strong></p>
                             <p class="mb-2">Additional settings and information passed along when making the AI call. Crucially includes `providerConfig` (containing the target provider, model, and adapter options), `stream` flag, and `callContext`.</p>
                             <details>
                                 <summary>Developer Notes</summary>
                                 Interface for the options object passed to `ReasoningEngine.call`. Includes `threadId`, `traceId`, `sessionId`, `stream?: boolean`, `callContext?: string`, and `providerConfig: RuntimeProviderConfig`. Your adapter's `call` method uses `providerConfig` to access runtime options and model details, and checks `stream` and `callContext`.
                             </details>
                         </div>
                         <div>
                             <p><strong><code>ConversationMessage</code>, <code>MessageRole</code></strong></p>
                             <p class="mb-2">Needed within the adapter's prompt translation logic to correctly interpret the roles and content within the received `ArtStandardPrompt` and map them to the provider's expected format.</p>
                             <details>
                                 <summary>Developer Notes</summary>
                                 Used within the adapter's `call` method (specifically in the `formatMessages` helper) during the prompt translation step before sending the request to the Anthropic API. System messages, user messages, assistant messages, tool requests, and tool results from the `ArtStandardPrompt` need careful mapping to Anthropic's structure.
                             </details>
                         </div>
                         <div>
                             <p><strong><code>StreamEvent</code>, <code>LLMMetadata</code></strong></p>
                             <p class="mb-2">These are the types your adapter's `call` method will yield via its `AsyncIterable` return value when streaming is enabled.</p>
                             <details>
                                 <summary>Developer Notes</summary>
                                 Your adapter needs to construct <code>StreamEvent</code> objects with the correct <code>type</code>, <code>data</code>, <code>tokenType</code>, and IDs. For <code>METADATA</code> events, the <code>data</code> should conform to the <code>LLMMetadata</code> interface. The adapter must parse the provider's specific stream format (e.g., Server-Sent Events) to generate these standard events.
                             </details>
                         </div>
                     </div>
                 </div>

                 <div id="scenario-3-implementation" class="mb-10">
                    <h3 class="text-xl font-medium mb-4 text-gray-800">5.2. Implementing <code>AnthropicAdapter</code> (with Streaming)</h3>
<pre><code class="language-typescript">// src/adapters/AnthropicAdapter.ts

import {
  ProviderAdapter, ArtStandardPrompt, ArtStandardMessage, CallOptions, MessageRole,
  StreamEvent, LLMMetadata, RuntimeProviderConfig // Import necessary types
} from 'art-framework';

// Example types matching Anthropic Messages API structure
interface AnthropicMessage {
  role: 'user' | 'assistant';
  // Content can be string or complex array for tool use/results
  content: string | Array<{ type: 'text', text: string } | { type: 'tool_use', id: string, name: string, input: any } | { type: 'tool_result', tool_use_id: string, content: string, is_error?: boolean }>;
}
interface AnthropicRequestBody {
  model: string;
  messages: AnthropicMessage[];
  system?: string;
  max_tokens: number;
  temperature?: number;
  stop_sequences?: string[];
  stream?: boolean;
  // Potentially add tool definitions if using Anthropic's native tool support
  // tools?: Array<{ name: string, description: string, input_schema: object }>;
}
// Type for non-streaming or final aggregated response
interface AnthropicResponse {
  content: Array<{ type: 'text', text: string } | { type: 'tool_use', id: string, name: string, input: any }>;
  stop_reason?: string;
  usage?: { input_tokens: number, output_tokens: number };
  // ... other fields
}

// Options expected by the constructor, derived from RuntimeProviderConfig.adapterOptions
interface AnthropicAdapterConstructorOptions {
  apiKey: string;
  model?: string; // Default model if not specified in RuntimeProviderConfig.modelId
  defaultMaxTokens?: number;
  defaultTemperature?: number;
  anthropicVersion?: string;
  // Add other constructor-time options
}

export class AnthropicAdapter implements ProviderAdapter {
  readonly providerName = 'anthropic';
  private options: AnthropicAdapterConstructorOptions; // Store constructor options

  // The constructor receives options when the ProviderManager instantiates the adapter.
  // These options come from RuntimeProviderConfig.adapterOptions.
  constructor(options: AnthropicAdapterConstructorOptions) {
    if (!options.apiKey) {
      throw new Error(`Anthropic adapter requires an apiKey in adapterOptions.`);
    }
    this.options = {
        ...options,
        // Set defaults if not provided in constructor options
        defaultMaxTokens: options.defaultMaxTokens ?? 1024,
        defaultTemperature: options.defaultTemperature ?? 0.7,
        anthropicVersion: options.anthropicVersion ?? '2023-06-01'
    };
  }

  // Helper to format ArtStandardPrompt messages to Anthropic format
  private formatMessages(prompt: ArtStandardPrompt): { messages: AnthropicMessage[], system?: string } {
    let systemPrompt: string | undefined = undefined;
    const anthropicMessages: AnthropicMessage[] = [];

    let lastRole: 'user' | 'assistant' | null = null;
    for (const message of prompt) {
      if (message.role === 'system') {
        systemPrompt = String(message.content); // Use the last system message content
        continue;
      }

      let role: 'user' | 'assistant';
      let content: AnthropicMessage['content'];

      // Map ART roles to Anthropic roles and content structure
      if (message.role === 'user') {
        role = 'user';
        content = String(message.content); // Simple user text
      } else if (message.role === 'assistant') {
        role = 'assistant';
        content = String(message.content); // Simple assistant text
      } else if (message.role === 'tool_request' && Array.isArray(message.content)) {
        // Assistant requests tool use
        role = 'assistant';
        content = message.content.map((toolCall: any) => ({
          type: 'tool_use',
          id: toolCall.id || `toolu_${Date.now()}`, // Ensure an ID exists
          name: toolCall.function.name,
          // Anthropic expects input as object, ART standard stores arguments string
          input: typeof toolCall.function.arguments === 'string' ? JSON.parse(toolCall.function.arguments) : toolCall.function.arguments,
        }));
      } else if (message.role === 'tool_result') {
        // User provides tool result
        role = 'user';
        // Tool results need to be wrapped in the specific Anthropic structure
        content = [{
          type: 'tool_result',
          tool_use_id: message.tool_call_id!, // tool_call_id is required
          content: typeof message.content === 'object' ? JSON.stringify(message.content) : String(message.content),
          // Optionally indicate errors
          // is_error: message.metadata?.isError // Assuming metadata might contain error flag
        }];
      } else {
        console.warn(`AnthropicAdapter: Skipping message with unhandled role: ${message.role}`);
        continue; // Skip unhandled roles
      }

      // Merge consecutive messages of the same mapped role if needed by Anthropic API rules
      if (role === lastRole) {
        console.warn(`AnthropicAdapter: Consecutive ${role} messages detected. Merging content.`);
        const lastMsg = anthropicMessages.pop()!;
        // Simple string concatenation for text, array concat for tool results/requests
        if (typeof lastMsg.content === 'string' && typeof content === 'string') {
          lastMsg.content = `${lastMsg.content}\n${content}`;
        } else if (Array.isArray(lastMsg.content) && Array.isArray(content)) {
          lastMsg.content = [...lastMsg.content, ...content];
        } else {
          // Handle mixed types? Convert to string array? Depends on API tolerance.
          // Forcing array structure:
          const lastContentArray = Array.isArray(lastMsg.content) ? lastMsg.content : [{ type: 'text', text: lastMsg.content }];
          const currentContentArray = Array.isArray(content) ? content : [{ type: 'text', text: String(content) }];
          lastMsg.content = [...lastContentArray, ...currentContentArray];
        }
        anthropicMessages.push(lastMsg);
      } else {
        anthropicMessages.push({ role, content });
        lastRole = role;
      }
    }

    // Ensure the conversation starts with a user message if possible (Anthropic requirement)
    if (anthropicMessages.length > 0 && anthropicMessages[0].role === 'assistant') {
        console.warn("AnthropicAdapter: Conversation starts with assistant message, prepending empty user message.");
        anthropicMessages.unshift({ role: 'user', content: "(Context setting: Conversation starts here)" });
    }
     // Ensure the conversation ends with a user message if the last was assistant (Anthropic requirement)
     if (anthropicMessages.length > 0 && anthropicMessages[anthropicMessages.length - 1].role === 'assistant') {
          console.warn("AnthropicAdapter: Conversation ends with assistant message. This might lead to issues if expecting immediate user input next.");
          // Depending on the use case, might need to append a placeholder user message or handle differently.
     }


    return { messages: anthropicMessages, system: systemPrompt };
  }

  // Updated to accept ArtStandardPrompt and return AsyncIterable<StreamEvent>
  async call(prompt: ArtStandardPrompt, options: CallOptions): Promise<AsyncIterable<StreamEvent>> {
    const { threadId, traceId = `anthropic-trace-${Date.now()}`, sessionId, stream, callContext, providerConfig } = options;

    const { messages, system } = this.formatMessages(prompt);
    // Model should primarily come from the RuntimeProviderConfig used for this call
    const modelToUse = providerConfig.modelId || this.options.model || 'claude-3-5-sonnet-20240620'; // Prefer model from runtime config
    // API key comes from constructor options (originally from RuntimeProviderConfig.adapterOptions)
    const apiKey = this.options.apiKey;

    // Other parameters can come from adapter defaults or RuntimeProviderConfig.adapterOptions
    // Note: providerConfig.adapterOptions contains the options passed during ProviderManager.getAdapter
    const adapterOpts = providerConfig.adapterOptions || {};
    const maxTokens = adapterOpts.max_tokens ?? this.options.defaultMaxTokens!;
    const temperature = adapterOpts.temperature ?? this.options.defaultTemperature!;
    const stopSequences = adapterOpts.stop_sequences; // Allow override from runtime config

    const requestBody: AnthropicRequestBody = {
      model: modelToUse,
      messages: messages,
      max_tokens: maxTokens,
      temperature: temperature,
    };

    if (system) {
      requestBody.system = system;
    }
    if (stopSequences) {
      requestBody.stop_sequences = stopSequences;
    }
    if (stream) {
        requestBody.stream = true;
    }
    // Add tool definitions to requestBody.tools if using Anthropic's native tool support
    // based on tools passed in options or derived from prompt context.

    const apiUrl = 'https://api.anthropic.com/v1/messages';

    const generator = async function*(this: AnthropicAdapter): AsyncIterable<StreamEvent> {
        try {
            const response = await fetch(apiUrl, {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                    'x-api-key': apiKey, // Use API key from constructor options
                    'anthropic-version': this.options.anthropicVersion!,
                    // 'anthropic-beta': 'tools-2024-04-04', // Enable if using native tool support
                },
                body: JSON.stringify(requestBody),
            });

            if (!response.ok) {
                const errorBody = await response.text();
                console.error(`Anthropic API Error (${response.status}): ${errorBody}`);
                yield { type: 'ERROR', data: new Error(`Anthropic API Error (${response.status}): ${errorBody}`), threadId, traceId, sessionId };
                yield { type: 'END', data: null, threadId, traceId, sessionId }; // Ensure END is yielded
                return;
            }

            // --- Handle Streaming Response ---
            if (stream && response.body) {
                const reader = response.body.pipeThrough(new TextDecoderStream()).getReader();
                let buffer = '';
                let messageStopReason: string | null = null;
                let finalUsageData: any = null;
                let thinkingTokens = 0; // Track thinking tokens if possible/needed

                while (true) {
                    const { value, done } = await reader.read();
                    if (done) break;

                    buffer += value;
                    const lines = buffer.split('\n');
                    buffer = lines.pop() || ''; // Keep incomplete line

                    for (const line of lines) {
                        if (line.startsWith('data: ')) {
                            const dataContent = line.substring(6).trim();
                            try {
                                const jsonData = JSON.parse(dataContent);
                                const type = jsonData.type;

                                // Determine tokenType based on callContext
                                const tokenTypeBase = callContext === 'AGENT_THOUGHT' ? 'AGENT_THOUGHT' : 'FINAL_SYNTHESIS';

                                if (type === 'content_block_delta' && jsonData.delta?.type === 'text_delta') {
                                    const textDelta = jsonData.delta.text;
                                    // Anthropic stream doesn't easily distinguish thinking vs response within a call
                                    // Rely solely on callContext for now.
                                    const tokenType = `${tokenTypeBase}_LLM_RESPONSE`;
                                    yield { type: 'TOKEN', data: textDelta, threadId, traceId, sessionId, tokenType: tokenType as StreamEvent['tokenType'] };
                                } else if (type === 'message_start') {
                                    finalUsageData = jsonData.message?.usage ?? finalUsageData;
                                } else if (type === 'message_delta') {
                                    finalUsageData = { ...(finalUsageData ?? {}), ...jsonData.usage };
                                    messageStopReason = jsonData.delta?.stop_reason ?? messageStopReason;
                                } else if (type === 'message_stop') {
                                    // Stream finished signal from Anthropic
                                    // Metadata is typically sent in the final 'message_stop' or derived from previous events
                                    if (finalUsageData || messageStopReason) {
                                        const metadata: LLMMetadata = {
                                            inputTokens: finalUsageData?.input_tokens,
                                            outputTokens: finalUsageData?.output_tokens,
                                            thinkingTokens: thinkingTokens > 0 ? thinkingTokens : undefined, // Include if tracked
                                            stopReason: messageStopReason,
                                            providerRawUsage: finalUsageData,
                                            traceId: traceId,
                                        };
                                        yield { type: 'METADATA', data: metadata, threadId, traceId, sessionId };
                                    }
                                    yield { type: 'END', data: null, threadId, traceId, sessionId };
                                    return; // Exit generator
                                }
                                // Handle tool_use deltas if needed (e.g., streaming arguments)
                                else if (type === 'content_block_start' && jsonData.content_block?.type === 'tool_use') {
                                    // Could yield a specific event for UI to indicate tool use start
                                } else if (type === 'content_block_stop') {
                                    // Tool use block finished
                                }

                            } catch (parseError: any) {
                                console.warn(`Failed to parse Anthropic stream chunk: ${dataContent}`, parseError);
                                yield { type: 'ERROR', data: new Error(`Stream parse error: ${parseError.message}`), threadId, traceId, sessionId };
                            }
                        }
                    }
                }
                // If loop finishes without message_stop (unlikely but possible), yield END
                yield { type: 'END', data: null, threadId, traceId, sessionId };

            // --- Handle Non-Streaming Response ---
            } else {
                const responseData: AnthropicResponse = await response.json();
                let responseContentText = '';
                let toolUseCalls: any[] = []; // Collect tool calls if any

                 if (responseData.content) {
                     for (const block of responseData.content) {
                         if (block.type === 'text') {
                             responseContentText += block.text;
                         } else if (block.type === 'tool_use') {
                             // Collect tool use information if needed for non-streaming response handling
                             toolUseCalls.push({ id: block.id, name: block.name, input: block.input });
                         }
                     }
                 }

                 // Determine tokenType based on callContext
                 const tokenTypeBase = callContext === 'AGENT_THOUGHT' ? 'AGENT_THOUGHT' : 'FINAL_SYNTHESIS';
                 const tokenType = `${tokenTypeBase}_LLM_RESPONSE`;

                 // Yield a single TOKEN event with the full text content
                 // If tool calls occurred, the agent core needs to handle them based on this response
                 yield { type: 'TOKEN', data: responseContentText, threadId, traceId, sessionId, tokenType: tokenType as StreamEvent['tokenType'] };
                 // Optionally yield a separate event for tool calls if needed by agent logic
                 // if (toolUseCalls.length > 0) yield { type: 'TOOL_CALLS', data: toolUseCalls, ... }

                // Yield METADATA
                const usage = responseData.usage;
                if (usage || responseData.stop_reason) {
                    const metadata: LLMMetadata = {
                        inputTokens: usage?.input_tokens,
                        outputTokens: usage?.output_tokens,
                        stopReason: responseData.stop_reason,
                        providerRawUsage: usage,
                        traceId: traceId,
                    };
                    yield { type: 'METADATA', data: metadata, threadId, traceId, sessionId };
                }
                // Yield END signal
                yield { type: 'END', data: null, threadId, traceId, sessionId };
            }

        } catch (error: any) {
            console.error(`${this.providerName} adapter error in generator:`, error);
            yield { type: 'ERROR', data: error instanceof Error ? error : new Error(String(error)), threadId, traceId, sessionId };
            yield { type: 'END', data: null, threadId, traceId, sessionId }; // Ensure END is yielded even after error
        }
    }.bind(this); // Bind the generator function to the class instance

    return generator(); // Return the async generator
  }
}
```</pre>
                    <h4 class="text-lg font-medium mt-6 mb-4 text-gray-800">Explanation:</h4>
                    <ol class="list-decimal list-inside space-y-3 text-gray-700 leading-relaxed">
                        <li><strong>Implements `ProviderAdapter`:** Adheres to the updated contract.</li>
                        <li><strong>Constructor:** Takes `AnthropicAdapterOptions` (like API key). These options are provided by the `ProviderManager` when it instantiates the adapter based on `RuntimeProviderConfig.adapterOptions`.</li>
                        <li><strong>`formatMessages` Helper (Updated):**
                            <ul class="list-disc list-inside ml-6 space-y-2 mt-2">
                                <li>Accepts `ArtStandardPrompt` as input.</li>
                                <li>Maps ART standard roles (`system`, `user`, `assistant`, `tool_request`, `tool_result`) to Anthropic roles (`user`, `assistant`) and content structures (including `tool_use` and `tool_result` block types).</li>
                                <li>Handles merging consecutive messages of the same mapped role.</li>
                                <li>Ensures conversation starts with a `user` message.</li>
                            </ul>
                        </li>
                        <li><strong>`call` Method (Updated):**
                            <ul class="list-disc list-inside ml-6 space-y-2 mt-2">
                                <li>Accepts `ArtStandardPrompt` and `CallOptions` (which includes `providerConfig`).</li>
                                <li>Returns `Promise<AsyncIterable<StreamEvent>>`.</li>
                                <li>Uses an `async function*` generator to yield `StreamEvent`s.</li>
                                <li>Calls `formatMessages` to translate the prompt.</li>
                                <li>Determines model, API key, and other parameters primarily from the options set in the constructor (`this.options`, derived from `RuntimeProviderConfig.adapterOptions`) and the `providerConfig` passed in `CallOptions`.</li>
                                <li>Constructs the Anthropic API request body, including `stream: true` if requested in `CallOptions`.</li>
                                <li>Makes the `fetch` request.</li>
                                <li>**Streaming Logic:** If `stream` is true and `response.body` exists:
                                    <ul>
                                        <li>Uses `TextDecoderStream` to read the response body.</li>
                                        <li>Parses Server-Sent Events (SSE) format typical of Anthropic streams.</li>
                                        <li>Yields `TOKEN` events for `content_block_delta` -> `text_delta`, determining `tokenType` based on `callContext`.</li>
                                        <li>Yields `METADATA` events based on `message_delta` or `message_stop`.</li>
                                        <li>Handles other stream event types (`message_start`, `message_stop`, `content_block_start`, `content_block_stop`).</li>
                                        <li>Yields `ERROR` events on API or parsing errors.</li>
                                        <li>Yields `END` event when the stream completes or stops.</li>
                                    </ul>
                                </li>
                                 <li>**Non-Streaming Logic:** If `stream` is false:
                                    <ul>
                                        <li>Reads the full JSON response.</li>
                                        <li>Extracts text content and any `tool_use` blocks.</li>
                                        <li>Yields a single `TOKEN` event with the aggregated text.</li>
                                        <li>Yields a `METADATA` event with usage info.</li>
                                        <li>Yields the `END` event.</li>
                                    </ul>
                                </li>
                                <li>Includes robust error handling within the generator.</li>
                            </ul>
                        </li>
                    </ol>
                 </div>

                 <div id="scenario-3-integration" class="mb-8">
                    <h3 class="text-xl font-medium mb-4 text-gray-800">5.3. Integrating the <code>AnthropicAdapter</code></h3>
                     <p class="mb-4 text-gray-700 leading-relaxed">Integrating a custom adapter is straightforward with the `ProviderManager`. You simply register the adapter *class* in the `ProviderManagerConfig` when calling `createArtInstance`. ART handles the instantiation when needed.</p>
<pre><code class="language-typescript">// --- Integration Example ---
import { AnthropicAdapter } from './adapters/AnthropicAdapter';
// Import other necessary components (Storage, Agent Core, Tools)
import { createArtInstance, ProviderManagerConfig, AgentFactoryConfig, IndexedDBStorageAdapter, PESAgent, CalculatorTool, ArtInstance, RuntimeProviderConfig } from 'art-framework';

async function setupWithCustomAdapter(): Promise&lt;ArtInstance&gt; {

    // Define the ProviderManagerConfig, including the custom adapter
    const providerConfig: ProviderManagerConfig = {
      availableProviders: [
        {
          name: 'anthropic', // Unique identifier for this provider configuration
          adapter: AnthropicAdapter, // Pass the adapter CLASS
        },
        // You could also register built-in adapters here if needed
        // { name: 'openai', adapter: OpenAIAdapter },
      ],
      // Optional global limits
      // maxParallelApiInstancesPerProvider: 2,
    };

    // Define the overall ART configuration
    const config: AgentFactoryConfig = {
      storage: new IndexedDBStorageAdapter({ dbName: 'artWithAnthropic' }), // Pass storage adapter instance
      providers: providerConfig, // Pass the provider config
      agentCore: PESAgent, // Use the default agent core
      tools: [new CalculatorTool()], // Include any tools
    };

    // Create the ART instance
    const art = await createArtInstance(config);
    console.log("ART instance created with Anthropic adapter registered.");
    return art;
}

// --- Runtime Usage (Conceptual) ---

async function useAnthropic(art: ArtInstance, threadId: string, query: string) {
    // Define the runtime configuration for this specific call
    const runtimeConfig: RuntimeProviderConfig = {
        providerName: 'anthropic', // Must match the name registered in ProviderManagerConfig
        modelId: 'claude-3-opus-20240229', // Specify the desired model
        adapterOptions: {
            apiKey: 'YOUR_ANTHROPIC_API_KEY', // Provide necessary options for the adapter constructor
            // Add other Anthropic-specific options if needed
            // defaultTemperature: 0.5
        }
    };

    // Persist this choice for the thread (optional but common)
    await art.stateManager.setThreadConfigValue(threadId, 'runtimeProviderConfig', runtimeConfig);

    // Make the call - the agent core will load the runtimeProviderConfig from state
    // Alternatively, pass it directly via configOverrides:
    // const response = await art.process({ query, threadId, configOverrides: { runtimeProviderConfig: runtimeConfig } });
    const response = await art.process({ query, threadId });
    console.log("Response from Anthropic:", response.responseText);
}

</code></pre>
                    <h4 class="text-lg font-medium mt-6 mb-4 text-gray-800">How it Works Now:</h4>
                    <ol class="list-decimal list-inside space-y-3 text-gray-700 leading-relaxed">
                        <li><strong>Registration:** `createArtInstance` receives the `ProviderManagerConfig` which includes the `AnthropicAdapter` class associated with the name 'anthropic'. The `ProviderManager` is initialized with this mapping.</li>
                        <li><strong>Runtime Selection:** When `art.process` is called, the `PESAgent` (or whichever `IAgentCore` is used) loads the `runtimeProviderConfig` from the `ThreadConfig` (which was set by the application, e.g., in `useAnthropic`).</li>
                        <li><strong>Instantiation:** The `PESAgent` passes this `runtimeProviderConfig` within `CallOptions` to the `ReasoningEngine`. The `ReasoningEngine` calls `providerManager.getAdapter(runtimeConfig)`.</li>
                        <li><strong>ProviderManager Logic:** The `ProviderManager` finds the registered class (`AnthropicAdapter`) for the name 'anthropic'. It creates a *new instance* of `AnthropicAdapter`, passing `runtimeConfig.adapterOptions` (containing the API key, etc.) to its constructor. It manages the lifecycle of this instance (pooling, caching based on config).</li>
                        <li><strong>Execution:** The `ReasoningEngine` receives the managed adapter instance and calls its `call` method with the prompt and options. The `AnthropicAdapter` instance uses its configured options (API key) to communicate with the Anthropic API.</li>
                        <li><strong>Release:** After the `call` completes (or the stream ends), the `ReasoningEngine` releases the adapter instance back to the `ProviderManager`.</li>
                    </ol>
                    <p class="mt-4 text-gray-700 leading-relaxed">This approach cleanly separates adapter implementation from instantiation and configuration, allowing flexible runtime selection and management of multiple providers.</p>
                 </div>
            </section>

             <section id="scenario-4" class="mb-16 p-6 bg-white rounded-lg shadow-lg fade-in-section">
                <h2 class="text-2xl md:text-3xl font-semibold mb-6 border-b pb-3 text-sky-700">6. Scenario 4: Adding a Custom Storage Adapter (DuckDB WASM Example)</h2>
                 <p class="mb-5 text-gray-700 leading-relaxed">
                    Let's explore using DuckDB WASM as a storage backend. DuckDB is an in-process analytical data management system, and its WASM version allows running it directly in the browser. This could enable more powerful local data storage and querying, including potential vector similarity search for RAG-like capabilities, compared to basic <code>localStorage</code> or <code>IndexedDB</code>.
                </p>
                 <p class="mb-5 text-gray-700 leading-relaxed"><strong>Goal:</strong> Implement a skeleton <code>DuckDBWasmAdapter</code> demonstrating basic CRUD and conceptual vector storage/search.</p>

                  <div id="scenario-4-simple-explanation" class="mb-10 p-4 bg-blue-50 border border-blue-200 text-blue-800 rounded-md text-sm leading-relaxed">
                     <h3 class="text-lg font-medium mb-2 text-blue-900">6.0 Simplified Explanation for Developers</h3>
                     <p>Imagine ART, our smart assistant, needs a place to keep its notes and memories (like conversation history or agent state). By default, it might use a simple notebook (IndexedDB) or just remember things short-term (in-memory). But you want it to use a more robust, cloud-based filing cabinet (like Supabase) for long-term storage, while still using a quick notepad (in-memory) for temporary notes to speed things up.</p>
                     <ol class="list-decimal list-inside mt-3 space-y-1">
                         <li><strong>Creating Your Filing Cabinet Connector (Your Custom Supabase Adapter):</strong> You need to build a special connector that knows how to talk to your Supabase filing cabinet. This connector is your custom **Storage Adapter**. You'll write code that implements ART's standard `StorageAdapter` blueprint. This blueprint requires your connector to have methods for basic filing operations:
                             <ul class="list-disc list-inside ml-6 text-xs space-y-1">
                                 <li>`get`: How to find a specific note in the filing cabinet.</li>
                                 <li>`set`: How to save or update a note in the filing cabinet.</li>
                                 <li>`delete`: How to throw away a note.</li>
                                 <li>`query`: How to find multiple notes based on certain criteria.</li>
                                 <li>`init` (optional): How to set up the connection to the filing cabinet when ART starts.</li>
                                 <li>`clearCollection` / `clearAll` (optional): How to empty specific drawers or the whole cabinet.</li>
                             </ul>
                             Your code for the Supabase adapter will use the Supabase client library to perform these operations against your Supabase database.
                         </li>
                         <li><strong>Setting Up the Quick Notepad (Using InMemoryStorageAdapter):</strong> ART already comes with a simple in-memory notepad (`InMemoryStorageAdapter`). This adapter is very fast because it just keeps notes in the assistant's short-term memory, but the notes are lost when the assistant is turned off (the browser tab is closed).</li>
                         <li><strong>Connecting the Notepad and the Filing Cabinet (Creating a Caching Adapter):</strong> This is where you get clever. You can create *another* custom adapter, let's call it `CachingStorageAdapter`. This adapter won't talk directly to a storage system itself. Instead, it will *use* both the `InMemoryStorageAdapter` (the notepad) and your `SupabaseAdapter` (the filing cabinet connector).
                              <ul class="list-disc list-inside ml-6 text-xs space-y-1">
                                 <li>When ART asks the `CachingStorageAdapter` to `get` a note, it first checks the notepad (`InMemoryStorageAdapter`). If the note is there, it returns it quickly.</li>
                                 <li>If the note is *not* in the notepad, the `CachingStorageAdapter` then asks the filing cabinet connector (`SupabaseAdapter`) to `get` it from Supabase. If found, it returns the note *and* saves a copy in the notepad for next time.</li>
                                 <li>When ART asks the `CachingStorageAdapter` to `set` or `delete` a note, it performs the operation on *both* the notepad (`InMemoryStorageAdapter`) and the filing cabinet connector (`SupabaseAdapter`) to keep them in sync.</li>
                                 <li>The `query` method might be more complex, potentially querying Supabase and then populating the cache.</li>
                             </ul>
                         </li>
                          <li><strong>Giving the Combined Setup to the Assistant:</strong> When you set up the ART assistant using `createArtInstance`, you provide your `CachingStorageAdapter` as the main `storage` component in the configuration. Your `CachingStorageAdapter` instance will be created with instances of the `InMemoryStorageAdapter` and your `SupabaseAdapter` inside it.</li>
                     </ol>
                      <p class="mt-3">So, to use Supabase with in-memory caching:</p>
                      <ul class="list-disc list-inside mt-2 space-y-1">
                         <li>You create a custom `SupabaseAdapter` class that implements ART's `StorageAdapter` interface and talks to your Supabase database.</li>
                         <li>You create a `CachingStorageAdapter` class that also implements `StorageAdapter`. Its constructor takes instances of a primary storage adapter (your `SupabaseAdapter`) and a cache adapter (`InMemoryStorageAdapter`). Its methods implement the caching logic (read from cache, fallback to primary, write to both).</li>
                         <li>In your application's setup code, you create instances: `const supabaseAdapter = new SupabaseAdapter(...)`, `const inMemoryAdapter = new InMemoryStorageAdapter()`, `const cachingAdapter = new CachingStorageAdapter(supabaseAdapter, inMemoryAdapter)`.</li>
                         <li>You pass the `cachingAdapter` instance in the `storage` part of the configuration when calling `createArtInstance`.</li>
                     </ul>
                     <p class="mt-3">You don't need to change any of ART's core files. You build custom components that adhere to ART's standard interfaces and wire them together during your application's initialization.</p>
                     <h4 class="text-lg font-medium mt-4 mb-2 text-blue-900">How to Create and Use Your Custom Storage Adapter:</h4>
                      <ol class="list-decimal list-inside mt-2 space-y-1">
                         <li><strong>Create Your Adapter File(s):</strong> Create new file(s) in your application's project, perhaps in a folder like `storage-adapters` or `data`. For example, `supabase-adapter.ts` and `caching-adapter.ts`.</li>
                         <li><strong>Import Necessary ART Components:** Inside your adapter files, import the required types and interfaces from `art-framework`. Key imports for a storage adapter include:
                             <ul class="list-disc list-inside ml-6 text-xs space-y-1">
                                 <li>`StorageAdapter`: The interface your adapter class must implement.</li>
                                 <li>`FilterOptions`: The type for query options.</li>
                                 <li>You might also need types for the data you are storing (e.g., `ConversationMessage`, `AgentState`, `Observation`) if you want type safety within your adapter, although the `StorageAdapter` interface uses generic types (`<T>`).</li>
                             </ul>
                         </li>
                         <li><strong>Implement Your Adapter Class(es):** Create the class(es) that implement the `StorageAdapter` interface.
                             <ul class="list-disc list-inside ml-6 text-xs space-y-1">
                                 <li>For the `SupabaseAdapter`, implement the `get`, `set`, `delete`, and `query` methods using the Supabase client library to interact with your database. Implement `init` if you need to establish the Supabase connection asynchronously.</li>
                                 <li>For the `CachingStorageAdapter`, implement the `get`, `set`, `delete`, and `query` methods by coordinating calls to the injected primary and cache adapters (e.g., check cache on `get`, write to both on `set`).</li>
                             </ul>
                         </li>
                         <li><strong>Import and Pass to `createArtInstance`:** In the file where you initialize ART, import your custom adapter class(es). In the configuration object passed to `createArtInstance`, create instances of your custom adapters and pass the top-level adapter (e.g., your `CachingStorageAdapter`) in the `storage` part:
<pre><code class="language-typescript">import { createArtInstance, InMemoryStorageAdapter, OpenAIAdapter, ProviderManagerConfig } from 'art-framework';
import { SupabaseAdapter } from './storage-adapters/supabase-adapter'; // Import your Supabase adapter
import { CachingStorageAdapter } from './storage-adapters/caching-adapter'; // Import your Caching adapter

// Assuming SupabaseAdapter constructor takes options like URL and Key
const supabaseAdapter = new SupabaseAdapter({ url: 'YOUR_SUPABASE_URL', apiKey: 'YOUR_SUPABASE_API_KEY' });
const inMemoryAdapter = new InMemoryStorageAdapter(); // Use the built-in in-memory adapter

// Instantiate your caching adapter with the primary and cache adapters
const cachingAdapter = new CachingStorageAdapter(supabaseAdapter, inMemoryAdapter);

// Define the ProviderManagerConfig
const providerConfig: ProviderManagerConfig = {
  availableProviders: [
    {
      name: 'openai', // Unique identifier for this provider configuration
      adapter: OpenAIAdapter, // The adapter class
      // isLocal: false (default)
    },
    // Add other providers like Ollama, Anthropic, etc. here
  ],
  // Optional global limits
  // maxParallelApiInstancesPerProvider: 5,
  // apiInstanceIdleTimeoutSeconds: 300,
};

const config = {
  storage: cachingAdapter, // Pass the caching adapter instance
  providers: providerConfig, // Pass the ProviderManagerConfig
  // ... other config (agentCore, tools)
};

const art = await createArtInstance(config);
</code></pre>
                        </li>
                    </ol>
                      <p class="mt-3">By following these steps, you can seamlessly integrate your custom storage solution(s) with ART without modifying the framework's core code.</p>

                 </div>

                 <p class="mb-8 p-4 bg-yellow-50 border border-yellow-200 text-yellow-800 rounded-md text-sm leading-relaxed">
                    <strong>Disclaimer:</strong> Integrating DuckDB WASM is significantly more complex than `localStorage` or `IndexedDB`. It involves asynchronous initialization, managing WASM bundles, understanding SQL, and potentially handling vector embeddings and similarity calculations. This example provides a conceptual structure.
                </p>

                 <div id="scenario-4-imports" class="mb-10">
                    <h3 class="text-xl font-medium mb-4 text-gray-800">6.1. Necessary Imports & Explanations</h3>
<pre><code class="language-typescript">// --- ART Storage Adapter Creation Imports ---
import {
  // The interface that a custom storage adapter must implement
  StorageAdapter,
  // Type defining options for querying data (filtering, sorting, limits)
  FilterOptions
} from 'art-framework';

// --- DuckDB WASM Imports ---
// You would typically install @duckdb/duckdb-wasm
import * as duckdb from '@duckdb/duckdb-wasm';
// Import specific types if needed, e.g., from duckdb-wasm
// import { AsyncDuckDB, AsyncDuckDBConnection } from '@duckdb/duckdb-wasm';

// --- Vector Embedding Imports (Conceptual) ---
// You would need a library or function to generate embeddings
// e.g., using Transformers.js or calling an embedding API
// import { pipeline } from '@xenova/transformers'; // Example
// const extractor = await pipeline('feature-extraction', 'Xenova/all-MiniLM-L6-v2');
// async function getEmbedding(text: string): Promise<number[]> {
//   const output = await extractor(text, { pooling: 'mean', normalize: true });
//   return Array.from(output.data as Float32Array);
// }
</code></pre>
                    <h4 class="text-lg font-medium mt-6 mb-4 text-gray-800">Explanation of Storage Adapter Imports:</h4>
                    <div class="space-y-4 text-gray-700 leading-relaxed">
                        <div>
                            <p><strong><code>StorageAdapter</code></strong></p>
                            <p class="mb-2">The blueprint for creating a custom way for ART to save and load its data (like chat history or agent memory) using DuckDB WASM.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                The interface your custom storage class must implement. Requires implementing methods for basic CRUD operations:
                                <ul class="list-disc list-inside ml-4">
                                    <li><code>async get&lt;T&gt;(collection: string, id: string): Promise&lt;T | null&gt;</code>: Retrieve a single item by ID.</li>
                                    <li><code>async set&lt;T&gt;(collection: string, id: string, data: T): Promise&lt;void&gt;</code>: Save (create or update) an item.</li>
                                    <li><code>async delete(collection: string, id: string): Promise&lt;void&gt;</code>: Delete an item by ID.</li>
                                    <li><code>async query&lt;T&gt;(collection: string, filterOptions: FilterOptions): Promise&lt;T[]&gt;</code>: Retrieve multiple items based on filter criteria.</li>
                                    <li>Optional: <code>async init?(config?: any): Promise&lt;void&gt;</code> (for async setup), <code>async clearCollection?(collection: string): Promise&lt;void&gt;</code>, <code>async clearAll?(): Promise&lt;void&gt;</code>. The implementation will translate these generic operations into DuckDB SQL commands executed via the WASM instance.</li>
                                </ul>
                            </details>
                        </div>
                         <div>
                            <p><strong><code>FilterOptions</code></strong></p>
                            <p class="mb-2">Describes how to filter, sort, or limit the data when asking the storage adapter for multiple items. Mapping this to SQL, especially for vector similarity, is the main challenge.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                Interface used as input for the <code>StorageAdapter.query</code> method. May include properties like:
                                <ul class="list-disc list-inside ml-4">
                                    <li><code>filters?: Array&lt;{ field: string; operator: string; value: any }&gt;</code>: Criteria to match items (e.g., <code>{ field: 'role', operator: '==', value: 'USER' }</code>).</li>
                                    <li><code>sortBy?: string</code>: Field name to sort by.</li>
                                    <li><code>sortDirection?: 'asc' | 'desc'</code>: Sorting order.</li>
                                    <li><code>limit?: number</code>: Maximum number of items to return.</li>
                                    <li><code>offset?: number</code>: Number of items to skip (for pagination).</li>
                                    <li>The <code>query</code> implementation in the DuckDB adapter will need to parse these options and construct appropriate <code>WHERE</code>, <code>ORDER BY</code>, and <code>LIMIT</code>/<code>OFFSET</code> clauses in SQL. Supporting complex filters or vector similarity searches (e.g., using a custom operator like <code>&lt;=&gt;</code> if using an extension, or calculating distance manually) requires specific logic.</li>
                                </ul>
                            </details>
                        </div>
                        <div>
                            <p><strong><code>@duckdb/duckdb-wasm</code></strong></p>
                            <p class="mb-2">The library providing the DuckDB WASM engine and browser integration.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                Used for initializing the database (<code>duckdb.selectBundle</code>, <code>db.instantiate</code>, <code>db.open</code>), establishing connections (<code>db.connect()</code>), and executing SQL queries (<code>connection.query()</code>, <code>connection.send()</code>, <code>connection.prepare()</code>, etc.). Requires careful handling of asynchronous initialization and potentially large WASM bundles. Consider using specific backends like OPFS (<code>db.registerFileURL</code>) for better persistence.
                            </details>
                        </div>
                        <div>
                            <p><strong>Vector Embedding Library (Conceptual)</strong></p>
                            <p class="mb-2">A library or function to convert text data (like conversation messages or state content) into numerical vectors (embeddings) for similarity search.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                Needed if implementing vector search capabilities. This is separate from DuckDB itself but crucial for the RAG use case. Embeddings would be generated before <code>set</code>ting data and used during <code>query</code> for similarity calculations. Libraries like <code>Transformers.js</code> can run embedding models client-side.
                            </details>
                        </div>
                    </div>
                 </div>

                 <div id="scenario-4-implementation" class="mb-10">
                    <h3 class="text-xl font-medium mb-4 text-gray-800">6.2. Implementing <code>DuckDBWasmAdapter</code> (Skeleton)</h3>
<pre><code class="language-typescript">// src/adapters/DuckDBWasmAdapter.ts
import { StorageAdapter, FilterOptions } from 'art-framework';
import * as duckdb from '@duckdb/duckdb-wasm';

// --- Vector Embedding Placeholder ---
async function getEmbedding(text: string): Promise&lt;number[]&gt; {
  // Placeholder: Replace with actual embedding generation
  console.warn("Using placeholder embedding function!");
  // Simple hash-based placeholder vector (NOT suitable for real use)
  let hash = 0;
  for (let i = 0; i &lt; text.length; i++) {
    hash = (hash &lt;&lt; 5) - hash + text.charCodeAt(i);
    hash |= 0; // Convert to 32bit integer
  }
  // Create a dummy vector based on the hash
  const vec = Array(16).fill(0); // Small dimension for example
  for(let i=0; i&lt;vec.length; i++) {
      vec[i] = (hash >> (i*2)) & 3; // Simple mapping
  }
  return vec;
}
// --- End Placeholder ---


// Define table schemas conceptually
const TABLE_SCHEMAS: Record&lt;string, string&gt; = {
  conversations: `(id VARCHAR PRIMARY KEY, threadId VARCHAR, role VARCHAR, content TEXT, timestamp BIGINT, embedding FLOAT[16])`, // Added embedding
  state: `(id VARCHAR PRIMARY KEY, threadId VARCHAR, config JSON, agentState JSON)`, // Store JSON directly
  observations: `(id VARCHAR PRIMARY KEY, threadId VARCHAR, traceId VARCHAR, type VARCHAR, timestamp BIGINT, content JSON, metadata JSON)`,
  // Add other collections as needed
};
const EMBEDDING_DIMENSION = 16; // Match the schema

export class DuckDBWasmAdapter implements StorageAdapter {
  private db: duckdb.AsyncDuckDB | null = null;
  private connection: duckdb.AsyncDuckDBConnection | null = null;
  private dbPath: string; // Path for storing DB file if using specific backend
  private initPromise: Promise&lt;void&gt; | null = null; // Prevent race conditions

  constructor(options: { dbPath?: string } = {}) {
      // dbPath might be used with specific backends like OPFS
      this.dbPath = options.dbPath || 'art_duckdb.db';
  }

  // Modified init to handle concurrent calls
  async init(): Promise&lt;void&gt; {
    if (!this.initPromise) {
        this.initPromise = this._initialize();
    }
    return this.initPromise;
  }

  private async _initialize(): Promise&lt;void&gt; {
    if (this.db) return; // Already initialized

    console.log("Initializing DuckDB WASM...");
    try {
      const JSDELIVR_BUNDLES = duckdb.getJsDelivrBundles();
      const bundle = await duckdb.selectBundle(JSDELIVR_BUNDLES);
      const worker_url = URL.createObjectURL(
        new Blob([`importScripts("${bundle.mainWorker!}");`], { type: 'text/javascript' })
      );
      const worker = new Worker(worker_url);
      const logger = new duckdb.ConsoleLogger(); // Or implement custom logger
      this.db = new duckdb.AsyncDuckDB(logger, worker);
      await this.db.instantiate(bundle.mainModule, bundle.pthreadWorker);
      URL.revokeObjectURL(worker_url);

      // Optional: Register a specific file persistence backend if needed (e.g., OPFS)
      // Requires specific browser support and setup
      // await this.db.registerFileURL(this.dbPath, `/${this.dbPath}`, duckdb.DuckDBDataProtocol.BROWSER_FSACCESS, false);

      await this.db.open({
          // path: this.dbPath, // Use if registered above
          query: {
              // Configure WASM specifics if needed, e.g., memory limits
              // initialMemory: '...',
          }
      });
      this.connection = await this.db.connect();
      console.log("DuckDB WASM Initialized and Connected.");

      // Ensure tables exist
      await this.ensureTables();

    } catch (error) {
      console.error("DuckDB WASM Initialization failed:", error);
      this.initPromise = null; // Reset promise on failure
      throw error;
    }
  }

  private async ensureTables(): Promise&lt;void&gt; {
      if (!this.connection) throw new Error("DuckDB connection not available.");
      console.log("Ensuring tables exist...");
      // Consider installing extensions like 'json' if not bundled
      // await this.connection.query(`INSTALL json; LOAD json;`);
      for (const [tableName, schema] of Object.entries(TABLE_SCHEMAS)) {
          try {
              await this.connection.query(`CREATE TABLE IF NOT EXISTS ${tableName} ${schema};`);
              console.log(`Table ${tableName} ensured.`);
          } catch(e) {
              console.error(`Failed to ensure table ${tableName}:`, e);
              throw e;
          }
      }
  }

  private async ensureConnection(): Promise&lt;duckdb.AsyncDuckDBConnection&gt; {
      await this.init(); // Ensure initialization is complete
      if (!this.connection) {
          throw new Error("Failed to establish DuckDB connection after init.");
      }
      return this.connection;
  }

  async get&lt;T&gt;(collection: string, id: string): Promise&lt;T | null&gt; {
    const conn = await this.ensureConnection();
    try {
      // Use prepared statements for safety
      const stmt = await conn.prepare(`SELECT * FROM ${collection} WHERE id = $1`);
      // Use arrow format for potentially better type handling with JSON
      const results = await stmt.query(id);
      await stmt.close(); // Close statement
      if (results.numRows > 0) {
        const row = results.get(0)?.toJSON();
        // DuckDB might return JSON columns as strings, parse them
        return this.parseJsonColumns(collection, row) as T;
      }
      return null;
    } catch (error) {
      console.error(`DuckDB get error in ${collection}:`, error);
      return null; // Or throw? Depends on desired error handling
    }
  }

  async set&lt;T extends { id: string, content?: string }&gt;(collection: string, id: string, data: T): Promise&lt;void&gt; {
    const conn = await this.ensureConnection();
    const schema = TABLE_SCHEMAS[collection];
    if (!schema) throw new Error(`Unknown collection: ${collection}`);

    // Prepare data for insertion (handle JSON, generate embedding)
    const values: any[] = [];
    const placeholders: string[] = [];
    const columns: string[] = [];

    let embedding: number[] | null = null;
    if (collection === 'conversations' && data.content && schema.includes('embedding')) {
        embedding = await getEmbedding(data.content); // Generate embedding
    }

    // Dynamically build based on schema and data properties
    // This is simplified; a real implementation needs robust mapping & type handling
    const columnDefs = schema.substring(1, schema.length - 1).split(',').map(s => s.trim().split(' ')[0]);

    for (const col of columnDefs) {
        if (col === 'embedding') {
            if (embedding) {
                columns.push(col);
                values.push(embedding); // DuckDB WASM might handle array types directly or need list_value syntax
                placeholders.push(`$${values.length}`);
            }
        } else if (col in data) {
            columns.push(col);
            let value = (data as any)[col];
            // Stringify JSON fields
            if (schema.includes(`${col} JSON`) && typeof value === 'object') {
                value = JSON.stringify(value);
            }
            values.push(value);
            placeholders.push(`$${values.length}`);
        } else if (col === 'id') { // Ensure ID is always included if not in data explicitly
             columns.push('id');
             values.push(id);
             placeholders.push(`$${values.length}`);
        }
    }


    const sql = `INSERT OR REPLACE INTO ${collection} (${columns.join(', ')}) VALUES (${placeholders.join(', ')})`;

    try {
      // Use prepared statements for insertion/replacement
      const stmt = await conn.prepare(sql);
      await stmt.send(...values); // Use send for operations not returning rows
      await stmt.close();
    } catch (error) {
      console.error(`DuckDB set error in ${collection}:`, error);
      throw error; // Re-throw to signal failure
    }
  }

  async delete(collection: string, id: string): Promise&lt;void&gt; {
    const conn = await this.ensureConnection();
    try {
      const stmt = await conn.prepare(`DELETE FROM ${collection} WHERE id = $1`);
      await stmt.send(id);
      await stmt.close();
    } catch (error) {
      console.error(`DuckDB delete error in ${collection}:`, error);
      throw error;
    }
  }

  async query&lt;T&gt;(collection: string, filterOptions: FilterOptions): Promise&lt;T[]&gt; {
    const conn = await this.ensureConnection();
    let sql = `SELECT * FROM ${collection}`;
    const params: any[] = [];
    let paramIndex = 1;

    // --- Basic Filtering ---
    if (filterOptions.filters && filterOptions.filters.length > 0) {
      const whereClauses = filterOptions.filters
        .map((filter) => {
            // Basic equality check - needs expansion for other operators
            if (filter.operator === '==') {
                params.push(filter.value);
                return `${filter.field} = $${paramIndex++}`;
            }
            // TODO: Add support for other operators like '!=', '>', '&lt;', 'in', etc.
            console.warn(`Unsupported filter operator: ${filter.operator}`);
            return null; // Ignore unsupported filters
        })
        .filter(clause => clause !== null); // Remove nulls from ignored filters

      if (whereClauses.length > 0) {
          sql += ` WHERE ${whereClauses.join(' AND ')}`;
      }
    }

    // --- Vector Similarity Search (Conceptual) ---
    // This requires a specific setup in DuckDB (e.g., vss extension)
    // or manual calculation. Let's assume a filter operator 'vector_similarity'.
    const vectorFilter = filterOptions.filters?.find(f => f.operator === 'vector_similarity');
    if (vectorFilter && collection === 'conversations' && TABLE_SCHEMAS[collection].includes('embedding')) {
        // Assuming vectorFilter.value is the query embedding (number[])
        // Assuming vectorFilter.field is 'embedding'
        const queryEmbedding = vectorFilter.value as number[];
        // Example using hypothetical list_dot_product (needs extension or UDF)
        // Or calculate distance manually if needed.
        // This SQL is conceptual and depends heavily on DuckDB setup.
        // sql = `SELECT *, list_dot_product(embedding, list_value(${queryEmbedding.join(',')})) AS similarity FROM ${collection}`;
        // sql += ` ORDER BY similarity DESC`; // Order by similarity
        console.warn("Vector similarity search requested but not fully implemented in this skeleton.");
        // Add placeholder WHERE clause if needed based on filtering logic
    } else {
        // --- Basic Sorting ---
        if (filterOptions.sortBy) {
            sql += ` ORDER BY ${filterOptions.sortBy} ${filterOptions.sortDirection === 'desc' ? 'DESC' : 'ASC'}`;
        }
    }


    // --- Pagination ---
    if (filterOptions.limit !== undefined) {
      sql += ` LIMIT $${paramIndex++}`;
      params.push(filterOptions.limit);
    }
    if (filterOptions.offset !== undefined) {
      sql += ` OFFSET $${paramIndex++}`;
      params.push(filterOptions.offset);
    }

    try {
      console.log("Executing DuckDB Query:", sql, params);
      const stmt = await conn.prepare(sql);
      const results = await stmt.query(...params);
      await stmt.close();
      // Parse JSON columns for all results
      return results.toArray().map(arrowRecord => this.parseJsonColumns(collection, arrowRecord.toJSON()) as T);
    } catch (error) {
      console.error(`DuckDB query error in ${collection}:`, error);
      return []; // Return empty on error, or re-throw
    }
  }

   async clearCollection(collection: string): Promise&lt;void&gt; {
       const conn = await this.ensureConnection();
       try {
           const stmt = await conn.prepare(`DELETE FROM ${collection}`);
           await stmt.send();
           await stmt.close();
       } catch (error) {
           console.error(`DuckDB clearCollection error for ${collection}:`, error);
           throw error;
       }
   }

   async clearAll(): Promise&lt;void&gt; {
       const conn = await this.ensureConnection();
       try {
           for (const tableName of Object.keys(TABLE_SCHEMAS)) {
               const stmt = await conn.prepare(`DELETE FROM ${tableName}`);
               await stmt.send();
               await stmt.close();
           }
       } catch (error) {
           console.error(`DuckDB clearAll error:`, error);
           throw error;
       }
   }

   // Helper to parse columns that should be JSON
   private parseJsonColumns(collection: string, row: any): any {
       if (!row) return null;
       const schema = TABLE_SCHEMAS[collection];
       if (!schema) return row;

       const jsonFields = ['config', 'agentState', 'content', 'metadata']; // Fields potentially stored as JSON strings
       for (const field of jsonFields) {
           // Check if schema defines field as JSON and if current value is string
           if (schema.includes(`${field} JSON`) && typeof row[field] === 'string') {
               try {
                   row[field] = JSON.parse(row[field]);
               } catch (e) {
                   console.warn(`Failed to parse JSON field ${field} in collection ${collection}`, e);
                   // Keep as string if parsing fails
               }
           }
       }
       return row;
   }

   async close(): Promise&lt;void&gt; {
       if (this.initPromise) {
           await this.initPromise; // Ensure init is done before closing
       }
       if (this.connection) {
           console.log("Closing DuckDB connection...");
           await this.connection.close();
           this.connection = null;
       }
       if (this.db) {
           console.log("Terminating DuckDB instance...");
           await this.db.terminate();
           this.db = null;
       }
       this.initPromise = null; // Reset init promise
       console.log("DuckDB terminated.");
   }
}
</code></pre>
                    <h4 class="text-lg font-medium mt-6 mb-4 text-gray-800">Explanation:</h4>
                    <ol class="list-decimal list-inside space-y-3 text-gray-700 leading-relaxed">
                        <li><strong>Implement <code>StorageAdapter</code>:** Fulfills the contract.</li>
                        <li><strong>DuckDB WASM Setup:**
                            <ul class="list-disc list-inside ml-6 space-y-2 mt-2">
                                <li>Imports <code>@duckdb/duckdb-wasm</code>.</li>
                                <li>The <code>init</code> method handles the complex asynchronous loading of the WASM bundle, worker instantiation, database opening, and connection establishment. Uses <code>initPromise</code> to prevent race conditions on concurrent calls.</li>
                                <li><code>ensureTables</code> creates the necessary tables (including conceptual <code>embedding</code> column) if they don't exist.</li>
                                <li><code>ensureConnection</code> is a helper to guarantee initialization.</li>
                            </ul>
                        </li>
                         <li><strong>CRUD Methods:**
                            <ul class="list-disc list-inside ml-6 space-y-2 mt-2">
                                <li>Translate operations into SQL using **prepared statements** (`prepare`, `query`, `send`) for security and efficiency.</li>
                                <li>Handles JSON stringification/parsing for relevant columns.</li>
                                <li>Includes conceptual embedding generation during <code>set</code> for the <code>conversations</code> table.</li>
                            </ul>
                        </li>
                        <li><strong><code>query</code> Method:**
                             <ul class="list-disc list-inside ml-6 space-y-2 mt-2">
                                <li>Constructs SQL <code>SELECT</code> query.</li>
                                <li>Maps simple equality filters from <code>FilterOptions</code> to <code>WHERE</code> clauses using parameterized queries.</li>
                                <li>Includes a *conceptual placeholder* for vector similarity search, noting its complexity and dependency on potential DuckDB extensions (like <code>vss</code>) or manual calculations.</li>
                                <li>Adds basic <code>ORDER BY</code>, <code>LIMIT</code>, and <code>OFFSET</code>.</li>
                                <li><strong>Limitation:</strong> Explicitly notes that complex filtering and efficient vector search are advanced topics.</li>
                            </ul>
                        </li>
                        <li><strong>Cleanup:** Includes an async <code>close</code> method to properly terminate the DB connection and worker, ensuring it waits for initialization if pending.</li>
                    </ol>
                 </div>

                 <div id="scenario-4-integration" class="mb-8">
                    <h3 class="text-xl font-medium mb-4 text-gray-800">6.3. Integrating the <code>DuckDBWasmAdapter</code></h3>
                    <p class="mb-4 text-gray-700 leading-relaxed">You integrate a custom storage adapter by passing an *instance* of it in the `storage` field of the configuration object given to `createArtInstance`.</p>
                    <p class="mb-4 text-gray-700 leading-relaxed"><strong>Example Integration Snippet:**</p>
<pre><code class="language-typescript">// --- Integration Example ---
import { DuckDBWasmAdapter } from './adapters/DuckDBWasmAdapter';
import { OpenAIAdapter, ProviderManagerConfig, PESAgent, CalculatorTool, createArtInstance, ArtInstance } from 'art-framework';

async function setupDuckDbArt(): Promise&lt;ArtInstance&gt; {
    // 1. Instantiate your custom storage adapter
    const storageAdapter = new DuckDBWasmAdapter(/* options */);
    // IMPORTANT: Initialize DuckDB WASM (or ensure it's called before first use)
    await storageAdapter.init();

    // 2. Define Provider Configuration
    const providerConfig: ProviderManagerConfig = {
        availableProviders: [
          { name: 'openai', adapter: OpenAIAdapter }
        ]
    };

    // 3. Define ART Configuration
    const config = {
        storage: storageAdapter, // Pass the initialized adapter instance
        providers: providerConfig,
        agentCore: PESAgent,
        tools: [new CalculatorTool()]
    };

    // 4. Create ART Instance
    const artInstance = await createArtInstance(config);

    // Optional: Add cleanup hook for DuckDB if your app lifecycle allows
    // window.addEventListener('beforeunload', () => storageAdapter.close());

    return artInstance;
}
</code></pre>
                    <h4 class="text-lg font-medium mt-6 mb-4 text-gray-800">How it Works Now:</h4>
                    <ul class="list-disc list-inside space-y-3 text-gray-700 leading-relaxed">
                        <li><strong>Node 1 (Developer Interface):** You define `DuckDBWasmAdapter`, instantiate it, call its `init()` method, and pass the instance in the `storage` field when calling `createArtInstance`.</li>
                        <li><strong>Node 2 (Core Orchestration):** The internal Repositories (like `ConversationRepository`, `StateRepository`) receive the `DuckDBWasmAdapter` instance via dependency injection. All internal calls to save or load data (e.g., `ConversationManager.getMessages`, `StateManager.loadThreadContext`) are routed through the Repositories to your adapter's `query`, `get`, `set`, or `delete` methods.</li>
                        <li><strong>Node 3 (External Dependencies & Interactions):** Your adapter interacts with the DuckDB WASM engine, executing SQL commands to manage data stored potentially in the browser's file system (e.g., OPFS) or memory. If vector search is implemented, it also handles embedding generation and similarity calculations.</li>
                    </ul>
                 </div>
            </section>

            <section id="scenario-5" class="mb-16 p-6 bg-white rounded-lg shadow-lg fade-in-section">
                <h2 class="text-2xl md:text-3xl font-semibold mb-6 border-b pb-3 text-sky-700">7. Scenario 5: Adding a Custom Agent Pattern (Advanced Usage)</h2>
                <p class="mb-5 text-gray-700 leading-relaxed">
                    Let's implement the ReAct (Reason -> Act -> Observe) agent pattern and allow the user to switch between PES and ReAct in the chatbot UI.
                </p>
                <p class="mb-8 text-gray-700 leading-relaxed"><strong>Goal:</strong> Create a <code>ReActAgent</code> class, integrate it, and add UI controls for switching.</p>

                <div id="scenario-5-imports" class="mb-10">
                    <h3 class="text-xl font-medium mb-4 text-gray-800">7.1. Necessary Imports & Explanations</h3>
                    <p class="mb-4 text-gray-700 leading-relaxed">In addition to imports from previous scenarios, you need these for creating a custom agent core:</p>
<pre><code class="language-typescript">// --- ART Agent Core Creation Imports ---
import {
  // The interface that a custom agent core must implement
  IAgentCore,
  // Input properties for the process method
  AgentProps,
  // Output type for the process method
  AgentFinalResponse,
  // --- Interfaces for Dependencies Injected into the Agent Core ---
  // These define the components your custom agent will use internally
  StateManager,
  ConversationManager,
  ToolRegistry,
  PromptManager, // Handles prompt assembly using blueprints
  ReasoningEngine, // Returns AsyncIterable<StreamEvent>
  OutputParser, // Parses aggregated LLM output
  ObservationManager,
  ToolSystem,
  UISystem, // Needed for broadcasting stream events
  // Other needed types
  ToolSchema, ParsedToolCall, ToolResult, ArtStandardPrompt, StreamEvent, LLMMetadata, ExecutionMetadata,
  // Types for provider selection
  RuntimeProviderConfig, CallOptions
} from 'art-framework';
</code></pre>
                     <h4 class="text-lg font-medium mt-6 mb-4 text-gray-800">Explanation of Agent Core Imports:</h4>
                     <div class="space-y-4 text-gray-700 leading-relaxed">
                         <div>
                            <p><strong><code>IAgentCore</code></strong></p>
                            <p class="mb-2">The main blueprint for creating a new "thinking style" or reasoning process for the agent. If you want the agent to think differently than the default "Plan -> Use Tools -> Answer" style, you implement this.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                The core interface for custom agent logic. Your class must implement <code>IAgentCore</code>. Key requirements:
                                <ul class="list-disc list-inside ml-4">
                                    <li>Implement <code>async process(props: AgentProps): Promise&lt;AgentFinalResponse&gt;</code>: This method *is* your agent's brain. It receives the <code>AgentProps</code> (query, threadId, etc.) and must orchestrate all steps (loading data, assembling prompts via `PromptManager`, determining `RuntimeProviderConfig`, creating `CallOptions`, calling LLMs via `ReasoningEngine`, handling the `AsyncIterable<StreamEvent>` response, parsing output via `OutputParser`, calling tools via `ToolSystem`, saving data, logging observations, pushing stream events via `UISystem`) according to your custom logic (e.g., a ReAct loop) and return the final response including aggregated metadata.</li>
                                    <li>Define a <code>constructor</code> that accepts a single argument: an object containing instances of the necessary ART subsystems (dependencies) defined by the interfaces below (e.g., `constructor(private deps: { stateManager: StateManager, reasoningEngine: ReasoningEngine, uiSystem: UISystem, ... })`). The `AgentFactory` will automatically provide (inject) these dependencies when it instantiates your custom agent core based on the `config.agentCore` setting.</li>
                                </ul>
                            </details>
                        </div>
                         <div>
                            <p><strong>Dependency Interfaces (`StateManager`, `ConversationManager`, etc.)</strong></p>
                            <p class="mb-2">These are the built-in helpers and managers that ART gives to your custom agent brain so it doesn't have to reinvent everything (like how to talk to the LLM, use tools, remember history, log events, or broadcast UI updates). Your custom `process` method will use these helpers.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                These interfaces define the contracts for the core ART subsystems injected into your <code>IAgentCore</code> constructor. You'll use their methods within your <code>process</code> implementation:
                                <ul class="list-disc list-inside ml-4">
                                    <li><code>StateManager</code>: Use `.loadThreadContext(threadId)` to get <code>ThreadConfig</code> and <code>AgentState</code>. Use `.saveStateIfModified(threadId)` to persist state changes. Use `.isToolEnabled(threadId, toolName)` for checks. Use `getThreadConfigValue(threadId, 'runtimeProviderConfig')` to retrieve the default provider config for the thread.</li>
                                    <li><code>ConversationManager</code>: Use `.getMessages(threadId, options)` to retrieve history. Use `.addMessages(threadId, messages)` to save new user/assistant messages.</li>
                                    <li><code>ToolRegistry</code>: Use `.getAvailableTools({ enabledForThreadId })` to get <code>ToolSchema[]</code> for prompt context. Use `.getToolExecutor(toolName)` if needed (though <code>ToolSystem</code> is usually preferred).</li>
                                    <li><code>PromptManager</code>: Now a stateless assembler. Use `.assemblePrompt(blueprint, context)` with your custom agent's blueprints and gathered <code>PromptContext</code> to create `ArtStandardPrompt` objects for the LLM.</li>
                                     <li><code>ReasoningEngine</code>: Use `.call(prompt, callOptions)` to interact with the LLM. The `callOptions` object (type `CallOptions`) must include the `RuntimeProviderConfig` (specifying provider name, model, API key, etc.) along with other parameters like `stream`, `threadId`, `traceId`. The `ReasoningEngine` uses this config to get the correct adapter instance from the `ProviderManager`. The method returns a `Promise<AsyncIterable<StreamEvent>>`, which your agent must consume.</li>
                                    <li><code>OutputParser</code>: Use `.parsePlanningOutput(...)`, `.parseSynthesisOutput(...)` (for PES-like flows) or potentially define/use custom methods to extract structured data (like thoughts, actions, final answers) from the LLM's raw response content (which your agent needs to buffer/aggregate from the stream).</li>
                                    <li><code>ObservationManager</code>: Use `.record(observationData)` frequently within your `process` logic to log key steps (start/end, LLM calls, tool calls, custom steps like 'thought' or 'action', and new <code>LLM_STREAM_...</code> events) for debugging and UI feedback via sockets.</li>
                                    <li><code>ToolSystem</code>: Use `.executeTools(parsedToolCalls, threadId, traceId)` to run one or more tools identified by your agent's logic. It handles retrieving the executor, validating input against the schema, calling `execute`, and returning `ToolResult[]`.</li>
                                    <li><code>UISystem</code>: Use `.getLLMStreamSocket()` to access the socket for broadcasting real-time <code>StreamEvent</code>s (received from the `ReasoningEngine`'s stream) to the UI. Also use `.getConversationSocket()` and `.getObservationSocket()`.</li>
                                </ul>
                            </details>
                        </div>
                    </div>
                </div>

                <div id="scenario-5-implementation" class="mb-10">
                    <h3 class="text-xl font-medium mb-4 text-gray-800">7.2. Implementing the <code>ReActAgent</code> (with Streaming & Provider Selection)</h3>
                    <p class="mb-4 text-gray-700 leading-relaxed">This requires defining the ReAct loop logic within the <code>process</code> method, including determining the `RuntimeProviderConfig`, creating `CallOptions`, handling the asynchronous stream from the `ReasoningEngine`, and pushing stream events via the `UISystem`.</p>
<pre><code class="language-typescript">// src/agents/ReActAgent.ts

import {
  IAgentCore, AgentProps, AgentFinalResponse, StateManager, ConversationManager, ToolRegistry,
  PromptManager, ReasoningEngine, OutputParser, ObservationManager, ToolSystem, UISystem,
  ObservationType, ConversationMessage, MessageRole, ToolSchema, ParsedToolCall, ToolResult,
  ArtStandardPrompt, StreamEvent, LLMMetadata, ExecutionMetadata, PromptContext,
  RuntimeProviderConfig, CallOptions // Ensure necessary types are imported
} from 'art-framework';

// Define a structure for the parsed ReAct step output
interface ReActStepOutput {
  thought: string;
  action?: string; // Tool name
  actionInput?: any; // Arguments for the tool
  finalAnswer?: string; // Final answer if found
  rawLLMOutput: string; // Aggregated output from the step's LLM call
}

// Define the structure for the ReAct scratchpad
interface ReActScratchpadEntry {
    thought: string;
    action?: string;
    actionInput?: any;
    observation: string; // Result of the action
}

export class ReActAgent implements IAgentCore {
    // Store injected dependencies, including UISystem
    constructor(private deps: {
        stateManager: StateManager;
        conversationManager: ConversationManager;
        toolRegistry: ToolRegistry;
        promptManager: PromptManager;
        reasoningEngine: ReasoningEngine;
        outputParser: OutputParser; // Consider a custom ReAct parser
        observationManager: ObservationManager;
        toolSystem: ToolSystem;
        uiSystem: UISystem; // Added UISystem
    }) {}

    // --- Custom ReAct Parsing Logic (Example) ---
    // This needs to parse the aggregated LLM output after the stream ends.
    private parseReActOutput(llmOutput: string): ReActStepOutput {
        // Example parsing logic (needs refinement for robustness)
        const thoughtMatch = llmOutput.match(/Thought:\s*([\s\S]*?)(?=Action:|Final Answer:|$)/i);
        const actionMatch = llmOutput.match(/Action:\s*(\w+)/i);
        // Try parsing JSON for input, fallback to multiline string capture
        const inputMatch = llmOutput.match(/Action Input:\s*({[\s\S]*?}|[\s\S]*?)(?=Thought:|Observation:|$)/i);
        const finalAnswerMatch = llmOutput.match(/Final Answer:\s*([\s\S]*)/i);

        let actionInput: any = null;
        if (inputMatch && inputMatch[1]) {
            const potentialInput = inputMatch[1].trim();
            try {
                actionInput = JSON.parse(potentialInput);
            } catch (e) {
                actionInput = potentialInput; // Fallback to raw string
            }
        }

        return {
            thought: thoughtMatch ? thoughtMatch[1].trim() : "Could not parse thought.",
            action: actionMatch ? actionMatch[1].trim() : undefined,
            actionInput: actionInput,
            finalAnswer: finalAnswerMatch ? finalAnswerMatch[1].trim() : undefined,
            rawLLMOutput: llmOutput
        };
    }

    // --- Custom ReAct Prompt Blueprint (Mustache Template Example) ---
    // This blueprint structures the input for the PromptManager.assemblePrompt
    private getReActBlueprint(): string {
        // Using Mustache syntax: {{variable}}, {{#section}}...{{/section}}, {{{unescapedVariable}}}
        return JSON.stringify([
            {
                "role": "system",
                "content": `{{{systemPrompt}}}\nYou are a helpful assistant that thinks step-by-step using the ReAct framework. Available tools: {{#tools}}Name: {{name}}, Description: {{description}}, Input Schema: {{{inputSchemaJson}}}{{/tools}}. Use tools to find information or perform actions. Respond with:\nThought: [Your reasoning]\nAction: [tool_name or Final Answer:]\nAction Input: [Arguments as JSON object or the final answer]`
            },
            {
                "role": "user",
                "content": `Conversation History:\n{{#history}}{{role}}: {{content}}\n{{/history}}\n\nUser Query: {{{query}}}`
            },
            {
                "role": "assistant",
                "content": `{{#scratchpad}}Thought: {{{thought}}}\nAction: {{{action}}}\nAction Input: {{{actionInputJson}}}\nObservation: {{{observation}}}\n{{/scratchpad}}Thought:` // LLM continues from here
            }
        ], null, 2); // Return as a JSON string template
    }

    async process(props: AgentProps): Promise<AgentFinalResponse> {
        const { query, threadId, userId, sessionId, configOverrides, executionContext } = props;
        const traceId = `react-trace-${Date.now()}`; // Ensure traceId is generated
        const llmStreamSocket = this.deps.uiSystem.getLLMStreamSocket(); // Get the stream socket

        await this.deps.observationManager.record({ type: ObservationType.PROCESS_START, threadId, traceId, content: { agentType: 'ReAct', query } });

        // 1. Load context, history, tools
        const context = await this.deps.stateManager.loadThreadContext(threadId, configOverrides);
        const initialHistory = await this.deps.conversationManager.getMessages(threadId);
        const tools = await this.deps.toolRegistry.getAvailableTools({ enabledForThreadId: threadId });
        const systemPrompt = context.threadConfig.systemPrompt || "You are a helpful assistant."; // Default system prompt

        // *** Determine RuntimeProviderConfig for this execution ***
        // Prefer override, then thread config, then potentially error or a default
        let runtimeConfig: RuntimeProviderConfig | undefined = context.threadConfig.runtimeProviderConfig;
        if (!runtimeConfig) {
             // Throw error or define a fallback if no provider is configured for the thread
             throw new Error(`No RuntimeProviderConfig found for thread ${threadId}. Please configure a provider.`);
             // Example fallback (NOT RECOMMENDED FOR PRODUCTION):
             // runtimeConfig = { providerName: 'openai', modelId: 'gpt-4o', adapterOptions: { apiKey: 'YOUR_FALLBACK_KEY' } };
        }

        const scratchpad: ReActScratchpadEntry[] = [];
        let step = 0;
        const maxSteps = 7; // Limit loops
        let aggregatedMetadata: Partial<LLMMetadata> = {}; // To aggregate metadata across LLM calls

        while (step < maxSteps) {
            step++;
            await this.deps.observationManager.record({ type: 'REACT_STEP' as ObservationType, threadId, traceId, content: { step } });

            // 2. Prepare Prompt Context and Assemble Prompt
            const promptContext: PromptContext = {
                systemPrompt: systemPrompt,
                query: query,
                history: initialHistory, // Pass full history or relevant snippet
                tools: tools.map(t => ({ ...t, inputSchemaJson: JSON.stringify(t.inputSchema) })), // Add stringified schema for template
                scratchpad: scratchpad.map(s => ({
                    ...s,
                    actionInputJson: JSON.stringify(s.actionInput) // Stringify input for template
                 })),
                // Add any other relevant context
            };
            const blueprint = this.getReActBlueprint();
            const currentPrompt: ArtStandardPrompt = await this.deps.promptManager.assemblePrompt(blueprint, promptContext);

            // 3. Call LLM and process stream
            // Construct CallOptions, including the determined runtimeConfig
            const callOptions: CallOptions = {
                providerConfig: runtimeConfig, // Use the determined config
                threadId: threadId,
                traceId: traceId,
                sessionId: sessionId,
                userId: userId,
                stream: true, // Explicitly request streaming
                callContext: 'AGENT_THOUGHT' // Mark this as an intermediate step
                // Pass other relevant options derived from threadConfig or executionContext if needed
            };
            await this.deps.observationManager.record({ type: ObservationType.LLM_REQUEST, threadId, traceId, content: { phase: `react_step_${step}`, prompt: currentPrompt, options: callOptions } });

            const stream = await this.deps.reasoningEngine.call(currentPrompt, callOptions);

            let llmResponseBuffer = '';
            let stepMetadata: Partial<LLMMetadata> = {}; // Metadata for this specific step

            for await (const event of stream) {
                 // Always forward stream events to the UI socket
                 llmStreamSocket.notify(event);

                switch (event.type) {
                    case 'TOKEN':
                        // Only buffer tokens, UI handles display
                        llmResponseBuffer += event.data;
                        break;
                    case 'METADATA':
                         await this.deps.observationManager.record({ type: ObservationType.LLM_STREAM_METADATA, content: event.data, threadId: event.threadId, traceId: event.traceId, sessionId: event.sessionId });
                         stepMetadata = { ...stepMetadata, ...event.data }; // Capture step-specific metadata
                         break;
                    case 'ERROR':
                        await this.deps.observationManager.record({ type: ObservationType.LLM_STREAM_ERROR, content: event.data, threadId: event.threadId, traceId: event.traceId, sessionId: event.sessionId });
                        console.error(`ReAct Agent LLM Stream Error (Step ${step}):`, event.data);
                        throw new Error(`LLM Stream Error during ReAct step ${step}: ${event.data.message || event.data}`);
                    case 'END':
                         await this.deps.observationManager.record({ type: ObservationType.LLM_STREAM_END, threadId: event.threadId, traceId: event.traceId, sessionId: event.sessionId });
                         break;
                }
            }
            // Aggregate metadata across steps (simple merge, adjust as needed)
            aggregatedMetadata = { ...aggregatedMetadata, ...stepMetadata };
            await this.deps.observationManager.record({ type: ObservationType.LLM_RESPONSE, threadId, traceId, content: { phase: `react_step_${step}`, response: llmResponseBuffer, metadata: stepMetadata } });

            // 4. Parse ReAct Output (using the buffered response)
            const parsedOutput = this.parseReActOutput(llmResponseBuffer);
            await this.deps.observationManager.record({ type: 'thought' as ObservationType, threadId, traceId, content: parsedOutput.thought });

            // 5. Check for Final Answer
            if (parsedOutput.finalAnswer) {
                 await this.deps.observationManager.record({ type: ObservationType.SYNTHESIS_OUTPUT, threadId, traceId, content: { responseText: parsedOutput.finalAnswer } });
                 await this.deps.observationManager.record({ type: ObservationType.PROCESS_END, threadId, traceId, content: { status: 'success', finalAnswer: parsedOutput.finalAnswer } });

                 // Save user query and final AI response
                 const finalMessages: ConversationMessage[] = [
                     // Assume user message was added optimistically by UI or calling code
                     { id: `react-final-${Date.now()}`, role: MessageRole.ASSISTANT, content: parsedOutput.finalAnswer, timestamp: Date.now(), threadId }
                 ];
                 await this.deps.conversationManager.addMessages(threadId, finalMessages);
                 await this.deps.stateManager.saveStateIfModified(threadId);

                 return { responseId: finalMessages[0].id, responseText: parsedOutput.finalAnswer, traceId, threadId, metadata: { llmMetadata: aggregatedMetadata } };
            }

             // 6. Execute Action (if any)
             let observationResultText = "No valid action found in LLM response.";
             if (parsedOutput.action && parsedOutput.actionInput !== undefined && parsedOutput.action !== 'Final Answer:') {
                const toolName = parsedOutput.action;
                const toolInput = parsedOutput.actionInput;
                await this.deps.observationManager.record({ type: ObservationType.TOOL_START, threadId, traceId, metadata: { toolName: toolName, input: toolInput } });

                const toolCall: ParsedToolCall = { toolName: toolName, args: toolInput };
                try {
                    // Pass executionContext if provided in AgentProps
                    const toolResults = await this.deps.toolSystem.executeTools([toolCall], threadId, traceId, executionContext);
                    const result = toolResults[0]; // Assuming single tool call per step for simplicity
                    observationResultText = JSON.stringify(result.status === 'success' ? result.output : { error: result.error });
                    await this.deps.observationManager.record({ type: ObservationType.TOOL_END, threadId, traceId, metadata: { toolName: toolName, resultStatus: result.status, output: result.output, error: result.error } });
                } catch (toolError: any) {
                     console.error(`ReAct Agent Tool Execution Error (Step ${step}):`, toolError);
                     observationResultText = JSON.stringify({ error: `Failed to execute tool ${toolName}: ${toolError.message}` });
                     await this.deps.observationManager.record({ type: ObservationType.TOOL_END, threadId, traceId, metadata: { toolName: toolName, resultStatus: 'error', error: toolError.message } });
                }

            } else {
                 // Log if no action was parsed or action was "Final Answer:" but no finalAnswer content was found
                 await this.deps.observationManager.record({ type: 'action' as ObservationType, threadId, traceId, content: 'No actionable tool call parsed.' });
            }

            // 7. Store step for next iteration scratchpad
             scratchpad.push({
                 thought: parsedOutput.thought,
                 action: parsedOutput.action,
                 actionInput: parsedOutput.actionInput,
                 observation: observationResultText
             });
            await this.deps.observationManager.record({ type: 'observation' as ObservationType, threadId, traceId, content: observationResultText });

        } // End while loop

        // Reached max steps
        const finalResponseText = "Reached maximum thinking steps without a final answer.";
        await this.deps.observationManager.record({ type: ObservationType.PROCESS_END, threadId, traceId, content: { status: 'max_steps_reached' } });

         // Save only a system message indicating failure
         const failureMessage: ConversationMessage = {
             id: `react-maxstep-${Date.now()}`, role: MessageRole.SYSTEM, content: finalResponseText, timestamp: Date.now(), threadId
         };
         await this.deps.conversationManager.addMessages(threadId, [failureMessage]);
         await this.deps.stateManager.saveStateIfModified(threadId);

        return { responseId: failureMessage.id, responseText: finalResponseText, traceId, threadId, metadata: { llmMetadata: aggregatedMetadata } };
    }
}

</code></pre>
                    <h4 class="text-lg font-medium mt-6 mb-4 text-gray-800">Explanation:</h4>
                    <ol class="list-decimal list-inside space-y-3 text-gray-700 leading-relaxed">
                        <li><strong>Implements `IAgentCore`:** Adheres to the contract.</li>
                        <li><strong>Constructor Dependencies:** Includes `UISystem` for accessing the `LLMStreamSocket`.</li>
                         <li><strong>`process` Method:** Contains the core ReAct loop:
                            <ul class="list-disc list-inside ml-6 space-y-2 mt-2">
                                <li>Loads context/history/tools.</li>
                                <li>**Determines `RuntimeProviderConfig`:** Checks `configOverrides` first, then falls back to the thread's default config loaded via `StateManager`. Throws an error if no provider is configured.</li>
                                <li>**Loop:**
                                    <ul>
                                        <li>Prepares `PromptContext` including the current scratchpad.</li>
                                        <li>Uses `PromptManager.assemblePrompt` with a ReAct-specific blueprint to create the `ArtStandardPrompt`.</li>
                                        <li>Creates `CallOptions`, importantly including the determined `runtimeProviderConfig`, `stream: true`, and `callContext: 'AGENT_THOUGHT'`.</li>
                                        <li>Calls `ReasoningEngine.call` with the prompt and options.</li>
                                        <li>**Stream Consumption:** Iterates through the `AsyncIterable<StreamEvent>`.
                                            <ul>
                                                <li>Pushes *every* event to `uiSystem.getLLMStreamSocket().notify(event)`.</li>
                                                <li>Buffers `TOKEN` data into `llmResponseBuffer`.</li>
                                                <li>Logs `LLM_STREAM_METADATA`, `LLM_STREAM_ERROR`, `LLM_STREAM_END` via `ObservationManager`.</li>
                                                <li>Aggregates metadata.</li>
                                            </ul>
                                        </li>
                                         <li>Logs the aggregated `LLM_RESPONSE` after the stream ends.</li>
                                        <li>Parses the *buffered* `llmResponseBuffer` for Thought, Action, Action Input, or Final Answer (using `parseReActOutput`).</li>
                                        <li>Logs the parsed `thought`.</li>
                                        <li>If "Final Answer:", logs synthesis/end, saves history, and returns the `AgentFinalResponse` with aggregated metadata.</li>
                                        <li>If "Action:", executes the tool using `ToolSystem`, logs start/end, and captures the result/error as the `observationResultText`.</li>
                                        <li>Adds the {thought, action, input, observation} to the `scratchpad` for the next loop iteration. Logs the `observation`.</li>
                                    </ul>
                                </li>
                                <li>Handles reaching max steps, saves a system failure message, and returns.</li>
                            </ul>
                        </li>
                         <li><strong>Custom Logic:** Requires a robust `parseReActOutput` function and a well-defined `getReActBlueprint` (e.g., as a Mustache template string) tailored to the specific LLM's expected ReAct format.</li>
                    </ol>
                </div>

                <div id="scenario-5-integration" class="mb-8">
                     <h3 class="text-xl font-medium mb-4 text-gray-800">7.3. Integrating ReAct and Agent Switching into the Chatbot</h3>
                    <p class="mb-4 text-gray-700 leading-relaxed">Modify the <code>ArtChatbot</code> component:</p>
<pre><code class="language-typescript">// src/components/ArtChatbot.tsx
import React, { useState, useEffect, useRef, useCallback } from 'react';
// --- ART Imports ---
import {
  createArtInstance, ArtInstance, AgentProps, AgentFinalResponse,
  ConversationMessage, MessageRole, Observation, ObservationType,
  StreamEvent, ProviderManagerConfig, RuntimeProviderConfig, // Core types
  PESAgent, // Default agent
  IndexedDBStorageAdapter, OpenAIAdapter, CalculatorTool // Default components
} from 'art-framework';
// --- Custom Agent/Tool Imports ---
import { ReActAgent } from '../agents/ReActAgent'; // Import custom agent (adjust path)
import { CurrentInfoTool } from '../tools/CurrentInfoTool'; // Import custom tool

// --- Add Agent Type State ---
type AgentType = 'pes' | 'react';

// Helper to generate temporary IDs
const tempId = () => `temp-${Date.now()}-${Math.random().toString(16).slice(2)}`;

const ArtChatbot: React.FC = () => {
  const [messages, setMessages] = useState&lt;ConversationMessage[]&gt;([]);
  const [input, setInput] = useState('');
  const [isLoading, setIsLoading] = useState(false);
  const [status, setStatus] = useState&lt;string&gt;('Initializing...');
  const [selectedAgent, setSelectedAgent] = useState&lt;AgentType&gt;('pes'); // State for agent choice
  const artInstanceRef = useRef&lt;ArtInstance | null&gt;(null);
  const messageListRef = useRef&lt;HTMLDivElement>(null);
  const threadId = 'web-chatbot-thread-1';
  const [isInitializing, setIsInitializing] = useState(true); // Track initialization state
  const streamingMessageIdRef = useRef<string | null>(null); // Track the ID of the message being streamed

  // --- Auto-scrolling ---
  useEffect(() => {
    if (messageListRef.current) {
      messageListRef.current.scrollTop = messageListRef.current.scrollHeight;
    }
  }, [messages]);

  // --- ART Initialization (Modified for Agent Switching) ---
  useEffect(() => {
    let isMounted = true;
    let unsubObservation: (() => void) | null = null;
    let unsubConversation: (() => void) | null = null;
    let unsubStream: (() => void) | null = null;

    const initializeArt = async () => {
      // Clear previous instance and subscriptions
      if (unsubObservation) unsubObservation();
      if (unsubConversation) unsubConversation();
      if (unsubStream) unsubStream();
      artInstanceRef.current = null; // Helps ensure we don't use stale instance

      if (!isMounted) return;
      setIsInitializing(true);
      setStatus(`Initializing ${selectedAgent.toUpperCase()} Agent...`);

      try {
        const AgentCoreClass = selectedAgent === 'react' ? ReActAgent : PESAgent;

        // Define Provider Configuration
        const providerConfig: ProviderManagerConfig = {
            availableProviders: [
              { name: 'openai', adapter: OpenAIAdapter }
              // Add other providers like AnthropicAdapter if needed
            ]
        };

        // Define ART Factory Configuration
        const config = {
          storage: new IndexedDBStorageAdapter({ dbName: `artWebChatHistory-${selectedAgent}` }), // Separate DB per agent?
          providers: providerConfig,
          agentCore: AgentCoreClass, // Dynamically set the agent core
          tools: [
              new CalculatorTool(),
              new CurrentInfoTool() // Include custom tool
          ]
          // logger: { level: 'debug' } // Optional: Enable detailed logging
        };

        const instance = await createArtInstance(config);
        if (!isMounted) return;

        artInstanceRef.current = instance;

        // Set default provider config for the thread if not already set
        // This ensures the AgentCore has a default provider to use
        try {
            const currentProviderConfig = await instance.stateManager.getThreadConfigValue(threadId, 'runtimeProviderConfig');
            if (!currentProviderConfig) {
                const defaultRuntimeConfig: RuntimeProviderConfig = {
                    providerName: 'openai', // Default to openai
                    modelId: 'gpt-4o',
                    adapterOptions: { apiKey: import.meta.env.VITE_OPENAI_API_KEY || 'YOUR_API_KEY' }
                };
                await instance.stateManager.setThreadConfigValue(threadId, 'runtimeProviderConfig', defaultRuntimeConfig);
                console.log("Set default provider config for thread:", threadId);
            }
        } catch (configError) {
            console.error("Error setting/getting default provider config:", configError);
            // Handle error appropriately - maybe setStatus
        }


        setStatus('Loading history...');
        await loadMessages(); // Reload messages for the new instance/config

        // --- Re-subscribe to Observations ---
        setStatus('Connecting observers...');
        const observationSocket = instance.uiSystem.getObservationSocket();
        unsubObservation = observationSocket.subscribe(
            (observation: Observation) => {
               if (observation.threadId === threadId && isMounted && !isLoading) { // Avoid status flicker during processing
                    let newStatus = status;
                    switch (observation.type) {
                        // ... (observation handling from Scenario 1, potentially add ReAct steps)
                        case ObservationType.PROCESS_START: newStatus = 'Processing...'; break;
                        case ObservationType.LLM_REQUEST: newStatus = 'Thinking...'; break;
                        case ObservationType.TOOL_START: newStatus = `Using ${observation.metadata?.toolName}...`; break;
                        case ObservationType.TOOL_END: newStatus = 'Tool finished.'; break;
                        case ObservationType.PROCESS_END: newStatus = 'Ready.'; streamingMessageIdRef.current = null; break;
                        case ObservationType.LLM_STREAM_END: newStatus = 'Receiving final response...'; break; // Update on stream end
                        case ObservationType.LLM_STREAM_ERROR: newStatus = 'Stream error.'; streamingMessageIdRef.current = null; break;
                        case 'thought' as ObservationType: newStatus = 'Thinking (ReAct)...'; break;
                        case 'observation' as ObservationType: newStatus = 'Observing (ReAct)...'; break;
                    }
                    setStatus(newStatus);
               }
            },
            undefined, // Subscribe to all types for simplicity here
            { threadId: threadId }
        );

        // --- Re-subscribe to Conversation (Final Messages) ---
         const conversationSocket = instance.uiSystem.getConversationSocket();
         unsubConversation = conversationSocket.subscribe(
             (message: ConversationMessage) => {
                 if (message.threadId === threadId && isMounted) {
                     console.log("Received final message via socket:", message);
                     setMessages(prev => {
                         const existingIndex = prev.findIndex(m => m.id === streamingMessageIdRef.current && m.role === MessageRole.ASSISTANT);
                         if (existingIndex > -1) {
                             const updatedMessages = [...prev];
                             updatedMessages[existingIndex] = message; // Replace temp with final
                             return updatedMessages;
                         } else if (!prev.some(m => m.id === message.id)) {
                             return [...prev, message].sort((a, b) => a.timestamp - b.timestamp); // Add if new
                         }
                         return prev;
                     });
                     streamingMessageIdRef.current = null; // Clear streaming ID
                 }
             },
             undefined, { threadId: threadId }
         );

        // --- Re-subscribe to LLM Stream ---
        const streamSocket = instance.uiSystem.getLLMStreamSocket();
        unsubStream = streamSocket.subscribe(
            (event: StreamEvent) => {
                 if (event.threadId === threadId && isMounted) {
                    if (event.type === 'TOKEN' &&
                       (event.tokenType === 'FINAL_SYNTHESIS_LLM_RESPONSE' || // PES final answer
                        event.tokenType?.startsWith('FINAL_SYNTHESIS')) // Catch potential thinking/response in final stage
                       ) {
                         setMessages(prev => {
                             const currentStreamingId = streamingMessageIdRef.current;
                             if (!currentStreamingId) {
                                 const newStreamingMessage: ConversationMessage = {
                                     id: tempId(), role: MessageRole.ASSISTANT, content: event.data,
                                     timestamp: Date.now(), threadId: threadId, metadata: { streaming: true }
                                 };
                                 streamingMessageIdRef.current = newStreamingMessage.id;
                                 return [...prev, newStreamingMessage];
                             } else {
                                 return prev.map(msg => msg.id === currentStreamingId ? { ...msg, content: msg.content + event.data } : msg);
                             }
                         });
                    } else if (event.type === 'ERROR') {
                        console.error("Stream Error:", event.data);
                        setStatus('Stream Error');
                        setMessages(prev => [...prev, { id: tempId(), role: MessageRole.SYSTEM, content: `Stream Error: ${event.data.message || event.data}`, timestamp: Date.now(), threadId: threadId }]);
                        streamingMessageIdRef.current = null;
                    }
                    // END event is handled by PROCESS_END observation or ConversationSocket
                 }
            },
            { threadId: threadId }
        );


        if (isMounted) setStatus('Ready.');

      } catch (error) {
        console.error(`Failed to initialize ${selectedAgent.toUpperCase()} ART:`, error);
        if (isMounted) setStatus(`Initialization Error: ${error instanceof Error ? error.message : 'Unknown error'}`);
      } finally {
         if (isMounted) setIsInitializing(false);
      }
    };

    initializeArt(); // Initialize on mount and when selectedAgent changes

    // Cleanup function
    return () => {
      isMounted = false;
      console.log("Cleaning up ART subscriptions...");
      if (unsubObservation) unsubObservation();
      if (unsubConversation) unsubConversation();
      if (unsubStream) unsubStream();
    };
  }, [selectedAgent, threadId]); // Re-run useEffect when selectedAgent changes!

  // --- Load Messages ---
  const loadMessages = useCallback(async () => {
    setMessages([]); // Clear messages before loading
    if (!artInstanceRef.current) return;
     try {
       // Use a local loading flag for history fetch to avoid interfering with process loading
       // setIsLoading(true); // Removed this, rely on isInitializing
       const history = await artInstanceRef.current.conversationManager.getMessages(threadId, { limit: 100 });
       setMessages(history.sort((a, b) => a.timestamp - b.timestamp));
     } catch (error) {
       console.error("Failed to load messages:", error);
       setStatus('Error loading history.');
     } finally {
       // setIsLoading(false); // Removed this
     }
  }, [threadId]); // Depends only on threadId now, called by useEffect

  // --- Handle Sending ---
  const handleSend = useCallback(async () => {
     if (!input.trim() || !artInstanceRef.current || isLoading || isInitializing) return;

     const userMessage: ConversationMessage = {
       id: `user-${Date.now()}`, role: MessageRole.USER, content: input,
       timestamp: Date.now(), threadId: threadId,
     };
     setMessages(prev => [...prev, userMessage]);
     streamingMessageIdRef.current = null; // Reset streaming ID before sending

     const currentInput = input;
     setInput('');
     setIsLoading(true); // Indicate processing started
     setStatus('Processing request...'); // Initial status

     try {
        // Retrieve the current runtime config for the thread to potentially pass as override
        // (Alternatively, rely on the AgentCore to load it internally)
        // const currentRuntimeConfig = await artInstanceRef.current.stateManager.getThreadConfigValue(threadId, 'runtimeProviderConfig');

        const props: AgentProps = {
            query: currentInput,
            threadId: threadId,
            // Optionally pass runtimeConfig directly if needed as an override
            // configOverrides: {
            //   runtimeProviderConfig: currentRuntimeConfig
            // }
        };

       // Process call is now async but UI updates via sockets
       const response: AgentFinalResponse = await artInstanceRef.current.process(props);
       console.log("ART process completed. Final Response:", response);
       // Final message display is handled by ConversationSocket subscription

     } catch (error) {
       console.error("Error processing message:", error);
       const errorMsg = error instanceof Error ? error.message : 'Failed to get response';
       setMessages(prev => [...prev, { id: `error-${Date.now()}`, role: MessageRole.SYSTEM, content: `Error: ${errorMsg}`, timestamp: Date.now(), threadId: threadId }]);
       setStatus('Error occurred.');
        streamingMessageIdRef.current = null;
     } finally {
       setIsLoading(false); // Indicate processing finished
       // Status should be updated to 'Ready' via PROCESS_END observation if successful
     }
  }, [input, isLoading, threadId, isInitializing, selectedAgent]); // Add selectedAgent dependency

  // --- Render Component ---
  return (
    &lt;div className="chatbot-container"&gt;
      {/* Agent Switcher UI */}
      &lt;div style={{ padding: '5px 10px', borderBottom: '1px solid #eee', textAlign: 'center' }}&gt;
        &lt;label&gt;Agent Mode: &lt;/label&gt;
        &lt;select value={selectedAgent} onChange={(e) => setSelectedAgent(e.target.value as AgentType)} disabled={isInitializing || isLoading}&gt;
          &lt;option value="pes"&gt;Plan-Execute-Synthesize&lt;/option&gt;
          &lt;option value="react"&gt;ReAct&lt;/option&gt;
        &lt;/select&gt;
        {(isInitializing || isLoading) && &lt;span style={{ marginLeft: '10px', fontSize: '0.8em' }}&gt;{status}...&lt;/span&gt;}
      &lt;/div&gt;

      &lt;div className="message-list" ref={messageListRef}&gt;
         {messages.map((msg) => (
           <div key={msg.id} className={`message ${msg.role} ${msg.metadata?.streaming ? 'streaming' : ''}`}>
             <pre style={{ whiteSpace: 'pre-wrap', margin: 0, fontFamily: 'inherit' }}>{msg.content}</pre>
           </div>
         ))}
      &lt;/div&gt;
      &lt;div className="status-indicator"&gt;{isInitializing ? 'Initializing...' : (isLoading ? status : 'Ready.')}&lt;/div&gt;
      &lt;div className="input-area"&gt;
        &lt;input
          type="text"
          value={input}
          onChange={(e) => setInput(e.target.value)}
          onKeyPress={(e) => e.key === 'Enter' && !isLoading && !isInitializing && handleSend()}
          disabled={isLoading || isInitializing || !artInstanceRef.current}
          placeholder={isInitializing ? 'Initializing...' : (artInstanceRef.current ? "Ask something..." : "Error - Init failed")}
        /&gt;
        &lt;button
          onClick={handleSend}
          disabled={isLoading || isInitializing || !artInstanceRef.current || !input.trim()}
        &gt;
          {isLoading ? '...' : 'Send'}
        &lt;/button&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  );
};

export default ArtChatbot;
</code></pre>
                    <h4 class="text-lg font-medium mt-6 mb-4 text-gray-800">Explanation of Changes:</h4>
                     <ol class="list-decimal list-inside space-y-3 text-gray-700 leading-relaxed">
                         <li><strong>State for Agent Type:** Added <code>selectedAgent</code> state (<code>'pes'</code> or <code>'react'</code>).</li>
                         <li><strong>Agent Switcher UI:** Added a <code>&lt;select&gt;</code> dropdown to allow the user to change the <code>selectedAgent</code> state.</li>
                         <li><strong>Dynamic Initialization:** The main <code>useEffect</code> hook now has <code>selectedAgent</code> in its dependency array. This means whenever <code>selectedAgent</code> changes, the effect re-runs:
                             <ul class="list-disc list-inside ml-6 space-y-2 mt-2">
                                 <li>It determines the correct <code>AgentCoreClass</code> (<code>PESAgent</code> or <code>ReActAgent</code>) based on the state.</li>
                                 <li>Updates the `createArtInstance` config to use the new `providers` structure, registering the `OpenAIAdapter` class.</li>
                                 <li>It calls <code>createArtInstance</code> with the chosen <code>agentCore</code>.</li>
                                 <li>It reloads messages (potentially from a different DB if configured, or clears the list for the new agent context).</li>
                                 <li>It re-subscribes to the `ObservationSocket`, `ConversationSocket`, and `LLMStreamSocket` for the new instance.</li>
                                 <li>Includes logic to set a default `RuntimeProviderConfig` for the thread if one doesn't exist.</li>
                             </ul>
                         </li>
                         <li><strong>Loading/Disabled States:** Added <code>isInitializing</code> state and updated `disabled` conditions on input/button/select to prevent interaction while ART is initializing or processing.</li>
                         <li><strong>Stream Handling in UI:**
                             <ul class="list-disc list-inside ml-6 space-y-2 mt-2">
                                 <li>Subscribes to `LLMStreamSocket`.</li>
                                 <li>Uses `streamingMessageIdRef` to track the temporary ID of the message being built from tokens.</li>
                                 <li>On receiving a `TOKEN` event (of the correct `tokenType`), it either creates a new temporary assistant message or appends the token to the existing one.</li>
                                 <li>On receiving the final message via `ConversationSocket`, it replaces the temporary message with the final, persisted one using the ID.</li>
                             </ul>
                          </li>
                         <li><strong>Handle Send:** Updated to retrieve the `RuntimeProviderConfig` (e.g., from thread state or component state) and pass it via `configOverrides` in `AgentProps`.</li>
                     </ol>
                    <h4 class="text-lg font-medium mt-6 mb-4 text-gray-800">How it Works Now:</h4>
                    <ul class="list-disc list-inside space-y-3 text-gray-700 leading-relaxed">
                        <li><strong>Node 1 (Developer Interface):** You've defined the <code>ReActAgent</code> and provided UI controls (<code>&lt;select&gt;</code>) to change the <code>selectedAgent</code> state. The `useEffect` hook dynamically sets the <code>agentCore</code> in the `config` based on this state before calling `createArtInstance`. The `handleSend` function now determines the `RuntimeProviderConfig` for the specific LLM call.</li>
                        <li><strong>Node 2 (Core Orchestration):** When the user selects an agent type, `createArtInstance` builds the internal engine using the *specified* <code>IAgentCore</code> implementation. When `art.process()` is called, the framework routes the call to the currently active agent core's `process` method. This method uses the provided `RuntimeProviderConfig` within `CallOptions` when calling the `ReasoningEngine`. The `ReasoningEngine` uses this config to get the correct adapter instance from the `ProviderManager` and execute the LLM call, handling streaming and releasing the adapter.</li>
                        <li><strong>Node 3 (External Dependencies & Interactions):** The `ProviderAdapter` instance obtained from the `ProviderManager` handles the streaming communication with the LLM. Tools are invoked based on the logic within the active `IAgentCore` implementation (PES or ReAct).</li>
                    </ul>
                </div>
            </section>

            <section id="conclusion" class="mb-16 p-6 bg-white rounded-lg shadow-lg fade-in-section">
                <h2 class="text-2xl md:text-3xl font-semibold mb-6 border-b pb-3 text-sky-700">8. Conclusion</h2>
                <p class="text-gray-700 leading-relaxed">
                    ART offers a layered approach to building browser-based AI agents. Developers can start simply by configuring built-in components, progress to extending capabilities with custom tools and adapters, and finally achieve deep customization by implementing entirely new agent reasoning patterns. Recent advancements, such as the flexible prompt management system using blueprints, the implementation of real-time LLM response streaming, dynamic provider management, and flexible state persistence, further enhance ART's capabilities. By understanding the 3-node architecture and the different usage levels, you can effectively leverage ART to create powerful and flexible client-side AI applications.
                </p>
            </section>

        </main>
    </div>

    <script>
        // Initialize Mermaid
        mermaid.initialize({ startOnLoad: true });

        // Initialize Highlight.js
        hljs.highlightAll();

        // Sidebar Active Link Highlighting
        const sections = document.querySelectorAll('main section[id], main div[id]'); // Include divs with IDs for subsections
        const navLinks = document.querySelectorAll('#sidebar-nav > li > a'); // Direct children links
        const subNavLinks = document.querySelectorAll('#sidebar-nav ul ul a'); // Sub-links

        function changeActiveLink() {
            let index = sections.length;
            let currentId = '';

            // Find the topmost section currently visible, considering a small offset
            while(--index >= 0 && window.scrollY + 150 < sections[index].offsetTop) {} // Increased offset slightly

             if (index >= 0) {
                currentId = sections[index].id;
             }

            // Remove active class from all links first
            navLinks.forEach((link) => link.classList.remove('active'));
            subNavLinks.forEach((link) => link.classList.remove('active'));

            if (currentId) {
                // Try finding the specific sub-link first
                let activeSubLink = document.querySelector(`.sidebar ul ul a[href="#${currentId}"]`);
                let activeMainLink = null;

                if (activeSubLink) {
                    // Highlight the sub-link
                    activeSubLink.classList.add('active');
                    // Find and highlight the parent main link
                    let parentLi = activeSubLink.closest('ul.ml-4')?.closest('li');
                    activeMainLink = parentLi?.querySelector('a.sidebar-link');
                } else {
                    // If no specific sub-link, find the main link
                    activeMainLink = document.querySelector(`.sidebar-link[href="#${currentId}"]`);
                }

                 // Add active class to the main link found (either direct match or parent)
                 if (activeMainLink) {
                     activeMainLink.classList.add('active');
                 }
            }
        }


        // Initial check and add listener
        changeActiveLink();
        window.addEventListener('scroll', changeActiveLink);

         // Smooth scroll for sidebar links (already handled by CSS `scroll-behavior: smooth;`)

         // Fade-in sections on scroll (Simple version)
         const observer = new IntersectionObserver((entries) => {
             entries.forEach(entry => {
                 if (entry.isIntersecting) {
                     entry.target.classList.add('visible'); // You might need to adjust CSS for .visible
                     // Optional: Stop observing after it becomes visible
                     // observer.unobserve(entry.target);
                 }
             });
         }, { threshold: 0.1 }); // Trigger when 10% visible

         document.querySelectorAll('.fade-in-section').forEach(section => {
             observer.observe(section);
         });

    </script>
</body>
</html>