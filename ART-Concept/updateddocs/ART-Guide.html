<!DOCTYPE html>
<html lang="en" class="scroll-smooth">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ART Framework: Comprehensive Developer Guide</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500;700;900&display=swap" rel="stylesheet">
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-light.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/typescript.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/javascript.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/json.min.js"></script>

    <style>
        /* Custom Styles */
        body {
            font-family: 'Roboto', sans-serif; /* UPDATED FONT */
            background-color: #f8fafc; /* Tailwind gray-50 */
            color: #1f2937; /* Tailwind gray-800 for body text */
            -webkit-font-smoothing: antialiased;
            -moz-osx-font-smoothing: grayscale;
            line-height: 1.7; /* Base line-height for readability */
        }

        /* Headings */
        h1, h2, h3, h4, h5, h6 {
            color: #075985; /* Tailwind sky-800 */
            font-weight: 700; /* Bolder for clear hierarchy */
        }

        h2.text-sky-700 {
            color: #0369a1; /* Tailwind sky-700 */
            border-bottom-color: #e0f2fe; /* sky-100 */
            margin-top: 2.5rem;
            margin-bottom: 1.5rem;
            padding-bottom: 0.75rem;
        }
        h3.text-gray-800 { /* For subheadings within sections */
            color: #1e293b; /* Tailwind slate-800 */
            font-weight: 600; /* Semibold */
            margin-top: 2rem; /* More space for H3 */
            margin-bottom: 1rem;
        }
        h4.text-gray-800 {
            color: #334155; /* Tailwind slate-700 */
            font-weight: 600;
            margin-top: 1.5rem;
            margin-bottom: 0.75rem;
        }

        /* Paragraphs in main content for consistent readability */
        .main-content section p, .main-content section ul, .main-content section ol {
             color: #374151; /* Tailwind gray-700, slightly softer than body default */
             margin-bottom: 1.25rem; /* Consistent bottom margin for paragraphs & lists */
        }
        .main-content section p a { /* Ensure links within paragraphs follow link styling */
             font-weight: 500; /* Keep link font weight distinct */
        }


        /* Style code blocks */
        pre code.hljs {
            display: block;
            overflow-x: auto;
            padding: 1.5em;
            border-radius: 0.5rem;
            background: #0f172a; /* Tailwind slate-900 */
            color: #e2e8f0;    /* Tailwind slate-200 */
            font-size: 0.875em;
            line-height: 1.6;
            border: 1px solid #1e293b; /* Tailwind slate-800 */
        }
        /* Inline code */
        code:not(pre > code) {
             background-color: #e0f2fe; /* Tailwind sky-100 */
             color: #0c4a6e;           /* Tailwind sky-800 */
             padding: 0.2em 0.4em;
             border-radius: 0.25rem;
             font-size: 0.875em;
             font-family: 'Roboto Mono', 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace; /* Monospace font */
        }

        /* Mermaid diagram */
        .mermaid {
            display: block;
            margin: 2.5rem auto;
            padding: 1.5rem; /* Increased padding */
            background-color: #ffffff; /* Cleaner white background */
            border: 1px solid #e5e7eb; /* gray-200 */
            border-radius: 0.5rem;
            box-shadow: 0 2px 8px rgba(0,0,0,0.05); /* Softer shadow */
            max-width: 100%;
            text-align: center;
        }
        .mermaid svg {
             max-width: 100%;
             height: auto;
        }

        /* Sidebar */
        .sidebar {
            background-color: #fff;
            box-shadow: 2px 0 10px rgba(0,0,0,0.03);
        }
        .sidebar h1 { /* Title in sidebar */
            color: #0369a1; /* sky-700 */
            padding-bottom: 0.5rem;
            border-bottom: 1px solid #f0f9ff; /* sky-50 */
            font-weight: 700; /* Ensure it's bold */
        }
        .sidebar-link {
            transition: background-color 0.2s ease-in-out, color 0.2s ease-in-out, padding-left 0.2s ease-in-out;
            font-size: 0.95rem;
            color: #374151; /* gray-700 for non-active links */
        }
        .sidebar-link:hover {
            background-color: #f0f9ff; /* Tailwind sky-50 */
            color: #0c4a6e; /* Tailwind sky-800 */
            padding-left: 1.25rem;
        }
        .sidebar-link.active {
            background-color: #e0f2fe; /* Tailwind sky-100 */
            color: #0369a1; /* Tailwind sky-700 */
            font-weight: 600; /* Semibold for active (H1 is bolder) */
            border-left: 4px solid #0ea5e9; /* Tailwind sky-500 */
            padding-left: calc(1rem - 4px + 0.25rem);
        }
        .sidebar ul ul a { /* Sub-links */
            font-size: 0.9rem;
            color: #475569; /* slate-600 */
        }
        .sidebar ul ul a:hover {
             color: #0c4a6e; /* sky-800 */
             background-color: #f0f9ff; /* sky-50 */
        }
        .sidebar ul ul a.active {
            color: #0ea5e9; /* sky-500 */
            font-weight: 500; /* Medium weight for active sub-link */
            background-color: transparent;
        }

        /* Details/Summary Styling - REVAMPED */
        details {
            background-color: #f9fafb; /* Tailwind gray-50, cleaner */
            border: 1px solid #e5e7eb; /* Tailwind gray-200 */
            border-left: 4px solid #2563eb; /* Tailwind blue-600, stronger accent */
            border-radius: 0.5rem; /* rounded-lg */
            padding: 1rem; /* p-5 */
            margin-top: 1.5rem; /* my-6 */
            margin-bottom: 1.5rem;
            box-shadow: 0 1px 2px rgba(0,0,0,0.03);
            transition: box-shadow 0.2s ease-in-out;
        }
        details:hover {
            box-shadow: 0 4px 8px rgba(0,0,0,0.05);
        }
        summary {
            font-weight: 600; /* semibold */
            color: #111827; /* gray-900, for stronger title */
            cursor: pointer;
            list-style: none; /* Remove default marker */
            padding: 0.25rem 0; /* Adjust vertical padding */
            padding-left: 1.75rem; /* Space for custom icon */
            position: relative;
            display: flex; /* For icon alignment */
            align-items: center;
            transition: color 0.2s ease-in-out;
        }
        summary::-webkit-details-marker { /* Hide Safari marker */
            display: none;
        }
        summary::before { /* Custom chevron icon */
            content: url('data:image/svg+xml;charset=UTF-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="%236b7280"><path fill-rule="evenodd" d="M7.293 14.707a1 1 0 010-1.414L10.586 10 7.293 6.707a1 1 0 011.414-1.414l4 4a1 1 0 010 1.414l-4 4a1 1 0 01-1.414 0z" clip-rule="evenodd" /></svg>');
            position: absolute;
            left: 0.25rem; /* Adjusted position */
            top: 50%;
            width: 1.25rem; /* 20px */
            height: 1.25rem; /* 20px */
            transform: translateY(-50%) rotate(0deg);
            transition: transform 0.2s ease-in-out;
        }
        summary:hover {
            color: #0369a1; /* sky-700 */
        }
        summary:hover::before {
             filter: brightness(0.8); /* Slightly darken icon on hover */
        }
        details[open] > summary {
             margin-bottom: 1rem; /* More space when open */
             color: #0369a1; /* sky-700 when open */
        }
        details[open] > summary::before {
            transform: translateY(-50%) rotate(90deg); /* Point down when open */
        }
        details > *:not(summary) {
            font-size: 0.925rem; /* Slightly adjusted for readability */
            color: #374151; /* gray-700 for content */
            line-height: 1.75; /* More generous line-height for content */
            padding-left: 0.25rem; /* Align with summary text if needed, or keep natural flow */
        }
        details ul:not(.list-none) { /* Ensure lists inside details are also styled */
            margin-top: 0.75rem;
            padding-left: 1.5rem; /* Indent lists inside details */
        }
         details ul li {
             margin-bottom: 0.4rem;
         }

        /* Special callout boxes (like the blue/yellow ones) */
        .p-4.bg-blue-50 {
            border-left: 4px solid #3b82f6; /* blue-500 */
            padding: 1.25rem 1.5rem; /* Adjusted padding */
            margin-top: 1.25rem;
            margin-bottom: 1.75rem; /* More margin for callouts */
        }
        .p-4.bg-yellow-50 {
            border-left: 4px solid #f59e0b; /* amber-500 */
            padding: 1.25rem 1.5rem;
            margin-top: 1.25rem;
            margin-bottom: 1.75rem;
        }


        /* Main content sections */
        .main-content section {
            padding: 1.75rem; /* Slightly more padding inside sections */
            background-color: #ffffff; /* Ensure sections are white */
            border-radius: 0.5rem; /* rounded-lg */
            box-shadow: 0 4px 12px rgba(0,0,0,0.04); /* Slightly adjusted shadow */
        }

        /* General links in content */
        .main-content a:not(.sidebar-link) {
            color: #0ea5e9; /* sky-500 */
            text-decoration: none;
            font-weight: 500; /* Medium weight for links */
            border-bottom: 1px solid transparent;
            transition: color 0.2s ease, border-bottom-color 0.2s ease;
        }
        .main-content a:not(.sidebar-link):hover {
            color: #0284c7; /* sky-600 */
            border-bottom-color: #7dd3fc; /* sky-300 */
        }

        /* Lists styling */
        .main-content ul:not(.list-none):not(#sidebar-nav ul),
        .main-content ol:not(.list-none) {
            padding-left: 1.5rem; /* Default indent for lists */
            margin-top: 0.75rem;
            margin-bottom: 1rem;
        }
        .main-content ul li, .main-content ol li {
            margin-bottom: 0.5rem;
            padding-left: 0.5rem; /* Space between marker and text */
        }
        .main-content ul.list-disc li::marker,
        .main-content ul.list-circle li::marker {
            color: #0ea5e9; /* sky-500 - color the bullets */
        }

        /* Smooth scroll */
        html {
            scroll-behavior: smooth;
        }
        /* Animation */
        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(10px); }
            to { opacity: 1; transform: translateY(0); }
        }
        .fade-in-section {
            animation: fadeIn 0.6s ease-out forwards;
        }

        /* Responsive adjustments */
        @media (max-width: 768px) {
            .sidebar {
                position: static;
                width: 100%;
                height: auto;
                margin-bottom: 1rem;
                border-right: none;
                border-bottom: 1px solid #e5e7eb;
                box-shadow: none;
            }
            .main-content {
                margin-left: 0;
                padding-top: 1rem;
            }
            .sidebar ul { /* Make sidebar nav horizontal on mobile */
                display: flex;
                flex-wrap: wrap;
                justify-content: flex-start; /* Align to start */
                padding-left: 0.5rem; /* Some padding */
            }
             .sidebar li {
                margin-right: 0.5rem;
                margin-bottom: 0.5rem;
            }
            .sidebar-link.active {
                border-left-width: 0;
                border-bottom: 3px solid #0ea5e9; /* Use bottom border */
                padding-left: 1rem; /* Reset padding */
                background-color: #f0f9ff; /* Light background for active */
            }
            .sidebar-link:hover {
                padding-left: 1rem;
            }
            h1 { font-size: 1.875rem; /* text-3xl on mobile might be too big, adjust if needed */ }
            h2.text-sky-700, .main-content h2 { /* Ensure main content h2 also responsive */
                font-size: 1.5rem; /* text-2xl */
            }
             h3.text-gray-800, .main-content h3 {
                font-size: 1.25rem; /* text-xl */
            }
            .main-content section {
                padding: 1rem; /* Less padding on mobile for sections */
            }
            details { padding: 1rem; } /* Less padding for details on mobile */
        }

    </style>
</head>
<body class="text-gray-800">

    <div class="flex flex-col md:flex-row min-h-screen">
        <aside class="sidebar w-full md:w-64 lg:w-72 bg-white border-r border-gray-200 p-4 md:p-6 md:sticky md:top-0 md:h-screen overflow-y-auto">
            <h1 class="text-xl lg:text-2xl mb-6">ART Framework</h1>
            <nav>
                <ul class="space-y-2" id="sidebar-nav">
                    <li>
                        <a href="#introduction" class="sidebar-link block px-4 py-2 rounded-md">1. Introduction</a>
                        <ul class="ml-4 mt-1 space-y-1 text-sm">
                            <li><a href="#usage-complexity" class="block px-3 py-1 rounded">1.1 Usage Complexity</a></li>
                        </ul>
                    </li>
                    <li>
                        <a href="#architecture" class="sidebar-link block px-4 py-2 rounded-md">2. Architecture</a>
                         <ul class="ml-4 mt-1 space-y-1 text-sm">
                            <li><a href="#streaming-architecture" class="block px-3 py-1 rounded">2.1 Streaming Architecture</a></li>
                            <li><a href="#prompt-architecture" class="block px-3 py-1 rounded">2.2 Prompt Architecture</a></li>
                            <li><a href="#state-management-architecture" class="block px-3 py-1 rounded">2.3 State Management</a></li>
                            <li><a href="#provider-manager-architecture" class="block px-3 py-1 rounded">2.4 Provider Manager</a></li>
                        </ul>
                    </li>
                    <li>
                        <a href="#scenario-1" class="sidebar-link block px-4 py-2 rounded-md">3. Scenario 1: React Chatbot</a>
                        <ul class="ml-4 mt-1 space-y-1 text-sm">
                            <li><a href="#scenario-1-imports" class="block px-3 py-1 rounded">3.1 Imports</a></li>
                            <li><a href="#scenario-1-component" class="block px-3 py-1 rounded">3.2 Component</a></li>
                             <li><a href="#scenario-1-workflow" class="block px-3 py-1 rounded">3.3 Internal Workflow</a></li>
                        </ul>
                    </li>
                    <li>
                        <a href="#scenario-2" class="sidebar-link block px-4 py-2 rounded-md">4. Scenario 2: Custom Tool</a>
                         <ul class="ml-4 mt-1 space-y-1 text-sm">
                            <li><a href="#scenario-2-simple-explanation" class="block px-3 py-1 rounded">4.0 Simplified Explanation</a></li>
                            <li><a href="#scenario-2-imports" class="block px-3 py-1 rounded">4.1 Imports</a></li>
                            <li><a href="#scenario-2-implementation" class="block px-3 py-1 rounded">4.2 Implementation</a></li>
                            <li><a href="#scenario-2-integration" class="block px-3 py-1 rounded">4.3 Integration</a></li>
                        </ul>
                    </li>
                     <li>
                        <a href="#scenario-3" class="sidebar-link block px-4 py-2 rounded-md">5. Scenario 3: Custom Provider</a>
                         <ul class="ml-4 mt-1 space-y-1 text-sm">
                            <li><a href="#scenario-3-simple-explanation" class="block px-3 py-1 rounded">5.0 Simplified Explanation</a></li>
                            <li><a href="#scenario-3-imports" class="block px-3 py-1 rounded">5.1 Imports</a></li>
                            <li><a href="#scenario-3-implementation" class="block px-3 py-1 rounded">5.2 Implementation</a></li>
                            <li><a href="#scenario-3-integration" class="block px-3 py-1 rounded">5.3 Integration</a></li>
                        </ul>
                    </li>
                     <li>
                        <a href="#scenario-4" class="sidebar-link block px-4 py-2 rounded-md">6. Scenario 4: Custom Storage</a>
                         <ul class="ml-4 mt-1 space-y-1 text-sm">
                            <li><a href="#scenario-4-simple-explanation" class="block px-3 py-1 rounded">6.0 Simplified Explanation</a></li>
                            <li><a href="#scenario-4-imports" class="block px-3 py-1 rounded">6.1 Imports</a></li>
                            <li><a href="#scenario-4-implementation" class="block px-3 py-1 rounded">6.2 Implementation</a></li>
                            <li><a href="#scenario-4-integration" class="block px-3 py-1 rounded">6.3 Integration</a></li>
                        </ul>
                    </li>
                    <li>
                        <a href="#scenario-5" class="sidebar-link block px-4 py-2 rounded-md">7. Scenario 5: Custom Agent</a>
                         <ul class="ml-4 mt-1 space-y-1 text-sm">
                            <li><a href="#scenario-5-imports" class="block px-3 py-1 rounded">7.1 Imports</a></li>
                            <li><a href="#scenario-5-implementation" class="block px-3 py-1 rounded">7.2 Implementation</a></li>
                            <li><a href="#scenario-5-integration" class="block px-3 py-1 rounded">7.3 Integration</a></li>
                        </ul>
                    </li>
                    <li><a href="#conclusion" class="sidebar-link block px-4 py-2 rounded-md">8. Conclusion</a></li>
                </ul>
            </nav>
        </aside>

        <main class="main-content flex-1 p-6 md:p-10 lg:p-12 overflow-y-auto">

            <section id="introduction" class="mb-16 fade-in-section">
                <h2 class="text-2xl md:text-3xl font-semibold mb-6 border-b pb-3 text-sky-700">1. Introduction: What is ART?</h2>
                <p class="mb-5 text-gray-700">
                    ART (Agent Reasoning & Tooling) is a JavaScript/TypeScript framework designed specifically for building intelligent, AI-powered agents that run <strong>directly in the user's web browser</strong>. Think of it as a toolkit that helps you create applications like sophisticated chatbots, research assistants, or automated helpers without needing a separate server for the core AI logic.
                </p>
                <h3 class="text-xl font-medium mb-3 text-gray-800">Core Goals:</h3>
                <ul class="list-disc list-inside mb-5 space-y-2 text-gray-700">
                    <li><strong>Client-Side First:</strong> Runs entirely in the browser, making web-native AI apps possible.</li>
                    <li><strong>Modular:</strong> Built like LEGO bricks â€“ different parts (like memory, reasoning engine, tools) can be swapped or added.</li>
                    <li><strong>Flexible:</strong> Adaptable to different AI models, tools, and agent behaviors.</li>
                    <li><strong>Decoupled:</strong> Components work together through defined contracts (interfaces), not direct dependencies, making the system easier to manage and extend.</li>
                </ul>
                <h3 class="text-xl font-medium mb-3 text-gray-800">Who is this guide for?</h3>
                <p class="mb-5 text-gray-700">
                    This guide is for web developers who want to build applications using Large Language Models (LLMs) directly in the browser. We'll cover everything from basic setup to advanced customization, using both technical terms and simpler explanations.
                </p>

                 <div id="usage-complexity" class="mt-10">
                     <h3 class="text-xl font-medium mb-4 text-gray-800">1.1. Usage Complexity Levels</h3>
                    <p class="mb-4 text-gray-700">Developers can engage with ART at different levels of complexity, depending on their needs:</p>
                    <ul class="list-disc list-inside space-y-4 text-gray-700">
                        <li>
                            <strong>Level 1: Simple Usage (Using Built-ins)</strong>
                            <ul class="list-circle list-inside ml-6 space-y-1 text-sm mt-1">
                                <li><strong>Focus:</strong> Configuration.</li>
                                <li><strong>Activities:</strong> Select from ART's pre-built adapters (storage, reasoning), use the default agent pattern (<code>PESAgent</code>), and potentially include built-in tools. Initialize via <code>createArtInstance</code> and use <code>art.process()</code>.</li>
                                <li><strong>Goal:</strong> Quickly set up a functional agent using standard components.</li>
                            </ul>
                        </li>
                        <li>
                            <strong>Level 2: Intermediate Usage (Extending with Custom Tools/Adapters)</strong>
                             <ul class="list-circle list-inside ml-6 space-y-1 text-sm mt-1">
                                <li><strong>Focus:</strong> Extension & Integration.</li>
                                <li><strong>Activities:</strong> Includes Simple Usage activities, PLUS implementing custom <code>IToolExecutor</code> interfaces to add specific capabilities (e.g., interacting with your backend, using specific libraries) or custom <code>ProviderAdapter</code>/<code>StorageAdapter</code> interfaces for unsupported services/storage.</li>
                                <li><strong>Goal:</strong> Tailor the agent's capabilities and integrations while leveraging the core framework's orchestration.</li>
                            </ul>
                        </li>
                        <li>
                            <strong>Level 3: Advanced Usage (Implementing Custom Agent Patterns)</strong>
                             <ul class="list-circle list-inside ml-6 space-y-1 text-sm mt-1">
                                <li><strong>Focus:</strong> Core Logic Customization.</li>
                                <li><strong>Activities:</strong> Includes Intermediate Usage activities, PLUS implementing a custom <code>IAgentCore</code> interface. This involves defining a completely new reasoning loop or modifying an existing one significantly, including logic for determining the <code>RuntimeProviderConfig</code> for each LLM call. Requires a deep understanding of how all internal ART components interact.</li>
                                <li><strong>Goal:</strong> Gain maximum control over the agent's behavior, reasoning process, and dynamic LLM selection logic.</li>
                            </ul>
                        </li>
                    </ul>
                </div>
            </section>

            <section id="architecture" class="mb-16 fade-in-section">
                <h2 class="text-2xl md:text-3xl font-semibold mb-6 border-b pb-3 text-sky-700">2. Understanding ART's Architecture: The 3 Nodes</h2>
                <p class="mb-5 text-gray-700">
                    Imagine ART as having three main layers or "nodes" that work together:
                </p>
                <div class="mermaid mb-8">
flowchart LR
    A["Node 1: Developer Interface\n(Your Code & Config)"] --> B["Node 2: ART Core Orchestration\n(The Framework's Brain)"]
    B --> C["Node 3: External Connections\n(LLMs, Tools, Storage)"]
    C --> B
                </div>

                <div class="space-y-8">
                    <div>
                        <h3 class="text-xl font-medium mb-3 text-gray-800">Node 1: Developer Interface (Your Code & Config)</h3>
                        <ul class="list-disc list-inside space-y-2 text-gray-700">
                            <li><strong>What it is:</strong> This is where you, the developer, interact with ART. You write the code to set up, configure, and control the agent.</li>
                            <li><strong>What you do here:</strong> Choose which AI model to use (like GPT-4 or Gemini), decide how the agent should remember things (in memory or browser storage), select which tools it can use, pick the agent's thinking style (its "pattern"), and tell the agent when to start processing a user's request.</li>
                            <li><strong>Key ART parts involved:</strong> <code>createArtInstance</code> (the function to start ART), configuration objects, <code>ArtInstance</code> (the main object you interact with), <code>art.process()</code> (the command to make the agent think).</li>
                        </ul>
                    </div>
                    <div>
                        <h3 class="text-xl font-medium mb-3 text-gray-800">Node 2: ART Core Orchestration (The Framework's Brain)</h3>
                         <ul class="list-disc list-inside space-y-2 text-gray-700">
                            <li><strong>What it is:</strong> This is the internal engine of ART, set up based on your configuration in Node 1. It manages the entire process of understanding a request, using tools, and generating a response.</li>
                            <li><strong>What it does:</strong> Follows the chosen agent pattern (like "Plan-Execute-Synthesize" or "ReAct"), manages conversation history, keeps track of the agent's state, constructs instructions (prompt objects) for the AI model using context data and reusable text fragments (provided and validated by <code>PromptManager</code>), understands the AI's responses (including streaming tokens), coordinates tool usage, logs important events (observations), and broadcasts real-time updates to the UI.</li>
                            <li><strong>Key ART parts involved:</strong> The specific Agent Core implementation (`PESAgent`, `ReActAgent`), Managers (`StateManager`, `ConversationManager`, `ObservationManager`), Systems (`ToolSystem`, `UISystem`), Reasoning Components (`ReasoningEngine`, `PromptManager`, `OutputParser`). You usually don't interact with these directly after setup unless you're doing advanced customization or consuming UI sockets.</li>
                        </ul>
                    </div>
                    <div>
                        <h3 class="text-xl font-medium mb-3 text-gray-800">Node 3: External Dependencies & Interactions (The Outside World)</h3>
                         <ul class="list-disc list-inside space-y-2 text-gray-700">
                            <li><strong>What it is:</strong> This node represents where the ART engine connects to resources outside its core orchestration logic. These are the pluggable pieces configured in Node 1, whose instances are often managed by Node 2.</li>
                            <li><strong>What it does:</strong> Makes the actual calls to the LLM provider APIs or local services (using adapter instances provided by the `ProviderManager`), executes tool logic (which might involve calling other web services or using browser features), and persists/retrieves data from the chosen storage mechanism (using the configured `StorageAdapter`).</li>
                            <li><strong>Key Elements (Interfaces & Implementations):</strong> Adapters (`ProviderAdapter` for LLMs, `StorageAdapter` for memory/storage), Tool Implementations (`IToolExecutor`). The actual `ProviderAdapter` instances are created and managed (pooled, cached, evicted) by the `ProviderManager` in Node 2.</li>
                        </ul>
                    </div>
                </div>

                 <div id="streaming-architecture" class="mt-10">
                    <h3 class="text-xl font-medium mb-4 text-gray-800">2.1. Core Concept: Real-time Streaming Architecture</h3>
                    <p class="mb-4 text-gray-700">To provide a more responsive and interactive user experience, ART incorporates a real-time streaming architecture for handling LLM responses. Instead of waiting for the entire response, the UI can receive and display tokens as soon as the LLM generates them.</p>

                    <h4 class="text-lg font-medium mt-6 mb-3 text-gray-800">Key Components:</h4>
                    <ul class="list-disc list-inside space-y-3 mb-6 text-gray-700">
                        <li>
                            <strong><code>ReasoningEngine.call</code> returning <code>AsyncIterable<StreamEvent></code>:</strong>
                            <ul class="list-circle list-inside ml-6 space-y-1 text-sm mt-1">
                                <li><strong>Non-Developer Explanation:</strong> Instead of the agent's "brain" waiting for the LLM's complete answer, it now gets a "conveyor belt" (<code>AsyncIterable</code>). Pieces of the answer (<code>StreamEvent</code> objects) arrive on the belt one by one as the LLM thinks and writes.</li>
                                <details>
                                    <summary>Developer Notes</summary>
                                    The core <code>ReasoningEngine</code> interface's <code>call</code> method now returns a <code>Promise</code> resolving to an <code>AsyncIterable</code>. This iterable yields <code>StreamEvent</code> objects, allowing the consuming code (typically the <code>AgentCore</code>) to process tokens, metadata, errors, and end signals asynchronously as they arrive from the <code>ProviderAdapter</code>.
                                </details>
                            </ul>
                        </li>
                         <li>
                            <strong><code>StreamEvent</code> Interface:</strong>
                            <ul class="list-circle list-inside ml-6 space-y-1 text-sm mt-1">
                                <li><strong>Non-Developer Explanation:</strong> Each item on the conveyor belt has a label (<code>StreamEvent</code>) saying what it is: a piece of text (<code>TOKEN</code>), statistics (<code>METADATA</code>), an error (<code>ERROR</code>), or the end signal (<code>END</code>). Text tokens also have a sub-label (<code>tokenType</code>) indicating if it's part of the LLM's internal "thinking" process or the final "response".</li>
                                <details>
                                    <summary>Developer Notes</summary>
                                    This interface (detailed in Section 3.1) standardizes the data flowing from the LLM stream. The <code>type</code> field is crucial for routing, and the <code>tokenType</code> field enables differentiating intermediate reasoning steps from the final output meant for the user. Adapters are responsible for correctly populating these fields based on provider-specific stream formats and the <code>callContext</code> option.
                                </details>
                            </ul>
                        </li>
                         <li>
                            <strong><code>LLMStreamSocket</code> (<code>UISystem</code>):</strong>
                            <ul class="list-circle list-inside ml-6 space-y-1 text-sm mt-1">
                                <li><strong>Non-Developer Explanation:</strong> ART uses an announcement system (sockets) for different parts to communicate, especially with the UI. A new, dedicated channel (<code>LLMStreamSocket</code>) was added specifically for broadcasting the live stream events (tokens, metadata, errors, end signals) from the LLM conveyor belt to any UI components listening.</li>
                                <details>
                                    <summary>Developer Notes</summary>
                                    Accessed via <code>artInstance.uiSystem.getLLMStreamSocket()</code>. The <code>AgentCore</code> consumes the <code>AsyncIterable</code> from the <code>ReasoningEngine</code> and pushes each <code>StreamEvent</code> to this socket. UI components subscribe to this socket to receive real-time updates, decoupling the UI from the stream source and providing a consistent subscription pattern (<code>socket.subscribe(...)</code>).
                                </details>
                            </ul>
                        </li>
                    </ul>

                     <h4 class="text-lg font-medium mt-6 mb-3 text-gray-800">Flow Overview:</h4>
                    <ol class="list-decimal list-inside space-y-3 mb-6 text-gray-700">
                        <li><strong>UI/App:</strong> Calls <code>artInstance.process(props)</code>.</li>
                        <li><strong>Agent Core (<code>PESAgent</code>, etc.):</strong> Calls <code>reasoningEngine.call(prompt, { stream: true, callContext: '...' })</code>.</li>
                        <li><strong>Reasoning Engine (Adapter - e.g., <code>OpenAIAdapter</code>):</strong> Makes a streaming request to the LLM provider API.</li>
                        <li><strong>Adapter:</strong> Receives stream chunks, parses them, determines <code>tokenType</code>, and <code>yield</code>s <code>StreamEvent</code> objects via the <code>AsyncIterable</code>.</li>
                        <li><strong>Agent Core:</strong> Consumes the <code>AsyncIterable</code> using <code>for await...of</code>.</li>
                        <li><strong>Agent Core:</strong> For each <code>StreamEvent</code>:
                            <ul class="list-disc list-inside ml-6 space-y-1 text-sm mt-1">
                                <li>Pushes the event to `uiSystem.getLLMStreamSocket().notify(event)`.</li>
                                <li>If it's a final response `TOKEN`, appends it to an internal buffer.</li>
                                <li>If it's `METADATA`, `ERROR`, or `END`, records it via `ObservationManager`.</li>
                                <li>Aggregates `METADATA`.</li>
                            </ul>
                        </li>
                        <li><strong>UI:</strong> Receives <code>StreamEvent</code>s via its `LLMStreamSocket` subscription and updates the display in real-time.</li>
                        <li><strong>Agent Core (After Stream Ends):</strong> Constructs the final `ConversationMessage` from the buffer, saves it via `ConversationManager`, and returns the `AgentFinalResponse` object containing the final `response` (as a `ConversationMessage`) and the execution `metadata` (including aggregated `llmMetadata`).</li>
                        <li><strong>UI:</strong> (Optional but recommended) Receives the final `ConversationMessage` via `ConversationSocket` subscription and replaces the temporary streamed message with the final, persisted one to ensure consistency.</li>
                    </ol>

                     <h4 class="text-lg font-medium mt-6 mb-3 text-gray-800">Thinking vs. Response Tokens (<code>tokenType</code>):</h4>
                     <ul class="list-disc list-inside space-y-2 mb-6 text-gray-700">
                         <li>The <code>StreamEvent.tokenType</code> field allows distinguishing between tokens generated during intermediate reasoning steps (e.g., <code>AGENT_THOUGHT_LLM_RESPONSE</code>) and tokens forming the final user-facing answer (e.g., <code>FINAL_SYNTHESIS_LLM_RESPONSE</code>).</li>
                         <li>Adapters determine the <code>LLM_THINKING</code> vs <code>LLM_RESPONSE</code> part based on provider-specific markers (if available).</li>
                         <li>The Agent Core provides the <code>AGENT_THOUGHT</code> vs <code>FINAL_SYNTHESIS</code> context via <code>CallOptions.callContext</code>.</li>
                         <li>The UI can use <code>tokenType</code> to visually differentiate these tokens (e.g., showing thinking steps faded or in a separate area).</li>
                     </ul>

                     <h4 class="text-lg font-medium mt-6 mb-3 text-gray-800">Metadata Delivery (<code>LLMMetadata</code>):</h4>
                     <ul class="list-disc list-inside space-y-2 mb-6 text-gray-700">
                          <li>Detailed LLM statistics (token counts, timing) are packaged into <code>LLMMetadata</code> objects.</li>
                          <li>Adapters yield these as <code>METADATA</code> <code>StreamEvent</code>s (either during the stream if the provider supports it, or after the stream ends based on the final response/usage info).</li>
                          <li>These events are broadcast via <code>LLMStreamSocket</code> for potential real-time display.</li>
                          <li>They are also logged as discrete observations (`LLM_STREAM_METADATA`) via `ObservationManager`.</li>
                          <li>Finally, the metadata from all relevant LLM calls within an execution cycle is aggregated and included in the `AgentFinalResponse.metadata` object (specifically within `metadata.llmMetadata`).</li>
                     </ul>

                    <div id="streaming-interfaces">
                        <h4 class="text-lg font-medium mt-6 mb-4 text-gray-800">Streaming-Related Interfaces and Types</h4>
                        <p class="mb-4 text-gray-700">To support real-time streaming and detailed metadata, the following interfaces and types were introduced or modified:</p>
                        <div class="space-y-4">
                            <div>
                                <p><strong><code>StreamEvent</code> Interface:</strong></p>
<pre><code class="language-typescript">// Non-Developer: Defines the "label" attached to each piece coming from the LLM stream, saying what it is.
// Developer Notes: Represents a single event from the LLM stream iterator. Should be defined in src/types/index.ts.
interface StreamEvent {
  type: 'TOKEN' | 'METADATA' | 'ERROR' | 'END'; // Kind of event
  data: any; // The actual content (string for TOKEN, LLMMetadata for METADATA, Error for ERROR)
  tokenType?: 'LLM_THINKING' | 'LLM_RESPONSE' | 'AGENT_THOUGHT_LLM_THINKING' | 'AGENT_THOUGHT_LLM_RESPONSE' | 'FINAL_SYNTHESIS_LLM_THINKING' | 'FINAL_SYNTHESIS_LLM_RESPONSE'; // Specific kind of token
  threadId: string; // Links event to the conversation thread. Essential for routing and context.
  traceId: string; // Links event to the specific agent.process() call. Essential for debugging and correlation.
  sessionId?: string; // Links event to a specific UI tab/window. Recommended for multi-session scenarios.
}
</code></pre>
                            </div>
                             <div>
                                <p><strong><code>LLMMetadata</code> Interface:</strong> Define the structure for detailed LLM statistics:</p>
<pre><code class="language-typescript">// Non-Developer: Defines what kind of statistics we want to capture about the LLM's work (like token counts).
// Developer Notes: Structure for holding parsed metadata from LLM responses/streams. Should be defined in src/types/index.ts.
interface LLMMetadata {
  inputTokens?: number;
  outputTokens?: number;
  thinkingTokens?: number; // If available from provider
  timeToFirstTokenMs?: number;
  totalGenerationTimeMs?: number;
  stopReason?: string; // e.g., 'stop_sequence', 'max_tokens', 'end_turn'
  providerRawUsage?: any; // Optional raw usage data from provider for extensibility
  // Developer Note: Include traceId if this object might be stored or passed independently.
  traceId?: string;
}
</code></pre>
                            </div>
                             <div>
                                <p><strong><code>CallOptions</code> Interface:</strong> Add options to control streaming and provide context:</p>
<pre><code class="language-typescript">// Non-Developer: Adds switches to turn streaming on/off and tells the LLM call whether it's for an internal "thought" or the final answer.
// Developer Notes: Extends options passed to ReasoningEngine.call. Defined in src/types/index.ts.
interface CallOptions {
  threadId: string;
  traceId?: string;
  sessionId?: string;
  userId?: string;
  stream?: boolean; // Request streaming response. Adapters MUST check this.
  callContext?: 'AGENT_THOUGHT' | 'FINAL_SYNTHESIS' | string; // Provides context for tokenType determination by the adapter. Agent Core MUST provide this.
  providerConfig: RuntimeProviderConfig; // Carries the specific target + config for this call. Agent Core MUST provide this.
  // Other potential options like stopSequences, etc. should be IN providerConfig.adapterOptions
}
</code></pre>
                            </div>
                            <div>
                                <p><strong><code>ExecutionMetadata</code> Interface:</strong> Add field to store final metadata:</p>
<pre><code class="language-typescript">// Non-Developer: Adds a place to store the final LLM statistics associated with the agent's overall response.
// Developer Notes: Extends the metadata returned by agent.process(). Defined in src/types/index.ts.
interface ExecutionMetadata {
  // ... existing fields
  llmMetadata?: LLMMetadata; // Aggregated metadata from LLM calls in the execution. Agent Core MUST populate this.
}
</code></pre>
                            </div>
                             <div>
                                <p><strong><code>ObservationType</code> Enum:</strong> Add types for logging discrete stream events:</p>
<pre><code class="language-typescript">// Non-Developer: Adds new categories to the agent's logbook specifically for important streaming events.
// Developer Notes: New enum values for ObservationManager.record(). Defined in src/types/index.ts.
enum ObservationType {
  // ... existing types
  LLM_STREAM_START = 'LLM_STREAM_START', // Optional: Logged by Agent Core when iterator consumption begins.
  LLM_STREAM_METADATA = 'LLM_STREAM_METADATA', // Logged by Agent Core upon receiving METADATA event. Content should be LLMMetadata.
  LLM_STREAM_END = 'LLM_STREAM_END', // Logged by Agent Core upon receiving END event.
  LLM_STREAM_ERROR = 'LLM_STREAM_ERROR', // Logged by Agent Core upon receiving ERROR event. Content should be Error object or message.
}
</code></pre>
                            </div>
                        </div>
                    </div>
                </div>

                 <div id="prompt-architecture" class="mt-10">
                     <h3 class="text-xl font-medium mb-4 text-gray-800">2.2. Core Concept: Prompt Management Architecture</h3>
                    <p class="mb-4 text-gray-700">ART's prompt management system has been refactored to provide greater flexibility, decouple agent patterns from the core framework, and give developers more control over prompt content.</p>

                    <h4 class="text-lg font-medium mt-6 mb-3 text-gray-800">Rationale:</h4>
                     <ul class="list-disc list-inside space-y-2 mb-6 text-gray-700">
                        <li><strong>Support Custom Agent Patterns:</strong> Decouple core prompt assembly from specific agent patterns (like PES) to allow developers to create and integrate arbitrary agent flows without modifying the ART framework.</li>
                        <li><strong>Developer Control over Content:</strong> Enable developers to define and control key prompt content elements, such as system prompts (via `ThreadConfig` or dynamic context), tool presentation (via descriptions/schemas in their tool definitions, interpreted by agent blueprints), and custom data relevant to their specific agent logic (via `PromptContext`).</li>
                        <li><strong>Provider Agnosticism:</strong> Achieve true decoupling between agent logic/prompt structure and provider-specific API requirements by introducing a standardized intermediate format (`ArtStandardPrompt`).</li>
                        <li><strong>Clear Responsibilities:</strong> Establish clear boundaries: Agent Patterns construct the prompt object and provide context; `PromptManager` provides reusable fragments and validates the final object; Adapters translate the standard format to the provider API.</li>
                         <li><strong>Reliability & Maintainability:</strong> Avoid fragile string-based template rendering for complex JSON structures, while still allowing prompt instructions to be managed separately as fragments.</li>
                    </ul>

                    <h4 class="text-lg font-medium mt-6 mb-3 text-gray-800">Hybrid Architecture Overview:</h4>
                    <div class="mermaid mb-8">
graph TD
    subgraph AgentLogic["Agent Pattern Logic"]
        A[Agent Step Logic] -->|Gathers data| C{Context Data}
        C -->|Includes| Q(Query)
        C -->|Includes| SP(System Prompt from Config)
        C -->|Includes| H(History from Storage)
        C -->|Includes| TS(Tool Schemas from Registry)
        C -->|Includes| TR(Tool Results if applicable)
        C -->|Includes| CD(Custom Data)
        A -->|Requests fragments| PM(PromptManager.getFragment)
        PM -->|Returns fragments| A
        A -->|Constructs object| ASP(ArtStandardPrompt Object)
        ASP -->|Sends for validation| PMV(PromptManager.validatePrompt)
        PMV -->|Returns validated object| A
    end

    subgraph ARTCore["ART Core"]
        PMV -->|Provides| PM(PromptManager)
        A -->|Passes validated object| RE(ReasoningEngine)
        RE -->|Forwards| PA(ProviderAdapter)
        PA -->|Translates| APIFormat(Provider API Payload)
    end

    PA -->|Sends| ExtAPI[External LLM API]
                    </div>

                    <h4 class="text-lg font-medium mt-6 mb-3 text-gray-800">Key Concepts:</h4>
                    <ul class="list-disc list-inside space-y-3 mb-6 text-gray-700">
                        <li><strong><code>ArtStandardPrompt</code> Format:</strong> A canonical, provider-agnostic message array format (e.g., `[{ role: 'system' | 'user' | 'assistant' | 'tool_request' | 'tool_result', content: string | object }]`). This replaces the previous `FormattedPrompt` type alias.</li>
                        <li><strong>Agent Pattern Responsibility:</strong> Gathers all necessary context data (query, system prompt string, history, tool schemas, tool results, custom data). Constructs the final `ArtStandardPrompt` *JavaScript object* directly. May retrieve reusable instruction text blocks (`fragments`) from the `PromptManager` to incorporate into the prompt object's content strings. Built-in agents define default system prompt strings, which can be overridden by `ThreadConfig`.</li>
                        <li><strong>Core `PromptManager`:</strong> Provides reusable prompt text fragments via `getFragment(name: string, context?: Record<string, any>): string`. Validates the final `ArtStandardPrompt` object constructed by the agent logic using `validatePrompt(prompt: ArtStandardPrompt): ArtStandardPrompt`. It no longer uses templates or assembles the prompt structure itself.</li>
                        <li><strong>Core `ReasoningEngine`:</strong>Receives the *validated* `ArtStandardPrompt` object from the Agent Logic (after it calls `promptManager.validatePrompt`) and passes it directly to the selected `ProviderAdapter`.</li>
                        <li><strong>Provider Adapters:</strong> Each adapter is responsible for <strong>translating</strong> the received `ArtStandardPrompt` object into the specific API format required by the target LLM provider (e.g., mapping roles, structuring content, handling tool calls/results according to that API's specification).</li>
                    </ul>
                    <p class="mb-4 text-gray-700">This architecture ensures reliability by avoiding complex string templating for JSON, while maintaining separation of concerns. Developers customize prompt behavior by modifying agent logic (how it constructs the object and uses fragments) and managing the fragments themselves, without needing to modify the core `PromptManager` validation or the provider-specific `Adapter` translation logic.</p>

                     <h4 class="text-lg font-medium mt-6 mb-3 text-gray-800">Example Data Flow (Hybrid Approach):</h4>
                    <ol class="list-decimal list-inside space-y-3 text-gray-700">
                        <li><em>Agent Logic (e.g., PES Planning):</em> Gathers context data: `query`, `history`, `availableTools`, `systemPrompt` (retrieved via `stateManager.getThreadConfigValue` or agent default).</li>
                        <li><em>Agent Logic Calls:</em> `promptManager.getFragment('pes_planning_instructions')`, `promptManager.getFragment('pes_tool_format_instructions')` etc. to retrieve necessary instruction texts.</li>
                        <li><em>Agent Logic Constructs Prompt:</em> Creates the `planningPromptObject` (an `ArtStandardPrompt` array) directly in code, embedding the context data (query, formatted history, tool details) and the retrieved fragments into the appropriate `content` strings of the message objects.</li>
                        <li><em>Agent Logic Calls:</em> `promptManager.validatePrompt(planningPromptObject)`.</li>
                        <li><em>`PromptManager` Execution:</em> Validates the structure and types of `planningPromptObject` against the `ArtStandardPromptSchema`. Returns the validated object (or throws a `ZodError` wrapped in `ARTError` if invalid).</li>
                        <li><em>Agent Logic:</em> Determines the `RuntimeProviderConfig` (e.g., from `ThreadConfig`). Creates `CallOptions` including the `providerConfig`. Calls `reasoningEngine.call(validatedPlanningPromptObject, callOptions)`.</li>
                        <li><em>`ReasoningEngine` Execution:</em> Calls `providerManager.getAdapter(callOptions.providerConfig)` to get a `ManagedAdapterAccessor`.</li>
                        <li><em>`ReasoningEngine` Execution:</em> Calls `accessor.adapter.call(validatedPlanningPromptObject, callOptions)` which translates the prompt object and interacts with the LLM API.</li>
                        <li><em>`ReasoningEngine` Execution (Post-Call/Stream):</em> Calls `accessor.release()` to return the adapter instance slot to the `ProviderManager`.</li>
                    </ol>
                </div>
                 <div id="state-management-architecture" class="mt-10">
                    <h3 class="text-xl font-medium mb-4 text-gray-800">2.3. Core Concept: State Management Architecture</h3>
                    <p class="mb-4 text-gray-700">ART's state management system is responsible for keeping track of important information beyond just the conversation history. This includes thread-specific configurations and the agent's internal state, which helps maintain context and manage complex tasks across turns.</p>

                    <h4 class="text-lg font-medium mt-6 mb-3 text-gray-800">Components of State Management:</h4>
                    <p class="mb-4 text-gray-700">ART's state management involves a few key components working together:</p>
                    <ul class="list-disc list-inside space-y-3 mb-6 text-gray-700">
                        <li><strong>The State Manager (`StateManager`):</strong> This is the primary interface within the ART core that agent implementations interact with. Its role is to handle the *logic* of state management. It knows *what* state is needed for a particular conversation or task and *when* to load or save it. It provides methods like `loadThreadContext(threadId)` to retrieve configuration (`ThreadConfig`) and state (`AgentState`) for a specific thread, and `saveStateIfModified(threadId)` to persist changes. The `ThreadConfig` is particularly important as it's the standard place for applications to store thread-specific settings, such as the desired LLM provider and model (`RuntimeProviderConfig`) for that conversation.</li>
                        <li><strong>The State Repository (`IStateRepository`):</strong> This component acts as a librarian between the `StateManager` and the actual storage mechanism. It defines *how* to find and organize the specific state information the `StateManager` asks for. It has methods like `getState(threadId)` or `saveState(threadId, state)`. However, the librarian doesn't handle the physical storage details; it delegates these operations to the `StorageAdapter`.</li>
                        <li><strong>The Storage Adapter (`StorageAdapter`):</strong> This is the layer responsible for the *actual saving and loading* of state data to and from a physical storage medium. It knows *how* to interact with the specific storage backend being used (e.g., IndexedDB, a database, a custom API). The `IStateRepository` tells the `StorageAdapter` *what* data to store or retrieve (e.g., "save this piece of state data with this ID in the 'state' collection"), and the `StorageAdapter` handles the *how* (e.g., "write this JSON object to the 'state' table in Supabase").</li>
                    </ul>

                     <h4 class="text-lg font-medium mt-6 mb-3 text-gray-800">Extensibility through the Storage Adapter:</h4>
                     <p class="mb-4 text-gray-700">A key aspect of ART's state management flexibility for developers using the npm package is the ability to provide a custom `StorageAdapter`. While the `StateManager` and `IStateRepository` interfaces and their default implementations are part of the core framework, the `IStateRepository` is designed to work with *any* component that implements the `StorageAdapter` interface.</p>
                     <p class="mb-4 text-gray-700">This means you, as the application developer, have full control over where and how the state is persisted by implementing your own `StorageAdapter` that connects to your desired storage backend.</p>

                      <h4 class="text-lg font-medium mt-6 mb-3 text-gray-800">How Custom Storage Adapters Enable Flexible State Management:</h4>
                      <p class="mb-4 text-gray-700">By creating a custom `StorageAdapter`, you can integrate ART's state management with virtually any storage solution. This allows for:</p>
                      <ul class="list-disc list-inside space-y-2 mb-6 text-gray-700">
                            <li><strong>Persistent State:</strong> Using databases (like Supabase, PostgreSQL, etc.) or other persistent storage mechanisms to ensure conversation settings and agent state are saved across application sessions.</li>
                            <li><strong>Caching:</strong> Implementing caching layers (like using `InMemoryStorageAdapter` as a cache in front of a persistent backend) to improve performance by quickly accessing frequently used state data.</li>
                            <li><strong>Custom Logic:</strong> Adding any custom logic needed for your specific storage requirements within your adapter, such as data transformation, encryption, or integration with specific APIs.</li>
                      </ul>
                      <p class="mb-4 text-gray-700">When you initialize ART using `createArtInstance`, you provide your custom `StorageAdapter` instance in the configuration. The ART factory will then ensure that the core state management components use your adapter for all state persistence and loading operations.</p>

                    <h4 class="text-lg font-medium mt-6 mb-4 text-gray-800">How to Create and Use Your Custom Storage Adapter:</h4>
                    <ol class="list-decimal list-inside space-y-3 text-gray-700">
                         <li><strong>Create Your Adapter File(s):</strong> Create new file(s) in your application's project, perhaps in a folder like `storage-adapters` or `data`. For example, `supabase-adapter.ts` and `caching-adapter.ts`.</li>
                         <li><strong>Import Necessary ART Components:</strong> Inside your adapter files, import the required types and interfaces from `art-framework`. Key imports for a storage adapter include:
                             <ul class="list-disc list-inside ml-6 space-y-1 text-sm mt-1">
                                 <li>`StorageAdapter`: The interface your adapter class must implement.</li>
                                 <li>`FilterOptions`: The type defining options for querying data.</li>
                                 <li>You might also need types for the data you are storing (e.g., `ConversationMessage`, `AgentState`, `Observation`) if you want type safety within your adapter, although the `StorageAdapter` interface uses generic types (`&lt;T&gt;`).</li>
                             </ul>
                         </li>
                         <li><strong>Implement Your Adapter Class(es):</strong> Create the class(es) that implement the `StorageAdapter` interface.
                             <ul class="list-disc list-inside ml-6 space-y-1 text-sm mt-1">
                                 <li>For a backend-specific adapter (like a `SupabaseAdapter`), implement the `get`, `set`, `delete`, and `query` methods using the client library for your chosen backend to interact with your database or storage service. Implement `init` if you need to establish the connection asynchronously.</li>
                                 <li>For a caching adapter (like a `CachingStorageAdapter`), implement the `get`, `set`, `delete`, and `query` methods by coordinating calls to the injected primary and cache adapters (e.g., check cache on `get`, write to both on `set`).</li>
                             </ul>
                         </li>
                         <li><strong>Import and Pass to `createArtInstance`:</strong> In the file where you initialize ART, import your custom adapter class(es). In the configuration object passed to `createArtInstance`, create instances of your custom adapters and pass the top-level adapter (e.g., your `CachingStorageAdapter`) in the `storage` part:
<pre><code class="language-typescript">import { createArtInstance, InMemoryStorageAdapter, OpenAIAdapter, ProviderManagerConfig } from 'art-framework'; // Make sure OpenAIAdapter and ProviderManagerConfig are imported
import { SupabaseAdapter } from './storage-adapters/supabase-adapter'; // Import your Supabase adapter
import { CachingStorageAdapter } from './storage-adapters/caching-adapter'; // Import your Caching adapter

// Assuming SupabaseAdapter constructor takes options like URL and Key
const supabaseAdapter = new SupabaseAdapter({ url: 'YOUR_SUPABASE_URL', apiKey: 'YOUR_SUPABASE_API_KEY' });
const inMemoryAdapter = new InMemoryStorageAdapter(); // Use the built-in in-memory adapter

// Instantiate your caching adapter with the primary and cache adapters
const cachingAdapter = new CachingStorageAdapter(supabaseAdapter, inMemoryAdapter);

// Define the ProviderManagerConfig
const providerConfig: ProviderManagerConfig = {
  availableProviders: [
    {
      name: 'openai', // Unique identifier for this provider configuration
      adapter: OpenAIAdapter, // The adapter class
      // isLocal: false (default)
    },
    // Add other providers like Ollama, Anthropic, etc. here
  ],
  // Optional global limits
  // maxParallelApiInstancesPerProvider: 5,
  // apiInstanceIdleTimeoutSeconds: 300,
};

const config = {
  storage: cachingAdapter, // Pass the caching adapter instance
  providers: providerConfig, // Pass the ProviderManagerConfig
  // ... other config (agentCore, tools)
};

const art = await createArtInstance(config);
</code></pre>
                        </li>
                    </ol>
                    <p class="mt-4 text-gray-700">By following these steps, you can seamlessly integrate your custom storage solution(s) with ART's state management system without modifying the framework's core code, providing the flexibility to handle various persistence and caching requirements.</p>
                </div>

                <div id="provider-manager-architecture" class="mt-10">
                    <h3 class="text-xl font-medium mb-4 text-gray-800">2.4. Core Concept: Provider Manager Architecture</h3>
                     <p class="mb-4 text-gray-700">ART's provider manager system enables flexible runtime selection and management of multiple LLM providers within a single ART instance. This architecture allows applications to seamlessly switch between different providers (like OpenAI, Anthropic, or local models) based on user choice, task requirements, or other runtime conditions.</p>

                     <h4 class="text-lg font-medium mt-6 mb-3 text-gray-800">Core Goals:</h4>
                     <ul class="list-disc list-inside space-y-2 mb-6 text-gray-700">
                        <li><strong>Dynamic Provider Selection:</strong> Configure multiple potential LLM providers at initialization, but select specific providers and models at runtime.</li>
                        <li><strong>Resource Management:</strong> Control concurrent API usage, manage instance lifecycles, and enforce provider-specific constraints.</li>
                        <li><strong>Separation of Concerns:</strong> Clearly separate provider configuration (available providers) from runtime selection (which provider to use for a specific call).</li>
                     </ul>

                     <h4 class="text-lg font-medium mt-6 mb-3 text-gray-800">Key Components:</h4>
                     <div class="mermaid mb-8">
graph TD
    subgraph DeveloperSetup ["Node 1: Developer Interface"]
        Config["createArtInstance(config)"] --> PM_Config{ProviderManagerConfig}
        PM_Config --> AvailProvider1["Available: OpenAI\nAdapter Class, Limits"]
        PM_Config --> AvailProviderN["Available: Ollama\nAdapter Class, isLocal=true"]
        AppLogic["Application Logic"] -- Manages --> RuntimeCfg(RuntimeProviderConfig)
    end

    subgraph ARTCore ["Node 2: ART Core Orchestration"]
        Factory["AgentFactory"] -->|Creates| ProvMgr(ProviderManager)
        RE[ReasoningEngine] -->|Requests Instance| ProvMgr
        ProvMgr -- Manages --> InstancePool["Instance Pool/Cache"]
        ProvMgr -- Returns --> Accessor["ManagedAdapterAccessor\n(adapter + release())"]
        RE -- Uses --> Accessor
        AgentCore["IAgentCore"] -->|Creates| RuntimeCfg
        AgentCore -->|Calls with RuntimeCfg| RE
    end

    subgraph ExternalConnections ["Node 3: External Providers"]
        Accessor --> API["API Providers\n(OpenAI, Anthropic)"]
        Accessor --> Local["Local Providers\n(Ollama)"]
    end

    style ProvMgr fill:#f9d,stroke:#333,stroke-width:2px
    style RuntimeCfg fill:#lightgreen,stroke:#333,stroke-width:1px
    style Accessor fill:#lightblue,stroke:#333,stroke-width:1px

                     </div>

                     <h4 class="text-lg font-medium mt-6 mb-3 text-gray-800">1. Provider Registration (Initialization Time):</h4>
                     <ul class="list-disc list-inside space-y-2 mb-6 text-gray-700">
                        <li>
                            <strong><code>ProviderManagerConfig</code>:</strong> Defines available provider adapters and global management rules:
<pre><code class="language-typescript">interface ProviderManagerConfig {
    availableProviders: {
        name: string;          // e.g., 'openai', 'anthropic', 'ollama_local'
        adapter: new (...args) => ProviderAdapter; // The adapter class
        isLocal?: boolean;     // Determines singleton vs pooling behavior
    }[];
    maxParallelApiInstancesPerProvider?: number; // Default: 5
    apiInstanceIdleTimeoutSeconds?: number;      // Default: 300
}
</code></pre>
                        </li>
                         <li>
                            <strong>Example Configuration:</strong>
<pre><code class="language-typescript">const config = {
    providers: {
        availableProviders: [
            {
                name: 'openai',
                adapter: OpenAIAdapter,
                // Default API provider behavior
            },
            {
                name: 'ollama_local',
                adapter: OllamaAdapter,
                isLocal: true  // Enforces singleton behavior
            }
        ],
        maxParallelApiInstancesPerProvider: 3
    }
    // ... other ART config
};
</code></pre>
                        </li>
                    </ul>

                     <h4 class="text-lg font-medium mt-6 mb-3 text-gray-800">2. Runtime Provider Selection:</h4>
                    <ul class="list-disc list-inside space-y-2 mb-6 text-gray-700">
                        <li>
                            <strong><code>RuntimeProviderConfig</code>:</strong> Specifies the desired provider and settings for a specific LLM call:
<pre><code class="language-typescript">interface RuntimeProviderConfig {
    providerName: string;    // Must match a registered name
    modelId: string;         // e.g., 'gpt-4o', 'llama3:latest'
    adapterOptions: {        // Provider-specific options
        apiKey?: string;
        baseUrl?: string;
        temperature?: number;
        // ... other options
    };
    // Note: The combination of providerName, modelId, and sorted adapterOptions
    // forms a unique 'configSignature' used by the ProviderManager to identify
    // and cache distinct provider instances.
}
</code></pre>
                        </li>
                        <li>
                            The application typically stores this configuration in <code>ThreadConfig</code> via <code>StateManager</code>:
<pre><code class="language-typescript">await stateManager.setThreadConfigValue(threadId, 'runtimeProviderConfig', {
    providerName: 'openai',
    modelId: 'gpt-4o',
    adapterOptions: {
        apiKey: process.env.OPENAI_API_KEY
    }
});
</code></pre>
                        </li>
                    </ul>

                    <h4 class="text-lg font-medium mt-6 mb-3 text-gray-800">3. Instance Management Rules:</h4>
                    <ul class="list-disc list-inside space-y-2 mb-6 text-gray-700">
                        <li><strong>API Providers (isLocal: false):</strong>
                            <ul class="list-circle list-inside ml-6 space-y-1 text-sm mt-1">
                                <li>Multiple instances allowed up to `maxParallelApiInstancesPerProvider` per provider.</li>
                                <li>Instances are pooled and reused when configurations match.</li>
                                <li>Idle instances are evicted after `apiInstanceIdleTimeoutSeconds`.</li>
                                <li>Requests queue when provider limit is reached.</li>
                            </ul>
                        </li>
                        <li><strong>Local Providers (isLocal: true):</strong>
                            <ul class="list-circle list-inside ml-6 space-y-1 text-sm mt-1">
                                <li>   <strong>Unique Instance Definition:</strong> A local provider "instance" is uniquely identified by its `configSignature`, which is derived from the `providerName`, `modelId`, and `adapterOptions` specified in the `RuntimeProviderConfig`. Requesting the same `providerName` with a different `modelId` or `adapterOptions` is considered a request for a new, distinct instance.</li>
                                <li>   <strong>Active State:</strong> An instance becomes 'active' when it's acquired via `ProviderManager.getAdapter()`. It remains 'active' until `release()` is called on its `ManagedAdapterAccessor`.</li>
                                <li>   <strong>Idle State & `release()`:</strong>
                                    <ul class="list-disc list-inside ml-6 space-y-1 text-xs mt-1">
                                        <li>   The `ReasoningEngine` is responsible for calling `release()` on the adapter's accessor. This happens automatically after the LLM call stream is fully consumed or if an error occurs during the call/stream, effectively making the instance 'idle' in the `ProviderManager` after every token generation cycle.</li>
                                        <li>   Once 'idle', a local provider instance is available for immediate reuse if the *exact same* `RuntimeProviderConfig` is requested again.</li>
                                    </ul>
                                </li>
                                <li>   <strong>No Idle Timeouts:</strong> Local provider instances do not have automatic idle timeouts. They persist in an 'idle' state indefinitely.</li>
                                <li>   <strong>Replacement and Eviction:</strong>
                                    <ul class="list-disc list-inside ml-6 space-y-1 text-xs mt-1">
                                        <li>   An <strong>idle</strong> local provider instance <strong>is replaced</strong> (evicted) if a request for a *different* local provider configuration (i.e., different `configSignature`) is made.</li>
                                        <li>   During eviction, the `ProviderManager` will call the `shutdown()` method on the adapter, if the adapter implements it. This `shutdown()` method is crucial for the adapter to perform actual resource cleanup, such as unloading an LLM model from memory/VRAM. If the adapter doesn't implement `shutdown()` or it's a no-op, the underlying model might remain in memory based on the local LLM server's own logic, even if the adapter instance is "evicted" by ART.</li>
                                    </ul>
                                </li>
                                <li>   <strong>Strict Singleton Behavior for Active Instances:</strong>
                                    <ul class="list-disc list-inside ml-6 space-y-1 text-xs mt-1">
                                        <li>   Only <strong>one</strong> local provider instance (as defined by its unique `configSignature`) can be <strong>active</strong> at any given time across all local providers.</li>
                                        <li>   If a local provider instance is currently 'active':
                                            <ul class="list-circle list-inside ml-6 space-y-1 text-xs mt-1">
                                                <li>   Attempting to acquire a *different* local provider instance (different `providerName`, or same `providerName` but different `modelId` or `adapterOptions`) will result in a `LocalProviderConflictError`.</li>
                                                <li>   Attempting to acquire the *exact same* local provider instance (same `configSignature`) again while it's still active will result in a `LocalInstanceBusyError`.</li>
                                            </ul>
                                        </li>
                                    </ul>
                                </li>
                                <li>   <strong>Switching Models/Configurations:</strong> To "switch" from one local model/configuration to another, the currently active instance (if any) must first be released (become idle). Requesting the new configuration will then cause the previously idle instance to be evicted and the new one to be created.</li>
                            </ul>
                        </li>
                    </ul>

                    <h4 class="text-lg font-medium mt-6 mb-3 text-gray-800">4. Workflow Example:</h4>
                    <ol class="list-decimal list-inside space-y-3 mb-6 text-gray-700">
                        <li>
                            <strong>Application Prepares:</strong>
<pre><code class="language-typescript">// In your React component or application logic
const runtimeConfig: RuntimeProviderConfig = {
    providerName: 'openai',
    modelId: 'gpt-4o',
    adapterOptions: {
        apiKey: apiKey,
        temperature: 0.7
    }
};
await art.stateManager.setThreadConfigValue(threadId, 'runtimeProviderConfig', runtimeConfig);
</code></pre>
                        </li>
                        <li>
                             <strong>Agent Core Acts:</strong>
<pre><code class="language-typescript">// Inside PESAgent or custom IAgentCore implementation
async process(props: AgentProps) {
    const context = await this.deps.stateManager.loadThreadContext(threadId);
    const runtimeConfig = context.config.runtimeProviderConfig;
    
    // Create CallOptions with the runtime config
    const callOptions: CallOptions = {
        providerConfig: runtimeConfig,
        threadId: props.threadId,
        stream: true,
        // ... other options
    };
    
    // ReasoningEngine will use this config to get the right adapter
    const responseStream = await this.deps.reasoningEngine.call(prompt, callOptions);
    // ... continue processing
}
</code></pre>
                        </li>
                        <li>
                            <strong>Provider Manager Handles Request:</strong>
<pre><code class="language-typescript">// Inside ProviderManagerImpl
async getAdapter(config: RuntimeProviderConfig): Promise<ManagedAdapterAccessor> {
    // Check if config is for a local provider
    if (this.isLocalProvider(config.providerName)) {
        // Enforce local provider singleton rules
        await this.enforceLocalProviderConstraints(config);
    } else {
        // Check API provider limits & queue if needed
        await this.enforceApiProviderLimits(config);
    }
    
    // Get or create adapter instance
    const instance = await this.getOrCreateInstance(config);
    
    // Return accessor with release function
    return {
        adapter: instance.adapter,
        release: () => this.releaseInstance(instance)
    };
}
</code></pre>
                        </li>
                        <li>
                            <strong>ReasoningEngine Uses Adapter:</strong>
<pre><code class="language-typescript">// Inside ReasoningEngineImpl
async call(prompt: ArtStandardPrompt, options: CallOptions): Promise<AsyncIterable<StreamEvent>> {
    // Get managed adapter
    const accessor = await this.providerManager.getAdapter(options.providerConfig);
    
    try {
        // Create releasing generator that ensures adapter is released
        return {
            [Symbol.asyncIterator]: async function*() {
                try {
                    const stream = accessor.adapter.call(prompt, options);
                    for await (const event of stream) {
                        yield event;
                    }
                } finally {
                    // Always release the adapter back to the ProviderManager.
                    // This transitions the instance to 'idle' or allows another queued request to proceed.
                    accessor.release();
                }
            }
        };
    } catch (error) {
        // Release on error if the accessor was obtained before the error.
        // This ensures the instance doesn't get stuck in an 'active' state in the ProviderManager.
        accessor.release();
        throw error;
    }
}
</code></pre>
                        </li>
                    </ol>

                    <h4 class="text-lg font-medium mt-6 mb-3 text-gray-800">5. Error Scenarios:</h4>
                     <ul class="list-disc list-inside space-y-2 mb-6 text-gray-700">
                        <li>
                            <strong>Local Provider Conflict:</strong>
<pre><code class="language-typescript">// Trying to use Ollama while another local provider is active
throw new LocalProviderConflictError(
    `Cannot use Ollama while LMStudio is active. Only one local provider can be active at a time.`
);
</code></pre>
                        </li>
                         <li>
                            <strong>API Limit Reached:</strong>
<pre><code class="language-typescript">// All OpenAI slots in use (hitting maxParallelApiInstancesPerProvider)
// Request will be queued, or throw error if queueing disabled
throw new ProviderLimitError(
    `Maximum parallel instances (3) reached for OpenAI. Try again later.`
);
</code></pre>
                        </li>
                    </ul>
                    <p class="mb-4 text-gray-700">This architecture enables applications to provide flexible LLM provider selection while maintaining control over resource usage and enforcing necessary constraints. The separation between initialization-time configuration (`ProviderManagerConfig`) and runtime selection (`RuntimeProviderConfig`) allows for dynamic provider switching without sacrificing stability or control.</p>
                </div>
            </section>

            <section id="scenario-1" class="mb-16 fade-in-section">
                <h2 class="text-2xl md:text-3xl font-semibold mb-6 border-b pb-3 text-sky-700">3. Scenario 1: Building a Feature-Rich React Chatbot (Simple Usage)</h2>
                <p class="mb-5 text-gray-700">
                    Let's build a chatbot component for a React website using only ART's built-in features. We'll aim to showcase several core ART capabilities.
                </p>
                <p class="mb-8 text-gray-700"><strong>Goal:</strong> A chat interface where users can talk to an AI powered by OpenAI, with conversation history saved in the browser, and some real-time feedback using ART's observation system.</p>

                <div id="scenario-1-imports" class="mb-10">
                    <h3 class="text-xl font-medium mb-4 text-gray-800">3.1. Necessary Imports & Explanations</h3>
<pre><code class="language-typescript">// src/components/ArtChatbot.tsx
import React, { useState, useEffect, useRef, useCallback } from 'react';

// --- ART Core Imports ---
import {
  // The main factory function to initialize ART
  createArtInstance,
  // Type definition for the initialized ART object
  ArtInstance,
  // Type for the properties needed to call the agent's process method
  AgentProps,
  // Type for the final response object from the agent
  AgentFinalResponse,
  // Type representing a single message in the conversation
  ConversationMessage,
  // Enum defining message roles (USER, ASSISTANT, SYSTEM, TOOL)
  MessageRole,
  // Type representing an internal event/observation within ART
  Observation,
  // Enum defining different types of observations (PROCESS_START, LLM_REQUEST, etc.)
  ObservationType,
  // The default Plan-Execute-Synthesize agent pattern implementation
  PESAgent,
  // Interfaces for core components (needed for type hints, less for direct use here)
  StorageAdapter, ProviderAdapter, ReasoningEngine, IToolExecutor, IAgentCore,
  StateManager, ConversationManager, ToolRegistry, ObservationManager, UISystem,
  // New/Updated types for streaming and metadata
  StreamEvent, LLMMetadata, ExecutionMetadata, PromptManager, // Added PromptManager
  // Types for Provider Manager configuration
  ProviderManagerConfig, RuntimeProviderConfig, CallOptions // Added CallOptions
} from 'art-framework'; // Assuming 'art-framework' is the installed package name

// --- ART Adapter Imports (Developer Choices) ---
import {
  // Storage adapter that uses the browser's IndexedDB for persistence
  IndexedDBStorageAdapter,
  // Storage adapter that uses temporary browser memory (data lost on refresh)
  // InMemoryStorageAdapter, // Alternative, uncomment if preferred
} from 'art-framework'; // Adapters are usually exported from the main package too

import {
  // Reasoning provider adapter for OpenAI models (GPT-3.5, GPT-4, etc.)
  OpenAIAdapter,
  // Reasoning provider adapter for Google Gemini models
  // GeminiAdapter, // Alternative, uncomment if preferred
  // Reasoning provider adapter for Anthropic Claude models
  // AnthropicAdapter,
} from 'art-framework';

// --- ART Built-in Tool Imports (Optional) ---
import {
  // A simple tool that can evaluate mathematical expressions
  CalculatorTool
} from 'art-framework';
</code></pre>
                    <h4 class="text-lg font-medium mt-6 mb-4 text-gray-800">Explanation of Imports:</h4>
                    <div class="space-y-4 text-gray-700">
                        <div>
                            <p><strong><code>createArtInstance</code></strong></p>
                            <p class="mb-2">This is the main function you use to start the ART framework. Think of it as the "ignition key" â€“ you give it instructions (configuration), and it builds and starts the ART engine for you.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                An asynchronous factory function (`async function createArtInstance(config: AgentFactoryConfig): Promise&lt;ArtInstance&gt;`). Takes `config` (object conforming to the `AgentFactoryConfig` interface: `{ storage: StorageAdapter, providers: ProviderManagerConfig, tools?: IToolExecutor[], agentCore?: new (deps: any) => IAgentCore, logger?: { level?: LogLevel } }`). Uses `AgentFactory` internally to instantiate and inject dependencies for all core components (Managers, Systems, Repositories, Adapters, ProviderManager). Returns a `Promise` resolving to the fully initialized `ArtInstance` object. Typically called once at application setup.
                            </details>
                        </div>
                         <div>
                            <p><strong><code>ArtInstance</code></strong></p>
                            <p class="mb-2">This describes the main control panel you get after starting ART. It's the object that lets you interact with the initialized framework, primarily by telling it to process user messages.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                A TypeScript interface defining the public API returned by <code>createArtInstance</code>. Key properties:
                                <ul class="list-disc list-inside ml-4">
                                    <li><code>process(props: AgentProps): Promise&lt;AgentFinalResponse&gt;</code>: The core method to run the agent's reasoning cycle.</li>
                                    <li><code>conversationManager: ConversationManager</code>: Access methods like <code>getMessages</code>, <code>addMessages</code>.</li>
                                    <li><code>stateManager: StateManager</code>: Access methods like <code>loadThreadContext</code>, <code>setThreadConfigValue</code>, `getThreadConfigValue`, `getAgentState`, `setAgentState`, `isToolEnabled`.</li>
                                    <li><code>toolRegistry: ToolRegistry</code>: Access methods like <code>registerTool</code>, <code>getToolExecutor</code>, <code>getAvailableTools</code>.</li>
                                    <li><code>observationManager: ObservationManager</code>: Access methods like <code>record</code>, <code>getObservations</code>.</li>
                                    <li><code>uiSystem: UISystem</code>: Access methods like <code>getObservationSocket</code>, <code>getConversationSocket</code>, and <code>getLLMStreamSocket</code> (for streaming) to get subscription interfaces.</li>
                                </ul>
                            </details>
                        </div>
                         <div>
                            <p><strong><code>AgentProps</code></strong></p>
                            <p class="mb-2">Describes the information you need to give the agent each time you want it to respond (your message and which chat it belongs to).</p>
                            <details>
                                <summary>Developer Notes</summary>
                                Interface for the input object to <code>ArtInstance.process()</code>.
                                <ul class="list-disc list-inside ml-4">
                                    <li>Required: <code>query: string</code> (user input), <code>threadId: string</code> (conversation ID).</li>
                                    <li>Optional: `configOverrides?: Partial<ThreadConfig>` (can include `runtimeProviderConfig` to override the thread's default provider for this call), `executionContext?: Record<string, any>`, `userId?: string`, `sessionId?: string`.</li>
                                </ul>
                            </details>
                        </div>
                         <div>
                            <p><strong><code>AgentFinalResponse</code></strong></p>
                             <p class="mb-2">Describes the information the agent gives back after processing your request (its reply and some tracking info).</p>
                             <details>
                                 <summary>Developer Notes</summary>
                                 Interface for the output object from `ArtInstance.process()`. Contains the final AI message and execution metadata.
                                 <ul class="list-disc list-inside ml-4">
                                     <li>`response: ConversationMessage`: The final message object generated by the AI (includes `id`, `content`, `threadId`, `role`, `timestamp`, etc.).</li>
                                     <li>`metadata: ExecutionMetadata`: Contains detailed metadata about the execution cycle (e.g., `traceId`, duration, status, aggregated `llmMetadata`).</li>
                                 </ul>
                             </details>
                        </div>
                         <div>
                            <p><strong><code>ConversationMessage</code></strong></p>
                            <p class="mb-2">How each chat bubble's information (who sent it, what it says, when) is organized.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                Interface representing a message. Properties: <code>id: string</code>, <code>role: MessageRole</code>, <code>content: string</code>, <code>timestamp: number</code>, <code>threadId: string</code>, <code>metadata?: Record&lt;string, any&gt;</code>. Used by <code>ConversationManager</code> (via <code>StorageAdapter</code>) and often directly in UI rendering logic.
                            </details>
                        </div>
                         <div>
                            <p><strong><code>MessageRole</code></strong></p>
                            <p class="mb-2">Labels to know if a message is from the User, the AI Assistant, the System (e.g., errors, info), or a Tool (results).</p>
                            <details>
                                <summary>Developer Notes</summary>
                                TypeScript enum: <code>USER</code>, <code>ASSISTANT</code>, <code>SYSTEM</code>, <code>TOOL</code>. Crucial for structuring prompts for the LLM (differentiating user input from previous AI responses) and for UI display logic.
                            </details>
                        </div>
                         <div>
                            <p><strong><code>Observation</code></strong></p>
                            <p class="mb-2">A notification about something happening inside the agent's brain while it's working (like "Thinking..." or "Using calculator...").</p>
                            <details>
                                <summary>Developer Notes</summary>
                                Interface for internal events. Properties: <code>id: string</code>, <code>timestamp: number</code>, <code>threadId: string</code>, <code>traceId?: string</code>, <code>type: ObservationType</code>, <code>content: any</code>, <code>metadata?: Record&lt;string, any&gt;</code>. Emitted by <code>ObservationManager</code> and broadcast via <code>UISystem</code>'s <code>ObservationSocket</code>. Useful for real-time UI updates (status indicators) and debugging.
                            </details>
                        </div>
                         <div>
                            <p><strong><code>ObservationType</code></strong></p>
                            <p class="mb-2">Labels for the different types of internal notifications (like "Started thinking", "Asking the AI", "Finished using a tool").</p>
                            <details>
                                <summary>Developer Notes</summary>
                                TypeScript enum listing event types (e.g., <code>PROCESS_START</code>, <code>LLM_REQUEST</code>, <code>LLM_RESPONSE</code>, <code>TOOL_START</code>, <code>TOOL_END</code>, <code>PLANNING_OUTPUT</code>, <code>SYNTHESIS_OUTPUT</code>, <code>PROCESS_END</code>, <code>REACT_STEP</code>, <code>thought</code>, <code>action</code>, <code>observation`). Includes new types for discrete streaming events: `LLM_STREAM_START`, `LLM_STREAM_METADATA`, `LLM_STREAM_END`, `LLM_STREAM_ERROR`. Used to categorize `Observation` events and filter subscriptions on the `ObservationSocket`.
                            </details>
                        </div>
                         <div>
                            <p><strong><code>StreamEvent</code></strong></p>
                            <p class="mb-2">Represents a single piece of information arriving from the LLM's real-time stream (like a word, statistics, or an end signal).</p>
                            <details>
                                <summary>Developer Notes</summary>
                                Interface defining the structure of events yielded by the `ReasoningEngine.call` async iterable. Properties: <code>type</code> ('TOKEN', 'METADATA', 'ERROR', 'END'), <code>data</code> (content), <code>tokenType</code> (classification like 'LLM_THINKING', 'FINAL_SYNTHESIS_LLM_RESPONSE'), <code>threadId</code>, <code>traceId</code>, <code>sessionId</code>. Consumed by the Agent Core and pushed to the <code>LLMStreamSocket</code>.
                            </details>
                        </div>
                          <div>
                            <p><strong><code>LLMMetadata</code></strong></p>
                            <p class="mb-2">A structured way to hold detailed statistics about an LLM call (token counts, timing, etc.).</p>
                            <details>
                                <summary>Developer Notes</summary>
                                Interface defining the structure for LLM statistics. Properties: <code>inputTokens?</code>, <code>outputTokens?</code>, <code>thinkingTokens?</code>, <code>timeToFirstTokenMs?</code>, <code>totalGenerationTimeMs?</code>, <code>stopReason?</code>, <code>providerRawUsage?</code>, <code>traceId?</code>. Delivered via <code>StreamEvent</code> (type 'METADATA') and aggregated into <code>ExecutionMetadata.llmMetadata</code>.
                            </details>
                        </div>
                         <div>
                            <p><strong><code>PESAgent</code></strong></p>
                            <p class="mb-2">The specific "thinking style" the agent will use by default (Plan -> Use Tools -> Answer).</p>
                            <details>
                                <summary>Developer Notes</summary>
                                Concrete class implementing <code>IAgentCore</code>. Instantiated by <code>AgentFactory</code> if specified in <code>config.agentCore</code> or if `agentCore` is omitted. Receives dependencies (`StateManager`, `ReasoningEngine`, `ToolSystem`, `UISystem`, etc.) in its constructor. Its `process` method orchestrates the Plan-Execute-Synthesize flow, determines the `RuntimeProviderConfig` for each LLM call, interacts with the injected dependencies, and handles the consumption and processing of the `AsyncIterable<StreamEvent>` from the `ReasoningEngine`.
                            </details>
                        </div>
                         <div>
                            <p><strong><code>PromptManager</code></strong></p>
                            <p class="mb-2">Provides reusable prompt text fragments (like instructions or formatting rules) and validates the final prompt object constructed by the agent logic before it's sent to the LLM.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                The `PromptManager` interface provides `getFragment(name, context)` to retrieve text blocks and `validatePrompt(promptObject)` to check the structure and types of the `ArtStandardPrompt` object created by the agent. Agent implementations (like `PESAgent`) are responsible for constructing the prompt object, potentially using fragments, and then calling `validatePrompt`.
                            </details>
                        </div>
                        <div>
                            <p><strong><code>ReasoningEngine</code></strong></p>
                            <p class="mb-2">This component is responsible for interacting with the configured `ProviderAdapter`. Its `call` method now returns a `Promise<AsyncIterable<StreamEvent>>`, allowing for real-time streaming of LLM responses.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                The `ReasoningEngine` interface's `call` method signature has been updated. Agent implementations consume this `AsyncIterable` to process the stream events.
                            </details>
                        </div>
                         <div>
                            <p><strong><code>UISystem</code></strong></p>
                            <p class="mb-2">This system provides access to communication channels for the UI. It now includes a dedicated <code>LLMStreamSocket</code> for broadcasting real-time <code>StreamEvent</code>s from the LLM.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                The <code>UISystem</code> interface includes a <code>getLLMStreamSocket(): LLMStreamSocket</code> method. UI components subscribe to this socket to receive and display streaming tokens and other stream events.
                            </details>
                        </div>
                         <div>
                            <p><strong><code>IndexedDBStorageAdapter</code> / <code>InMemoryStorageAdapter</code></strong></p>
                            <p class="mb-2">How the agent remembers the conversation. <code>IndexedDB</code> is like saving to a file (remembers after closing), <code>InMemory</code> is like writing on a whiteboard (erased when closed).</p>
                            <details>
                                <summary>Developer Notes</summary>
                                Concrete classes implementing <code>StorageAdapter</code> (`get`, `set`, `delete`, `query`). Passed directly in `config.storage` during `createArtInstance`. Used by internal Repositories. `IndexedDB` provides persistence across browser sessions; `InMemory` does not.
                            </details>
                        </div>
                         <div>
                            <p><strong><code>OpenAIAdapter</code> / <code>GeminiAdapter</code> / <code>AnthropicAdapter</code></strong></p>
                            <p class="mb-2">The specific translator the agent uses to talk to a particular AI brain (like OpenAI's GPT, Google's Gemini, or Anthropic's Claude).</p>
                            <details>
                                <summary>Developer Notes</summary>
                                Concrete classes implementing <code>ProviderAdapter</code>. Registered by *class* in `config.providers.availableProviders`. The `ProviderManager` instantiates these adapters at runtime, passing `RuntimeProviderConfig.adapterOptions` to their constructor. The `call` method returns `Promise<AsyncIterable<StreamEvent>>` to support streaming. Adapters must check `options.stream` and `options.callContext`.
                            </details>
                        </div>
                         <div>
                            <p><strong><code>CalculatorTool</code></strong></p>
                            <p class="mb-2">A specific skill the agent can use, like a pocket calculator.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                Concrete class implementing <code>IToolExecutor</code>. Provides <code>schema</code> (`name`, `description`, `inputSchema`) and `execute(input, context)`. Instances are passed in `config.tools`. Registered with `ToolRegistry` and executed by `ToolSystem` when planned by the `IAgentCore`.
                            </details>
                        </div>
                         <div>
                            <p><strong><code>ProviderManagerConfig</code> / <code>RuntimeProviderConfig</code></strong></p>
                            <p class="mb-2">Configuration types for managing and selecting LLM providers.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                <code>ProviderManagerConfig</code> is passed to `createArtInstance` to register available provider adapter *classes*. <code>RuntimeProviderConfig</code> is determined by the application/agent at runtime (often stored in `ThreadConfig` or passed via `AgentProps.configOverrides`) and passed in `CallOptions` to specify which provider/model/options to use for a specific LLM call.
                            </details>
                        </div>
                    </div>
                </div>

                <div id="scenario-1-component" class="mb-8">
                     <h3 class="text-xl font-medium mb-4 text-gray-800">3.2. React Component Implementation</h3>
<pre><code class="language-typescript jsx">// src/components/ArtChatbot.tsx
import React, { useState, useEffect, useRef, useCallback } from 'react';
import {
  createArtInstance, ArtInstance, AgentProps, AgentFinalResponse,
  ConversationMessage, MessageRole, Observation, ObservationType, PESAgent,
  StreamEvent, // Import StreamEvent for socket subscription
  ProviderManagerConfig, RuntimeProviderConfig // Import provider config types
} from 'art-framework';
import { IndexedDBStorageAdapter } from 'art-framework'; // Or InMemoryStorageAdapter
import { OpenAIAdapter } from 'art-framework'; // Or GeminiAdapter, etc.
import { CalculatorTool } from 'art-framework';

// Basic CSS (add this to a corresponding CSS file or use styled-components/tailwind)
/*
.chatbot-container { max-width: 600px; margin: auto; border: 1px solid #ccc; border-radius: 8px; display: flex; flex-direction: column; height: 70vh; }
.message-list { flex-grow: 1; overflow-y: auto; padding: 10px; display: flex; flex-direction: column; }
.message { margin-bottom: 10px; padding: 8px 12px; border-radius: 15px; max-width: 80%; word-wrap: break-word; }
.message.USER { background-color: #dcf8c6; align-self: flex-end; border-bottom-right-radius: 0; }
.message.ASSISTANT { background-color: #f1f0f0; align-self: flex-start; border-bottom-left-radius: 0; }
.message.SYSTEM, .message.TOOL { background-color: #e0e0e0; font-style: italic; font-size: 0.9em; align-self: center; text-align: center; }
.message.ASSISTANT.streaming { background-color: #e6f7ff; /* Lighter blue for streaming */ }
.input-area { display: flex; padding: 10px; border-top: 1px solid #ccc; }
.input-area input { flex-grow: 1; padding: 10px; border: 1px solid #ccc; border-radius: 20px; margin-right: 10px; }
.input-area button { padding: 10px 15px; border: none; background-color: #007bff; color: white; border-radius: 20px; cursor: pointer; }
.input-area button:disabled { background-color: #aaa; cursor: not-allowed; }
.status-indicator { padding: 5px 10px; font-size: 0.8em; color: #666; text-align: center; height: 20px; }
*/

// Helper to generate temporary IDs
const tempId = () => `temp-${Date.now()}-${Math.random().toString(16).slice(2)}`;

const ArtChatbot: React.FC = () => {
  const [messages, setMessages] = useState&lt;ConversationMessage[]&gt;([]);
  const [input, setInput] = useState('');
  const [isLoading, setIsLoading] = useState(false);
  const [status, setStatus] = useState&lt;string&gt;('Initializing...'); // For observation feedback
  const artInstanceRef = useRef&lt;ArtInstance | null&gt;(null);
  const messageListRef = useRef&lt;HTMLDivElement>(null); // For auto-scrolling
  const threadId = 'web-chatbot-thread-1'; // Consistent ID for this chat instance
  const streamingMessageIdRef = useRef<string | null>(null); // Track the ID of the message being streamed

  // --- Auto-scrolling ---
  useEffect(() => {
    if (messageListRef.current) {
      messageListRef.current.scrollTop = messageListRef.current.scrollHeight;
    }
  }, [messages]);

  // --- ART Initialization ---
  useEffect(() => {
    let isMounted = true; // Prevent state updates on unmounted component
    let unsubObservation: (() => void) | null = null;
    let unsubConversation: (() => void) | null = null;
    let unsubStream: (() => void) | null = null; // Subscription for LLM stream

    const initializeArt = async () => {
      if (!artInstanceRef.current) {
        try {
          setStatus('Initializing ART Engine...');

          // Define Provider Configuration
          const providerConfig: ProviderManagerConfig = {
              availableProviders: [
                {
                  name: 'openai', // Name used in RuntimeProviderConfig
                  adapter: OpenAIAdapter, // Pass the adapter class
                }
                // Add other providers like GeminiAdapter, AnthropicAdapter here if needed
              ]
              // Optional global limits can be set here
          };

          // Define ART Factory Configuration
          const config = {
            storage: new IndexedDBStorageAdapter({ dbName: 'artWebChatHistory' }), // Use IndexedDB adapter directly
            providers: providerConfig, // Pass the provider config
            agentCore: PESAgent, // Explicitly using the default
            tools: [new CalculatorTool()] // Include the calculator
          };

          const instance = await createArtInstance(config);
          if (!isMounted) return; // Check if component unmounted during async init

          artInstanceRef.current = instance;
          setStatus('Loading history...');
          await loadMessages(); // Load history after successful init

          // --- Subscribe to Observations (UI Feedback) ---
          setStatus('Connecting observers...');
          const observationSocket = instance.uiSystem.getObservationSocket();
          unsubObservation = observationSocket.subscribe(
            (observation: Observation) => {
              if (observation.threadId === threadId && isMounted) {
                // Simple status updates based on observations
                let newStatus = status;
                switch (observation.type) {
                  case ObservationType.LLM_REQUEST: newStatus = 'Asking AI...'; break;
                  case ObservationType.TOOL_START: newStatus = `Using ${observation.metadata?.toolName}...`; break;
                  case ObservationType.TOOL_END: newStatus = 'Tool finished.'; break;
                  case ObservationType.PROCESS_START: newStatus = 'Processing request...'; break;
                  case ObservationType.PROCESS_END:
                      newStatus = 'Ready.';
                      streamingMessageIdRef.current = null; // Clear streaming ID on process end
                      break;
                  // Add new stream observations if needed for status
                  case ObservationType.LLM_STREAM_START: newStatus = 'Receiving response...'; break;
                  case ObservationType.LLM_STREAM_END: newStatus = 'Response received.'; break;
                  case ObservationType.LLM_STREAM_ERROR: newStatus = 'Stream error.'; break;
                }
                 // Avoid overwriting status if currently processing
                 if (!isLoading) {
                     setStatus(newStatus);
                 }
              }
            },
            undefined, // Subscribe to all relevant types
            { threadId: threadId } // Filter for this specific chat thread
          );

           // --- Subscribe to Conversation (Update UI with Final Messages) ---
           const conversationSocket = instance.uiSystem.getConversationSocket();
           unsubConversation = conversationSocket.subscribe(
             (message: ConversationMessage) => {
               if (message.threadId === threadId && isMounted) {
                 console.log("Received final message via socket:", message);
                 setMessages(prev => {
                   // Replace the temporary streaming message with the final one
                   const existingIndex = prev.findIndex(m => m.id === streamingMessageIdRef.current && m.role === MessageRole.ASSISTANT);
                   if (existingIndex > -1) {
                     const updatedMessages = [...prev];
                     updatedMessages[existingIndex] = message;
                     return updatedMessages;
                   } else if (!prev.some(m => m.id === message.id)) {
                     // Add if not already present (e.g., user message from another client)
                     return [...prev, message].sort((a, b) => a.timestamp - b.timestamp);
                   }
                   return prev;
                 });
                  streamingMessageIdRef.current = null; // Clear after receiving final message
               }
             },
             undefined, // No role filter
             { threadId: threadId }
           );


          // --- Subscribe to LLM Stream (Real-time Token Updates) ---
          const streamSocket = instance.uiSystem.getLLMStreamSocket();
          unsubStream = streamSocket.subscribe(
            (event: StreamEvent) => {
              if (event.threadId === threadId && isMounted) {
                 // Focus on displaying tokens for the final synthesis stage
                if (event.type === 'TOKEN' && event.tokenType === 'FINAL_SYNTHESIS_LLM_RESPONSE') {
                  setMessages(prev => {
                    const currentStreamingId = streamingMessageIdRef.current;
                    if (!currentStreamingId) {
                      // Start of a new streaming message
                      const newStreamingMessage: ConversationMessage = {
                        id: tempId(), // Use temporary ID
                        role: MessageRole.ASSISTANT,
                        content: event.data,
                        timestamp: Date.now(),
                        threadId: threadId,
                        metadata: { streaming: true } // Mark as streaming
                      };
                      streamingMessageIdRef.current = newStreamingMessage.id;
                      return [...prev, newStreamingMessage];
                    } else {
                      // Append token to existing streaming message
                      return prev.map(msg =>
                        msg.id === currentStreamingId
                          ? { ...msg, content: msg.content + event.data }
                          : msg
                      );
                    }
                  });
                } else if (event.type === 'END') {
                    // Handled by PROCESS_END observation or ConversationSocket update
                    // streamingMessageIdRef.current = null; // Clear streaming ID
                } else if (event.type === 'METADATA') {
                    console.log("Stream Metadata:", event.data);
                    // Optionally display metadata
                } else if (event.type === 'ERROR') {
                    console.error("Stream Error:", event.data);
                    setStatus('Stream Error');
                    // Optionally display error message in UI
                    setMessages(prev => [...prev, {
                         id: tempId(), role: MessageRole.SYSTEM, content: `Stream Error: ${event.data.message || event.data}`,
                         timestamp: Date.now(), threadId: threadId
                    }]);
                     streamingMessageIdRef.current = null; // Clear streaming ID on error
                }
              }
            },
            { threadId: threadId } // Filter stream events for this thread
          );


          if (isMounted) setStatus('Ready.');

        } catch (error) {
          console.error("Failed to initialize ART:", error);
          if (isMounted) setStatus(`Initialization Error: ${error instanceof Error ? error.message : 'Unknown error'}`);
        }
      }
    };

    initializeArt();

    // Cleanup function
    return () => {
      isMounted = false;
      console.log("Cleaning up ART subscriptions...");
      if (unsubObservation) unsubObservation();
      if (unsubConversation) unsubConversation();
      if (unsubStream) unsubStream(); // Unsubscribe from stream socket
    };
  }, [threadId]); // Rerun if threadId changes (it doesn't in this example)

  // --- Load Messages ---
  const loadMessages = useCallback(async () => {
    if (!artInstanceRef.current) return;
    try {
      // Use a local loading flag for history fetch to avoid interfering with process loading
      // setIsLoading(true); // Removed this, rely on status indicator
      const history = await artInstanceRef.current.conversationManager.getMessages(threadId, { limit: 100 });
      setMessages(history.sort((a, b) => a.timestamp - b.timestamp)); // Sort oldest to newest
    } catch (error) {
      console.error("Failed to load messages:", error);
      setStatus('Error loading history.');
    } finally {
      // setIsLoading(false); // Removed this
    }
  }, [threadId]);

  // --- Handle Sending ---
  const handleSend = useCallback(async () => {
    if (!input.trim() || !artInstanceRef.current || isLoading) return;

    const userMessage: ConversationMessage = {
      id: `user-${Date.now()}`, // Consider using UUIDs for production
      role: MessageRole.USER,
      content: input,
      timestamp: Date.now(),
      threadId: threadId,
    };

    // Add user message optimistically
    setMessages(prev => [...prev, userMessage]);
    // Clear streaming ref in case a previous stream was interrupted
    streamingMessageIdRef.current = null;

    const currentInput = input; // Capture input before clearing
    setInput('');
    setIsLoading(true);
    setStatus('Processing request...'); // Initial status, will be updated by observations

    try {
       // Define the runtime provider configuration for this specific call
       const runtimeConfig: RuntimeProviderConfig = {
         providerName: 'openai', // Must match the name registered in createArtInstance config.providers
         modelId: 'gpt-4o', // Specify the desired model for this call
         adapterOptions: {
           apiKey: import.meta.env.VITE_OPENAI_API_KEY || 'YOUR_OPENAI_API_KEY', // Provide API key at runtime
           // Add other OpenAI specific options if needed, e.g., temperature
           // temperature: 0.7
         }
       };

      // Prepare the properties for the agent process call
      // Pass the runtimeConfig via configOverrides. The AgentCore will extract this
      // and pass it within CallOptions to the ReasoningEngine.
      const props: AgentProps = {
        query: currentInput,
        threadId: threadId,
        configOverrides: {
          runtimeProviderConfig: runtimeConfig
        }
      };

      // process() now resolves AFTER the stream is complete and the final message is saved.
      // The UI updates come via the LLMStreamSocket and ConversationSocket.
      const response: AgentFinalResponse = await artInstanceRef.current.process(props);

      console.log("ART process completed. Final Response:", response);
      // The final AI message is now part of the AgentFinalResponse object
      // The UI will also get this via the ConversationSocket subscription,
      // which handles replacing the temporary streamed message.
      // However, for robustness or if ConversationSocket is not strictly relied upon for this update:
      if (response.response && !messages.some(m => m.id === response.response.id)) {
          const aiMessage: ConversationMessage = {
            ...response.response, // Spread the properties from the nested ConversationMessage
            id: response.response.id || `ai-${Date.now()}`, // Ensure ID exists or generate fallback
            timestamp: response.response.timestamp || Date.now(), // Use provided timestamp or fallback
            metadata: { ...response.response.metadata, traceId: response.metadata.traceId }
          };
           // This might cause a duplicate if ConversationSocket also adds it.
           // The ConversationSocket handler is better suited for replacing the streaming message.
           // Consider only adding here if the message isn't already present from the socket.
          setMessages(prev => {
              const streamingId = streamingMessageIdRef.current;
              if (streamingId && prev.some(m => m.id === streamingId)) {
                  // If we were streaming, let socket handle replacement
                  return prev;
              }
              // If not streaming or socket missed it, add it
              return [...prev.filter(m => m.id !== aiMessage.id), aiMessage].sort((a, b) => a.timestamp - b.timestamp);
          });
      }


    } catch (error) {
      console.error("Error processing message:", error);
      const errorMessage: ConversationMessage = {
        id: `error-${Date.now()}`,
        role: MessageRole.SYSTEM,
        content: `Error: ${error instanceof Error ? error.message : 'Failed to get response'}`,
        timestamp: Date.now(),
        threadId: threadId,
      };
      setMessages(prev => [...prev, errorMessage]);
      setStatus('Error occurred.');
       streamingMessageIdRef.current = null; // Clear streaming ID on error
    } finally {
      setIsLoading(false); // Set loading false only after process() finishes
      // Status should update to 'Ready.' via PROCESS_END observation if successful
    }
  }, [input, isLoading, threadId, messages]); // Added messages to dep array for robust final message handling

  // --- Render Component ---
  return (
    &lt;div className="chatbot-container"&gt;
      &lt;div className="message-list" ref={messageListRef}&gt;
        {messages.map((msg) => (
          &lt;div key={msg.id} className={`message ${msg.role} ${msg.metadata?.streaming ? 'streaming' : ''}`}&gt;
            {/* Simple rendering, consider markdown parsing for content */}
            &lt;pre style={{ whiteSpace: 'pre-wrap', margin: 0, fontFamily: 'inherit' }}&gt;{msg.content}&lt;/pre&gt;
          &lt;/div&gt;
        ))}
      &lt;/div&gt;
      &lt;div className="status-indicator"&gt;{isLoading ? status : 'Ready.'}&lt;/div&gt; {/* Show status while loading */}
      &lt;div className="input-area"&gt;
        &lt;input
          type="text"
          value={input}
          onChange={(e) => setInput(e.target.value)}
          onKeyPress={(e) => e.key === 'Enter' && !isLoading && handleSend()}
          disabled={isLoading || !artInstanceRef.current}
          placeholder={artInstanceRef.current ? "Ask something..." : "Initializing..."}
        /&gt;
        &lt;button onClick={handleSend} disabled={isLoading || !artInstanceRef.current || !input.trim()}&gt;
          {isLoading ? '...' : 'Send'}
        &lt;/button&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  );
};

export default ArtChatbot;

</code></pre>
                    <h4 class="text-lg font-medium mt-6 mb-4 text-gray-800">Explanation of Features Used:</h4>
                    <ol class="list-decimal list-inside space-y-3 text-gray-700">
                        <li><strong>Initialization (`createArtInstance`):</strong> Sets up ART with a configured `StorageAdapter` (IndexedDB), registers available `ProviderAdapter` *classes* (OpenAIAdapter) via `ProviderManagerConfig`, specifies the default `agentCore` (PESAgent), and includes a built-in `CalculatorTool`.</li>
                        <li><strong>Conversation Management (`conversationManager.getMessages`):</strong> Loads previous messages from storage when the component mounts, providing history. Messages are saved implicitly by the `PESAgent` after `process` completes. The final AI message is available in `AgentFinalResponse.response`.</li>
                        <li><strong>State Management (`StateManager`):</strong> Used internally by ART to load thread configuration (like the `RuntimeProviderConfig`) and potentially save agent state between turns.</li>
                        <li><strong>Provider Management (`ProviderManager`):</strong> Manages the lifecycle of `ProviderAdapter` instances based on the configuration provided during initialization.</li>
                        <li><strong>Reasoning (`PESAgent`, `ReasoningEngine`):</strong> Handles the core logic of understanding the query, planning, and synthesizing the response. The `PESAgent` determines the appropriate `RuntimeProviderConfig` for each LLM call (using overrides or thread defaults), passes it via `CallOptions` to the `ReasoningEngine`. The `ReasoningEngine` gets the correct adapter instance from the `ProviderManager` and handles the streaming interaction.</li>
                        <li><strong>Tools (`CalculatorTool`, `ToolSystem`):</strong> The calculator is available. If the user asks "What is 5*5?", the `PESAgent` should plan to use it, the `ToolSystem` will execute it, and the result will inform the final answer.</li>
                        <li><strong>Storage (`IndexedDBStorageAdapter`):</strong> Ensures conversation history persists even if the user closes and reopens the browser tab.</li>
                        <li><strong>Observations & UI Sockets (`uiSystem`):</strong> The UI subscribes to the `ObservationSocket` for discrete events, the `LLMStreamSocket` for real-time `StreamEvent`s (tokens, metadata, errors, end signals) from the LLM, and the `ConversationSocket` to receive the final, persisted message after streaming is complete.</li>
                    </ol>
                     <p class="mt-6 text-gray-700">
                        This component provides a solid foundation, demonstrating the core ART features working together in a practical application, including the new streaming and enhanced observation capabilities.
                    </p>
                </div>

                 <div id="scenario-1-workflow" class="mt-10">
                    <h3 class="text-xl font-medium mb-4 text-gray-800">3.3. Detailed Internal Workflow: `art.process()` with PESAgent</h3>
                    <p class="mb-4 text-gray-700">When you call `art.process()` using the default Plan-Execute-Synthesize (PES) agent, a sequence of steps occurs internally to understand your request, potentially use tools, and generate a response. Hereâ€™s a breakdown, with both technical details and simpler explanations:</p>
                    <ol class="list-decimal list-inside space-y-4 text-gray-700">
                        <li>
                            <strong>Call Received:</strong> <code>PESAgent.process(props)</code> starts.
                            <details>
                                <summary>Simple Terms</summary>
                                The agent receives your query and gets ready to work.
                            </details>
                        </li>
                        <li>
                            <strong>Record Start:</strong> Log <code>PROCESS_START</code> observation.
                             <details>
                                <summary>Simple Terms</summary>
                                The agent makes a note that it has started processing a new request. This helps track what's happening internally.
                            </details>
                        </li>
                        <li>
                            <strong>Load Context:</strong> Fetch <code>ThreadConfig</code> and <code>AgentState</code> via <code>StateManager</code>.
                            <details>
                                <summary>Simple Terms</summary>
                                The agent retrieves any specific settings for this conversation (like which AI model to use) and any memory it has about the current state of the conversation.
                            </details>
                        </li>
                        <li>
                            <strong>Load History:</strong> Fetch recent <code>ConversationMessage</code>s via <code>ConversationManager</code>.
                            <details>
                                <summary>Simple Terms</summary>
                                The agent looks up the recent messages exchanged in this specific chat thread to understand the context.
                            </details>
                        </li>
                        <li>
                            <strong>Get Available Tools:</strong> Fetch enabled <code>ToolSchema</code>s via <code>ToolRegistry</code> (using `StateManager` to check permissions).
                            <details>
                                <summary>Simple Terms</summary>
                                The agent checks which tools (like a calculator or weather lookup) it's allowed to use in this conversation.
                            </details>
                        </li>
                        <li>
                            <strong>Construct Planning Prompt:</strong> The agent logic (e.g., `PESAgent`) constructs the `ArtStandardPrompt` object for planning. It combines the query, history, system prompt (from `ThreadConfig`), and tool schemas. It might use `PromptManager.getFragment()` to retrieve standard instruction texts. It then calls `PromptManager.validatePrompt()` on the constructed object.
                            <details>
                                <summary>Simple Terms</summary>
                                The agent builds the instructions for the AI brain (the LLM). It includes your query, the chat history, its available tools, and asks the AI to figure out a plan, including whether any tools are needed. It double-checks the instructions are formatted correctly.
                            </details>
                        </li>
                        <li>
                             <strong>Execute Planning LLM Call & Stream Processing:</strong>
                             <ul class="list-disc list-inside ml-6 space-y-1 text-sm mt-1">
                                 <li>Determine `RuntimeProviderConfig` (from `props.configOverrides` or thread defaults).</li>
                                 <li>Create `CallOptions` including the `providerConfig`, `stream: true`, `callContext: 'AGENT_THOUGHT'`.</li>
                                 <li>Log `LLM_REQUEST` observation.</li>
                                 <li>Call `ReasoningEngine.call(planningPrompt, callOptions)`.</li>
                                 <li>`ReasoningEngine` requests the adapter instance from `ProviderManager`.</li>
                                 <li>`ReasoningEngine` calls the adapter, gets the `AsyncIterable<StreamEvent>`.</li>
                                 <li>The Agent Core consumes the returned `AsyncIterable<StreamEvent>`.</li>
                                 <li>For each `StreamEvent` received:
                                     <ul class="list-circle list-inside ml-6 space-y-1 text-xs mt-1">
                                         <li>If `type` is `TOKEN`, it's pushed to `uiSystem.getLLMStreamSocket()` and potentially buffered for the final response (if `tokenType` indicates final output).</li>
                                         <li>If `type` is `METADATA`, it's pushed to `uiSystem.getLLMStreamSocket()`, recorded as an `LLM_STREAM_METADATA` observation, and aggregated.</li>
                                         <li>If `type` is `ERROR`, it's pushed to `uiSystem.getLLMStreamSocket()` and recorded as an `LLM_STREAM_ERROR` observation.</li>
                                         <li>If `type` is `END`, it's pushed to `uiSystem.getLLMStreamSocket()` and recorded as an `LLM_STREAM_END` observation.</li>
                                     </ul>
                                 </li>
                                 <li>The `finally` block of the generator (within `ReasoningEngine`) ensures the adapter instance is released back to `ProviderManager`.</li>
                                 <li>Log `LLM_RESPONSE` observation (with aggregated content & metadata) after stream ends.</li>
                             </ul>
                             <details class="mt-1">
                                <summary>Simple Terms</summary>
                                The agent figures out which AI connection to use, sends the planning instructions to the AI brain (e.g., OpenAI's GPT-4). It then starts receiving the response piece by piece, broadcasting to the UI, logging events, collecting stats, and ensuring the AI connection is released.
                            </details>
                         </li>
                        <li>
                            <strong>Parse Planning Output:</strong> After the planning LLM call stream ends, use `OutputParser` to extract intent, plan description, and <code>ParsedToolCall[]</code> from the *aggregated* LLM response content.
                            <details>
                                <summary>Simple Terms</summary>
                                The agent reads the complete planning response (assembled from the streamed tokens) and tries to understand the plan it came up with, specifically looking for which tools (if any) the AI wants to use and what information to give them.
                            </details>
                        </li>
                        <li>
                            <strong>Record Plan:</strong> Log <code>PLANNING_OUTPUT</code> observation.
                            <details>
                                <summary>Simple Terms</summary>
                                The agent notes down the plan it received from the AI.
                            </details>
                        </li>
                        <li>
                            <strong>Execute Tools (if <code>ParsedToolCall[]</code> is not empty):</strong>
                            <ul class="list-disc list-inside ml-6 space-y-1 text-sm mt-1">
                                <li>Call <code>ToolSystem.executeTools()</code>.</li>
                                <li><code>ToolSystem</code> iterates through calls: validates tool enablement (`StateManager`), gets executor (`ToolRegistry`), validates args, logs <code>TOOL_START</code>, calls <code>executor.execute()</code>, logs <code>TOOL_END</code> with result/error.</li>
                                <li>Log <code>TOOL_EXECUTION_COMPLETE</code> observation.</li>
                            </ul>
                             <details class="mt-1">
                                <summary>Simple Terms</summary>
                                If the plan requires using tools, the agent now runs them one by one. For each tool, it checks if it's allowed, gets the tool ready, gives it the necessary information (e.g., the city for the weather tool), runs the tool, and records the result (or any errors).
                            </details>
                        </li>
                        <li>
                            <strong>Construct Synthesis Prompt:</strong> The agent logic constructs the `ArtStandardPrompt` object for synthesis. It combines the original query, plan, tool results, history, and the resolved system prompt. It might use `PromptManager.getFragment()` for instructions. It then calls `PromptManager.validatePrompt()` on the constructed object.
                            <details>
                                <summary>Simple Terms</summary>
                                The agent gathers everything â€“ your original query, the AI's plan, the results from any tools used, and the chat history â€“ and builds the final instructions for the AI brain. This time, it asks the AI to write the answer you will see. It double-checks these instructions are formatted correctly.
                            </details>
                        </li>
                        <li>
                            <strong>Execute Synthesis LLM Call & Stream Processing:</strong>
                            <ul class="list-disc list-inside ml-6 space-y-1 text-sm mt-1">
                                 <li>Determine `RuntimeProviderConfig` (e.g., from `AgentProps.configOverrides` or thread defaults - same as planning step).</li>
                                 <li>Create `CallOptions` including the `providerConfig`, `stream: true`, `callContext: 'FINAL_SYNTHESIS'`.</li>
                                 <li>Log `LLM_REQUEST`.</li>
                                 <li>Call `ReasoningEngine.call(synthesisPrompt, callOptions)`.</li>
                                 <li>`ReasoningEngine` requests the appropriate adapter instance from `ProviderManager` based on `providerConfig`.</li>
                                 <li>`ReasoningEngine` calls adapter, gets `AsyncIterable<StreamEvent>`.</li>
                                 <li>`ReasoningEngine` returns the stream wrapped in a releasing generator.</li>
                                 <li>The Agent Core consumes the `AsyncIterable` (releasing generator), pushes `TOKEN`, `METADATA`, `ERROR`, and `END` events to `uiSystem.getLLMStreamSocket()`, records `METADATA`, `ERROR`, and `END` as observations, and buffers final response tokens. The `finally` block of the generator ensures the adapter instance is released.</li>
                             </ul>
                            <details class="mt-1">
                                <summary>Simple Terms</summary>
                                The agent figures out which AI connection to use again, sends the final instructions, and starts receiving the final response piece by piece, broadcasting these to the UI for display, logging events, collecting stats, and ensuring the AI connection is released.
                            </details>
                        </li>
                        <li>
                            <strong>Parse Synthesis Output:</strong> After the synthesis LLM call stream ends, use `OutputParser` to extract the final `responseText` from the *aggregated* LLM response content.
                            <details>
                                <summary>Simple Terms</summary>
                                The agent reads the complete final response (assembled from the streamed tokens) and extracts the actual chat message to send back to you.
                            </details>
                        </li>
                        <li>
                            <strong>Record Synthesis:</strong> Log <code>SYNTHESIS_OUTPUT</code> observation.
                            <details>
                                <summary>Simple Terms</summary>
                                The agent notes down the final answer it generated.
                            </details>
                        </li>
                        <li>
                            <strong>Save History:</strong> Persist user query and AI response via <code>ConversationManager</code>.
                            <details>
                                <summary>Simple Terms</summary>
                                The agent saves your query and its final response to the chat history so they can be remembered for later.
                            </details>
                        </li>
                        <li>
                            <strong>Save State:</strong> Persist any changes to <code>AgentState</code> via <code>StateManager</code>.
                            <details>
                                <summary>Simple Terms</summary>
                                If the agent learned something or changed its internal state during the process, it saves that information.
                            </details>
                        </li>
                        <li>
                            <strong>Record End:</strong> Log <code>PROCESS_END</code> observation.
                            <details>
                                <summary>Simple Terms</summary>
                                The agent notes down that it has finished processing your request.
                            </details>
                        </li>
                        <li>
                            <strong>Return Result:</strong> Return the `AgentFinalResponse` object. This object contains:
                            <ul class="list-disc list-inside ml-6 text-sm mt-1">
                                <li>`response`: A `ConversationMessage` object representing the final AI message.</li>
                                <li>`metadata`: An `ExecutionMetadata` object containing the `traceId`, aggregated `llmMetadata`, and other execution details.</li>
                            </ul>
                            <details class="mt-1">
                                <summary>Simple Terms</summary>
                                The agent sends the final response message and collected statistics back to the part of the application that called it (e.g., the chatbot UI).
                            </details>
                        </li>
                    </ol>
                </div>
            </section>

            <section id="scenario-2" class="mb-16 fade-in-section">
                <h2 class="text-2xl md:text-3xl font-semibold mb-6 border-b pb-3 text-sky-700">4. Scenario 2: Adding a Custom Tool (Intermediate Usage)</h2>
                <p class="mb-5 text-gray-700">
                    Now, let's extend our chatbot by adding a custom tool that provides current information like date, time, and approximate location/locale.
                </p>
                 <p class="mb-8 text-gray-700"><strong>Goal:</strong> Create a <code>CurrentInfoTool</code> and integrate it into the ART configuration.</p>

                 <div id="scenario-2-simple-explanation" class="mb-10 p-4 bg-blue-50 border border-blue-200 text-blue-800 rounded-md text-sm">
                     <h3 class="text-lg font-medium mb-2 text-blue-900">4.0 Simplified Explanation for Developers</h3>
                     <p class="mb-2">Imagine ART is like a highly capable smart assistant you've hired for your application. This assistant comes with some built-in abilities (like a calculator), but its real power is that you can easily teach it *new* skills that you create yourself.</p>
                     <ol class="list-decimal list-inside mt-3 space-y-1">
                         <li><strong>Creating Your Custom Skill (Your Tool):</strong> You, as the developer, define the new skill you want the assistant to have. This involves writing the code for that skill (your custom tool) and describing what it does and what information it needs to work. ART provides a standard way to define these skills (an "interface" called `IToolExecutor`). You just need to make sure your skill follows this standard format so the ART assistant can understand it.</li>
                         <li><strong>Giving the Skill to the Assistant:</strong> When you set up the ART assistant for your application (using the `createArtInstance` function from the ART package), you provide it with a configuration. This configuration is like giving the assistant its instructions and resources. Crucially, this configuration includes a list of *all* the skills you want the assistant to have. You add your newly created custom skill to this list, along with any of ART's built-in skills you want to use.</li>
                         <li><strong>The Assistant Learns Your Skill:</strong> When `createArtInstance` runs, the ART framework reads your configuration. It sees the list of skills you provided and adds them to its internal "skill library" (the `ToolRegistry`).</li>
                     </ol>
                     <p class="mt-3 mb-2">So, to integrate your custom tool without modifying the ART framework's source code:</p>
                     <ul class="list-disc list-inside mt-2 space-y-1">
                         <li>You create your custom tool's code in a file within your application's project structure (like a `tools` folder).</li>
                         <li>Inside that file, you define a class for your tool and make sure it follows the rules defined by ART's `IToolExecutor` interface.</li>
                         <li>In the part of your application where you set up ART (where you call `createArtInstance`), you import the custom tool class you just created.</li>
                         <li>When you call `createArtInstance`, you pass a configuration object. Within this object, there's a `tools` array. You create a new instance of your custom tool class (`new YourCustomTool()`) and include it in this array.</li>
                     </ul>
                     <p class="mt-3">By doing this, you're effectively handing your custom skill to the ART assistant during its setup. ART then knows about your tool and how to use it when needed, all without you having to touch the core ART framework code itself.</p>
                 </div>

                <div id="scenario-2-imports" class="mb-10">
                     <h3 class="text-xl font-medium mb-4 text-gray-800">4.1. Necessary Imports & Explanations</h3>
                     <p class="mb-4 text-gray-700">In addition to the imports from Scenario 1, you'll need these specifically for creating a tool:</p>
<pre><code class="language-typescript">// --- ART Tool Creation Imports ---
import {
  // The interface that every tool must implement
  IToolExecutor,
  // The type defining the tool's description, name, and input/output schemas
  ToolSchema,
  // The type defining the structure of the result returned by a tool's execute method
  ToolResult,
  // The type providing context (like threadId, traceId) to the tool's execute method
  ExecutionContext
} from 'art-framework';
</code></pre>
                    <h4 class="text-lg font-medium mt-6 mb-4 text-gray-800">Explanation of Tool Imports:</h4>
                     <div class="space-y-4 text-gray-700">
                         <div>
                            <p><strong><code>IToolExecutor</code></strong></p>
                            <p class="mb-2">The blueprint or set of rules your custom skill needs to follow so ART knows how to use it.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                The core interface for creating custom tools. Your tool class must implement this. Key requirements:
                                <ul class="list-disc list-inside ml-4">
                                    <li>Implement a readonly <code>schema</code> property of type <code>ToolSchema</code>.</li>
                                    <li>Implement an <code>async execute(input: any, context: ExecutionContext): Promise&lt;ToolResult&gt;</code> method. This method receives validated <code>input</code> (based on <code>schema.inputSchema</code>) and the <code>context</code> object. It should perform the tool's action and return a <code>Promise</code> resolving to a <code>ToolResult</code>.</li>
                                </ul>
                            </details>
                        </div>
                         <div>
                            <p><strong><code>ToolSchema</code></strong></p>
                            <p class="mb-2">The tool's "instruction manual" for the AI â€“ its name, what it does, and what information it needs to run.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                Interface defining the tool's metadata, used by both the LLM (via prompts) and the <code>ToolSystem</code>. Properties:
                                <ul class="list-disc list-inside ml-4">
                                    <li><code>name: string</code>: The unique function name the LLM will use to call the tool (e.g., "get_current_weather"). Use snake_case.</li>
                                    <li><code>description: string</code>: Detailed explanation for the LLM about the tool's purpose, capabilities, and when it should be used. Crucial for effective tool selection by the LLM.</li>
                                    <li><code>inputSchema: object</code>: A standard JSON Schema object describing the expected structure, types (string, number, boolean, object, array), required fields, and descriptions for the <code>input</code> argument of the <code>execute</code> method. Used by <code>ToolSystem</code> to validate arguments before execution.</li>
                                </ul>
                            </details>
                        </div>
                         <div>
                            <p><strong><code>ToolResult</code></strong></p>
                            <p class="mb-2">The format for the tool's answer â€“ whether it worked, and either the result or an error message.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                Interface defining the object returned by <code>IToolExecutor.execute</code>. Properties:
                                <ul class="list-disc list-inside ml-4">
                                    <li><code>status: 'success' | 'error'</code>: Must indicate the outcome.</li>
                                    <li><code>output?: any</code>: Required if <code>status</code> is 'success'. Contains the result data. Aim for JSON-serializable data (strings, numbers, booleans, arrays, plain objects) so the LLM can easily understand and incorporate it into its response.</li>
                                    <li><code>error?: string</code>: Required if <code>status</code> is 'error'. Provides a descriptive error message for logging and potentially for the LLM to understand the failure.</li>
                                </ul>
                            </details>
                        </div>
                         <div>
                            <p><strong><code>ExecutionContext</code></strong></p>
                            <p class="mb-2">Extra information passed to your tool when it runs, like which chat it's running for, useful for tracking or context-specific logic.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                Interface for the context object passed as the second argument to <code>IToolExecutor.execute</code>. Provides runtime context. Properties:
                                <ul class="list-disc list-inside ml-4">
                                    <li><code>threadId: string</code>: The ID of the conversation thread this execution belongs to.</li>
                                    <li><code>traceId?: string</code>: The ID tracing the entire <code>ArtInstance.process</code> call, useful for correlating logs across multiple steps and tool calls within a single user request.</li>
                                    <li>May contain other properties passed down from the <code>AgentProps</code> or added by the <code>IAgentCore</code> implementation.</li>
                                </ul>
                            </details>
                        </div>
                    </div>
                </div>

                <div id="scenario-2-implementation" class="mb-10">
                    <h3 class="text-xl font-medium mb-4 text-gray-800">4.2. Implementing the <code>CurrentInfoTool</code></h3>
<pre><code class="language-typescript">// src/tools/CurrentInfoTool.ts (or define within the component file for simplicity)

import { IToolExecutor, ToolSchema, ToolResult, ExecutionContext } from 'art-framework';

export class CurrentInfoTool implements IToolExecutor {
  readonly schema: ToolSchema = {
    name: "get_current_info",
    description: "Provides the current date, time, approximate user location (requires permission), and browser language/locale.",
    inputSchema: { // No specific input needed for this tool
      type: "object",
      properties: {},
    }
  };

  async execute(input: any, context: ExecutionContext): Promise&lt;ToolResult&gt; {
    console.log(`Executing CurrentInfoTool, Trace ID: ${context.traceId}`);
    try {
      const now = new Date();
      const dateTimeInfo = {
        date: now.toLocaleDateString(),
        time: now.toLocaleTimeString(),
        timezoneOffset: now.getTimezoneOffset(), // In minutes from UTC
        isoString: now.toISOString(),
      };

      let locationInfo: any = { status: 'permission_denied_or_unavailable' };
      try {
        // Use browser Geolocation API - Requires HTTPS and user permission
        if ('geolocation' in navigator) {
          locationInfo = await new Promise((resolve, reject) => {
            navigator.geolocation.getCurrentPosition(
              (position) => {
                resolve({
                  status: 'success',
                  latitude: position.coords.latitude,
                  longitude: position.coords.longitude,
                  accuracy: position.coords.accuracy, // In meters
                });
              },
              (error) => {
                // Handle errors (PERMISSION_DENIED, POSITION_UNAVAILABLE, TIMEOUT)
                resolve({ status: 'error', code: error.code, message: error.message });
              },
              { timeout: 5000 } // Set a timeout
            );
          });
        }
      } catch (geoError) {
         console.warn("Geolocation API error:", geoError);
         // Error already captured in the promise resolution
      }


      const localeInfo = {
        language: navigator.language, // e.g., "en-US"
        languages: navigator.languages, // Array of preferred languages
      };

      // Note: Getting local currency reliably client-side is complex.
      // We'll just include the locale as a hint.

      return {
        status: "success",
        output: {
          dateTime: dateTimeInfo,
          location: locationInfo,
          locale: localeInfo,
        }
      };
    } catch (error) {
      console.error("CurrentInfoTool Error:", error);
      return { status: "error", error: error instanceof Error ? error.message : "Unknown error fetching current info" };
    }
  }
}
</code></pre>
                    <h4 class="text-lg font-medium mt-6 mb-4 text-gray-800">Explanation:</h4>
                    <ol class="list-decimal list-inside space-y-3 text-gray-700">
                        <li><strong>Implement <code>IToolExecutor</code>:</strong> The class declares it follows the tool contract.</li>
                        <li><strong>Define <code>schema</code>:</strong> Provides the name (<code>get_current_info</code>), description, and specifies no required input.</li>
                        <li><strong>Implement <code>execute</code>:</strong>
                            <ul class="list-disc list-inside ml-6 space-y-2 mt-2">
                                <li>Gets the current date/time using the <code>Date</code> object.</li>
                                <li>Attempts to get the location using the browser's <code>navigator.geolocation</code> API. This is asynchronous and requires user permission (and usually HTTPS). It handles success and error cases gracefully.</li>
                                <li>Gets browser language/locale using <code>navigator.language(s)</code>.</li>
                                <li>Bundles all collected information into the <code>output</code> field of a successful <code>ToolResult</code>.</li>
                                <li>Includes error handling for unexpected issues.</li>
                            </ul>
                        </li>
                    </ol>
                </div>

                <div id="scenario-2-integration" class="mb-8">
                    <h3 class="text-xl font-medium mb-4 text-gray-800">4.3. Integrating the Tool into the Chatbot</h3>
                    <p class="mb-4 text-gray-700">Modify the ART configuration within the <code>ArtChatbot</code> component's <code>useEffect</code> hook:</p>
<pre><code class="language-typescript jsx">// Inside the useEffect hook in ArtChatbot.tsx

import { CurrentInfoTool } from './tools/CurrentInfoTool'; // Adjust path if needed
import { OpenAIAdapter, ProviderManagerConfig } from 'art-framework'; // Import adapter class and provider config type
import { PESAgent } from 'art-framework'; // Import agent core
import { CalculatorTool } from 'art-framework'; // Import built-in tool
import { IndexedDBStorageAdapter } from 'art-framework'; // Import storage adapter

// ... inside initializeArt function ...
          // Define Provider Configuration
          const providerConfig: ProviderManagerConfig = {
              availableProviders: [
                {
                  name: 'openai', // Identifier for this provider setup
                  adapter: OpenAIAdapter, // Pass the adapter CLASS
                }
              ]
          };
          const config = {
            storage: new IndexedDBStorageAdapter({ dbName: 'artWebChatHistory' }), // Pass storage adapter instance
            providers: providerConfig, // Pass the ProviderManagerConfig
            agentCore: PESAgent, // Specify agent core class
            tools: [
                new CalculatorTool(),
                new CurrentInfoTool() // Add an instance of the new tool
            ]
          };

          const instance = await createArtInstance(config);
// ... rest of the initialization ...
</code></pre>
                    <h4 class="text-lg font-medium mt-6 mb-4 text-gray-800">How it Works Now:</h4>
                    <ul class="list-disc list-inside space-y-3 text-gray-700">
                         <li><strong>Node 1 (Developer Interface):</strong> You've defined the <code>CurrentInfoTool</code> and told ART about it by adding `new CurrentInfoTool()` to the `tools` array in the configuration. You've also registered available LLM provider adapter *classes* (like `OpenAIAdapter`) in the `providers` configuration.</li>
                        <li><strong>Node 2 (Core Orchestration):</strong> When the user asks something like "What time is it?", the `PESAgent` gathers context data (including the query, history, and tool schemas like `get_current_info`). It constructs the `ArtStandardPrompt` object directly (possibly using fragments from `PromptManager`) and validates it with `PromptManager.validatePrompt()`. It determines the `RuntimeProviderConfig` (specifying provider name, model, API key, etc., likely from `AgentProps.configOverrides` or `ThreadConfig`), creates `CallOptions`, and calls `ReasoningEngine`. The `ReasoningEngine` uses the `RuntimeProviderConfig` to request the appropriate adapter instance (e.g., `OpenAIAdapter`) from the `ProviderManager` and sends the prompt via the adapter's `call` method. The `OutputParser` parses the LLM's response stream. If the LLM plans to use `get_current_info`, the `ToolSystem` finds and executes your tool. The results are used in the synthesis step, which again involves constructing and validating an `ArtStandardPrompt`, determining `RuntimeProviderConfig`, `ReasoningEngine`, `ProviderManager`, and the LLM adapter to generate the final response stream.</li>
                        <li><strong>Node 3 (External Connections):</strong> The `CurrentInfoTool` interacts with browser APIs (`Date`, `navigator.geolocation`, `navigator.language`). The specific `ProviderAdapter` instance (instantiated and managed by `ProviderManager` based on the `RuntimeProviderConfig`) handles communication with the external LLM API (e.g., OpenAI), including processing streaming responses.</li>
                    </ul>
                </div>
            </section>

            <section id="scenario-3" class="mb-16 fade-in-section">
                <h2 class="text-2xl md:text-3xl font-semibold mb-6 border-b pb-3 text-sky-700">5. Scenario 3: Adding a Custom Provider Adapter (Anthropic Example)</h2>
                <p class="mb-5 text-gray-700">
                    Sometimes, you might want to connect ART to an LLM provider that isn't supported out-of-the-box, like Anthropic's Claude models, or perhaps use a proxy or a self-hosted model with a unique API. This requires creating a custom Provider Adapter.
                </p>
                 <p class="mb-8 text-gray-700"><strong>Goal:</strong> Implement a functional <code>AnthropicAdapter</code> using the Anthropic Messages API, supporting streaming.</p>

                 <div id="scenario-3-simple-explanation" class="mb-10 p-4 bg-blue-50 border border-blue-200 text-blue-800 rounded-md text-sm">
                     <h3 class="text-lg font-medium mb-2 text-blue-900">5.0 Simplified Explanation for Developers</h3>
                     <p class="mb-2">Think of ART as that smart assistant again. This assistant needs to talk to different "AI brains" (Large Language Models like GPT, Gemini, Claude, Ollama, etc.) to get its work done. But each AI brain speaks a slightly different language (their API format).</p>
                     <ol class="list-decimal list-inside mt-3 space-y-1">
                         <li><strong>Creating a Translator (Your Custom Provider Adapter):</strong> If you want the ART assistant to be able to talk to a *new* AI brain it doesn't already know, you need to create a special "translator" for that specific AI brain. This translator is what we call a <strong>Provider Adapter</strong>. Your job is to write the code for this translator. ART provides a standard blueprint (the `ProviderAdapter` interface) that your translator must follow. This blueprint ensures that your translator knows how to:
                             <ul class="list-disc list-inside ml-6 text-xs space-y-1">
                                 <li>Receive instructions from the ART assistant in a standard format (the `ArtStandardPrompt`).</li>
                                 <li>Translate those standard instructions into the specific language the new AI brain understands (its API format).</li>
                                 <li>Send the translated request to the new AI brain.</li>
                                 <li>Receive the response back from the new AI brain (including handling streaming responses).</li>
                                 <li>Translate the AI brain's response back into a standard format that the ART assistant can understand (`AsyncIterable<StreamEvent>`).</li>
                             </ul>
                         </li>
                         <li><strong>Registering the Translator:</strong> When you set up the ART assistant using `createArtInstance`, you provide a configuration (`ProviderManagerConfig`) that lists all the available translators (Provider Adapters) the assistant *could* use. You add your custom adapter *class* to this list, giving it a unique name (e.g., 'anthropic').</li>
                         <li><strong>The Assistant Selects and Uses Your Translator:</strong> When the ART assistant needs to talk to an AI brain (when the `ReasoningEngine` is called), the application logic (usually the `AgentCore`) decides *which* translator to use for that specific call (based on user choice, task requirements, etc., often stored in `ThreadConfig`). It tells the `ProviderManager` the name of the desired translator (e.g., 'anthropic') and any specific options (like API key, model). The `ProviderManager` then creates an instance of your custom adapter class (using the options provided) and gives it to the `ReasoningEngine` to make the call.</li>
                     </ol>
                      <p class="mt-3 mb-2">So, in simple terms:</p>
                      <p class="mb-2">You create a custom Provider Adapter that acts as a translator for a specific LLM API, making sure it follows ART's standard `ProviderAdapter` blueprint. Then, when you initialize ART in your application, you tell it about your custom adapter *class*. When your application needs to use that specific LLM, it provides the necessary runtime details (like API key and model), and ART's internal `ProviderManager` handles creating and using an instance of your translator. You don't need to change any of ART's core files; you just provide your new component during the setup process.</p>
                      <p class="mt-2 mb-2">This allows you to connect ART to virtually any LLM provider by writing a single translator for that provider, without altering the core framework.</p>
                     <h4 class="text-lg font-medium mt-4 mb-2 text-blue-900">How to Create and Use Your Custom Adapter:</h4>
                     <ol class="list-decimal list-inside mt-2 space-y-1">
                         <li><strong>Create Your Adapter File:</strong> Create a new file in your application's project, perhaps in a folder like `llm-adapters` or `providers`. For example, `anthropic-adapter.ts`.</li>
                         <li><strong>Import Necessary ART Components:</strong> Inside your adapter file, import the required types and interfaces from `art-framework`. Key imports include:
                             <ul class="list-disc list-inside ml-6 text-xs space-y-1">
                                 <li>`ProviderAdapter`: The interface your adapter class must implement.</li>
                                 <li>`ArtStandardPrompt`: The input format your adapter's `call` method will receive.</li>
                                 <li>`CallOptions`: Contains options for the LLM call (like `stream` and `callContext`).</li>
                                 <li>`StreamEvent`: The format for events yielded by your adapter's `call` method when streaming.</li>
                                 <li>`LLMMetadata`: The format for metadata events.</li>
                             </ul>
                         </li>
                         <li><strong>Implement Your Adapter Class:</strong> Create a class that implements the `ProviderAdapter` interface. This class will contain the logic to:
                              <ul class="list-disc list-inside ml-6 text-xs space-y-1">
                                 <li>Receive the `ArtStandardPrompt` and `CallOptions` in its `call` method.</li>
                                 <li>Translate the `ArtStandardPrompt` into the specific API request format for your LLM provider (e.g., Anthropic).</li>
                                 <li>Make the API call (using `fetch` or a library), handling both non-streaming and streaming responses.</li>
                                 <li>If streaming, parse the provider's stream chunks and yield `StreamEvent` objects (`TOKEN`, `METADATA`, `ERROR`, `END`), ensuring correct `tokenType` based on `callContext` and provider markers.</li>
                                 <li>If not streaming, make the call, parse the full response, and yield a minimal sequence of `StreamEvent`s.</li>
                                 <li>Extract and include `LLMMetadata` in `METADATA` events.</li>
                                 <li>The constructor will receive options (like API keys, base URLs) when the `ProviderManager` instantiates the adapter at runtime, based on the `RuntimeProviderConfig`.</li>
                                 <li><strong>Implement `async shutdown(): Promise<void>` (Optional but Recommended for Local Providers):</strong> If your adapter manages persistent connections or significant resources (especially true for local providers that might load models into memory/VRAM), implement this optional method. The `ProviderManager` will call `shutdown()` when an instance is evicted (e.g., an idle local provider being replaced by a new one, or an API provider timing out). This method is your adapter's opportunity to gracefully release those resources (e.g., tell a local LLM server to unload a model, close database connections). Without a proper `shutdown()` implementation, resources might not be cleaned up effectively when an adapter instance is discarded by the `ProviderManager`.</li>
                             </ul>
                         </li>
                         <li><strong>Import and Register in `createArtInstance`:</strong> In the file where you initialize ART, import your custom adapter class. In the configuration object passed to `createArtInstance`, include your adapter class in the `providers.availableProviders` array within the `ProviderManagerConfig`:
<pre><code class="language-typescript">import { createArtInstance, IndexedDBStorageAdapter } from 'art-framework';
import { ProviderManagerConfig, AgentFactoryConfig } from 'art-framework';
import { AnthropicAdapter } from './llm-adapters/anthropic-adapter'; // Import your custom adapter class

const config: AgentFactoryConfig = { // Ensure correct type for the main config object
  storage: new IndexedDBStorageAdapter({ dbName: 'myAppHistory' }), // Pass instance
  providers: {
    availableProviders: [
      {
        name: 'anthropic', // Unique identifier for this provider configuration
        adapter: AnthropicAdapter, // Pass the adapter CLASS
        // isLocal: true // Indicate if it's a local provider (optional, defaults to false)
      },
      // You could also register built-in adapters here, e.g., OpenAIAdapter
    ],
    // Optional global limits
    // maxParallelApiInstancesPerProvider: 5,
    // apiInstanceIdleTimeoutSeconds: 300,
  },
  // ... other config (agentCore, tools)
};

const art = await createArtInstance(config);
</code></pre>
                         </li>
                     </ol>
                      <p class="mt-3">By following these steps, you can seamlessly register your custom LLM provider adapter with ART, allowing the `ProviderManager` to instantiate and use it at runtime.</p>
                 </div>

                 <div id="scenario-3-imports" class="mb-10">
                     <h3 class="text-xl font-medium mb-4 text-gray-800">5.1. Necessary Imports & Explanations</h3>
<pre><code class="language-typescript">// --- ART Provider Adapter Creation Imports ---
import {
  // The base interface for LLM provider adapters
  ProviderAdapter,
  // The core interface for making LLM calls (ProviderAdapter extends this) - Now returns AsyncIterable<StreamEvent>
  ReasoningEngine,
  // Type for standardized prompts (array of messages) ART uses internally
  ArtStandardPrompt,
  ArtStandardMessage,
  // Type for options passed to the LLM call (model params, streaming flags, context, etc.)
  CallOptions,
  // Type for conversation messages (may be part of ArtStandardPrompt context)
  ConversationMessage,
  // Enum for message roles
  MessageRole,
  // Types for streaming output
  StreamEvent,
  LLMMetadata,
  // Type for runtime provider config passed in CallOptions
  RuntimeProviderConfig
} from 'art-framework';

// --- Potentially types from Anthropic SDK if used, or define manually ---
// Example manual types for Anthropic Messages API
interface AnthropicMessage {
  role: 'user' | 'assistant';
  // Content can be string or complex array for tool use/results
  content: string | Array<{ type: 'text', text: string } | { type: 'tool_use', id: string, name: string, input: any } | { type: 'tool_result', tool_use_id: string, content: string, is_error?: boolean }>;
}
interface AnthropicRequestBody {
  model: string;
  messages: AnthropicMessage[];
  system?: string;
  max_tokens: number;
  temperature?: number;
  stop_sequences?: string[];
  stream?: boolean; // Added for streaming
  // Potentially add tool definitions if using Anthropic's native tool support
  // tools?: Array<{ name: string, description: string, input_schema: object }>;
}
// Type for non-streaming or final aggregated response
interface AnthropicResponse {
  content: Array<{ type: 'text', text: string } | { type: 'tool_use', id: string, name: string, input: any }>;
  stop_reason?: string;
  usage?: { input_tokens: number, output_tokens: number };
  // ... other fields
}
</code></pre>
                    <h4 class="text-lg font-medium mt-6 mb-4 text-gray-800">Explanation of Provider Adapter Imports:</h4>
                     <div class="space-y-4 text-gray-700">
                         <div>
                             <p><strong><code>ProviderAdapter</code></strong></p>
                             <p class="mb-2">The blueprint for creating a translator between ART's general way of thinking about AI models and the specific way a particular AI provider's API works (like Anthropic).</p>
                             <details>
                                 <summary>Developer Notes</summary>
                                 The interface your custom LLM adapter class must implement. It extends <code>ReasoningEngine</code>, meaning it must primarily implement the <code>call</code> method (which now returns `Promise<AsyncIterable<StreamEvent>>`). It also requires a <code>readonly providerName: string</code> property to identify the adapter (e.g., 'anthropic').
                             </details>
                         </div>
                         <div>
                             <p><strong><code>ReasoningEngine</code></strong></p>
                             <p class="mb-2">Defines the basic capability of making a call to an AI model with a prompt. `ProviderAdapter` builds upon this.</p>
                             <details>
                                 <summary>Developer Notes</summary>
                                 The base interface defining the core `async call(prompt: ArtStandardPrompt, options: CallOptions): Promise<AsyncIterable<StreamEvent>>` method signature (updated for streaming and standardized prompt). Your `ProviderAdapter` implementation provides the concrete logic for this method, typically returning an async generator function.
                             </details>
                         </div>
                         <div>
                             <p><strong><code>ArtStandardPrompt</code> / <code>ArtStandardMessage</code></strong></p>
                             <p class="mb-2">Represents the standardized, provider-agnostic instructions prepared for the AI model by the `PromptManager`. This is now an array of `ArtStandardMessage` objects.</p>
                             <details>
                                 <summary>Developer Notes</summary>
                                 Your adapter's <code>call</code> method receives this standard format (`ArtStandardPrompt`) from the `ReasoningEngine` and is responsible for translating it into the specific message structure and format required by the target LLM provider's API (e.g., mapping roles, handling content types, structuring tool calls/results).
                             </details>
                         </div>
                         <div>
                             <p><strong><code>CallOptions</code></strong></p>
                             <p class="mb-2">Additional settings and information passed along when making the AI call. Crucially includes `providerConfig` (containing the target provider, model, and adapter options), `stream` flag, and `callContext`.</p>
                             <details>
                                 <summary>Developer Notes</summary>
                                 Interface for the options object passed to `ReasoningEngine.call`. Includes `threadId`, `traceId`, `sessionId`, `stream?: boolean`, `callContext?: string`, and `providerConfig: RuntimeProviderConfig`. Your adapter's `call` method uses `providerConfig` to access runtime options and model details, and checks `stream` and `callContext`.
                             </details>
                         </div>
                         <div>
                             <p><strong><code>ConversationMessage</code>, <code>MessageRole</code></strong></p>
                             <p class="mb-2">Needed within the adapter's prompt translation logic to correctly interpret the roles and content within the received `ArtStandardPrompt` and map them to the provider's expected format.</p>
                             <details>
                                 <summary>Developer Notes</summary>
                                 Used within the adapter's `call` method (specifically in the `formatMessages` helper) during the prompt translation step before sending the request to the Anthropic API. System messages, user messages, assistant messages, tool requests, and tool results from the `ArtStandardPrompt` need careful mapping to Anthropic's structure.
                             </details>
                         </div>
                         <div>
                             <p><strong><code>StreamEvent</code>, <code>LLMMetadata</code></strong></p>
                             <p class="mb-2">These are the types your adapter's `call` method will yield via its `AsyncIterable` return value when streaming is enabled.</p>
                             <details>
                                 <summary>Developer Notes</summary>
                                 Your adapter needs to construct <code>StreamEvent</code> objects with the correct <code>type</code>, <code>data</code>, <code>tokenType</code>, and IDs. For <code>METADATA</code> events, the <code>data</code> should conform to the <code>LLMMetadata</code> interface. The adapter must parse the provider's specific stream format (e.g., Server-Sent Events) to generate these standard events.
                             </details>
                         </div>
                     </div>
                 </div>
                 <div id="scenario-3-implementation" class="mb-10">
                    <h3 class="text-xl font-medium mb-4 text-gray-800">5.2. Implementing <code>AnthropicAdapter</code> (with Streaming)</h3>
<pre><code class="language-typescript">// src/adapters/AnthropicAdapter.ts

import {
  ProviderAdapter, ArtStandardPrompt, ArtStandardMessage, CallOptions, MessageRole,
  StreamEvent, LLMMetadata, RuntimeProviderConfig // Import necessary types
} from 'art-framework';

// Example types matching Anthropic Messages API structure
interface AnthropicMessage {
  role: 'user' | 'assistant';
  // Content can be string or complex array for tool use/results
  content: string | Array<{ type: 'text', text: string } | { type: 'tool_use', id: string, name: string, input: any } | { type: 'tool_result', tool_use_id: string, content: string, is_error?: boolean }>;
}
interface AnthropicRequestBody {
  model: string;
  messages: AnthropicMessage[];
  system?: string;
  max_tokens: number;
  temperature?: number;
  stop_sequences?: string[];
  stream?: boolean;
  // Potentially add tool definitions if using Anthropic's native tool support
  // tools?: Array<{ name: string, description: string, input_schema: object }>;
}
// Type for non-streaming or final aggregated response
interface AnthropicResponse {
  content: Array<{ type: 'text', text: string } | { type: 'tool_use', id: string, name: string, input: any }>;
  stop_reason?: string;
  usage?: { input_tokens: number, output_tokens: number };
  // ... other fields
}

// Options expected by the constructor, derived from RuntimeProviderConfig.adapterOptions
interface AnthropicAdapterConstructorOptions {
  apiKey: string;
  model?: string; // Default model if not specified in RuntimeProviderConfig.modelId
  defaultMaxTokens?: number;
  defaultTemperature?: number;
  anthropicVersion?: string;
  // Add other constructor-time options
}

export class AnthropicAdapter implements ProviderAdapter {
  readonly providerName = 'anthropic';
  private options: AnthropicAdapterConstructorOptions; // Store constructor options

  // The constructor receives options when the ProviderManager instantiates the adapter.
  // These options come from RuntimeProviderConfig.adapterOptions.
  constructor(options: AnthropicAdapterConstructorOptions) {
    if (!options.apiKey) {
      throw new Error(`Anthropic adapter requires an apiKey in adapterOptions.`);
    }
    this.options = {
        ...options,
        // Set defaults if not provided in constructor options
        defaultMaxTokens: options.defaultMaxTokens ?? 1024,
        defaultTemperature: options.defaultTemperature ?? 0.7,
        anthropicVersion: options.anthropicVersion ?? '2023-06-01'
    };
  }

  // Helper to format ArtStandardPrompt messages to Anthropic format
  private formatMessages(prompt: ArtStandardPrompt): { messages: AnthropicMessage[], system?: string } {
    let systemPrompt: string | undefined = undefined;
    const anthropicMessages: AnthropicMessage[] = [];

    let lastRole: 'user' | 'assistant' | null = null;
    for (const message of prompt) {
      if (message.role === 'system') {
        systemPrompt = String(message.content); // Use the last system message content
        continue;
      }

      let role: 'user' | 'assistant';
      let content: AnthropicMessage['content'];

      // Map ART roles to Anthropic roles and content structure
      if (message.role === 'user') {
        role = 'user';
        content = String(message.content); // Simple user text
      } else if (message.role === 'assistant') {
        role = 'assistant';
        // Assistant content can be text or tool_request
        if (Array.isArray(message.content) && message.content.every(item => item.type === 'tool_use')) { // Check if it's tool_request like structure
            content = message.content.map((toolCall: any) => ({
                type: 'tool_use',
                id: toolCall.id || `toolu_${Date.now()}`,
                name: toolCall.name,
                input: toolCall.input
            }));
        } else {
            content = String(message.content); // Simple assistant text
        }
      } else if (message.role === 'tool_request' && Array.isArray(message.content)) {
        // ART's tool_request is an array of tool calls from assistant
        role = 'assistant';
        content = message.content.map((toolCall: any) => ({
          type: 'tool_use',
          id: toolCall.id || `toolu_${Date.now()}`, // Ensure an ID exists
          name: toolCall.function.name,
          // Anthropic expects input as object, ART standard stores arguments string or object
          input: typeof toolCall.function.arguments === 'string' ? JSON.parse(toolCall.function.arguments) : toolCall.function.arguments,
        }));
      } else if (message.role === 'tool_result') {
        // User provides tool result
        role = 'user';
        // Tool results need to be wrapped in the specific Anthropic structure
        content = [{
          type: 'tool_result',
          tool_use_id: message.tool_call_id!, // tool_call_id is required
          content: typeof message.content === 'object' ? JSON.stringify(message.content) : String(message.content),
          // is_error: message.metadata?.isError // Assuming metadata might contain error flag
        }];
      } else {
        console.warn(`AnthropicAdapter: Skipping message with unhandled role: ${message.role}`);
        continue; // Skip unhandled roles
      }

      // Merge consecutive messages of the same mapped role if needed by Anthropic API rules
      if (role === lastRole && anthropicMessages.length > 0) {
        console.warn(`AnthropicAdapter: Consecutive ${role} messages detected. Merging content.`);
        const lastMsg = anthropicMessages[anthropicMessages.length -1]!;
        // Simple string concatenation for text, array concat for tool results/requests
        if (typeof lastMsg.content === 'string' && typeof content === 'string') {
          lastMsg.content = `${lastMsg.content}\n${content}`;
        } else if (Array.isArray(lastMsg.content) && Array.isArray(content)) {
          lastMsg.content = [...lastMsg.content, ...content];
        } else if (typeof lastMsg.content === 'string' && Array.isArray(content)) { // prev string, current array
          lastMsg.content = [{type: 'text', text: lastMsg.content}, ...content];
        } else if (Array.isArray(lastMsg.content) && typeof content === 'string') { // prev array, current string
          lastMsg.content = [...lastMsg.content, {type: 'text', text: content}];
        } else {
            // Fallback or error for unhandled merge
            console.error("AnthropicAdapter: Unhandled content merge scenario.", lastMsg.content, content);
            anthropicMessages.push({ role, content }); // Add as new message
            lastRole = role;
        }
      } else {
        anthropicMessages.push({ role, content });
        lastRole = role;
      }
    }

    // Ensure the conversation starts with a user message if possible (Anthropic requirement)
    if (anthropicMessages.length > 0 && anthropicMessages[0].role === 'assistant') {
        console.warn("AnthropicAdapter: Conversation starts with assistant message, prepending empty user message.");
        anthropicMessages.unshift({ role: 'user', content: "(System Note: Conversation context begins)" });
    }
    // Anthropic also requires that the last message isn't an assistant's partial tool_use response
    // This is usually handled by ensuring a user 'tool_result' message follows an assistant 'tool_use'


    return { messages: anthropicMessages, system: systemPrompt };
  }

  // Updated to accept ArtStandardPrompt and return AsyncIterable<StreamEvent>
  async call(prompt: ArtStandardPrompt, options: CallOptions): Promise<AsyncIterable<StreamEvent>> {
    const { threadId, traceId = `anthropic-trace-${Date.now()}`, sessionId, stream, callContext, providerConfig } = options;

    const { messages, system } = this.formatMessages(prompt);
    // Model should primarily come from the RuntimeProviderConfig used for this call
    const modelToUse = providerConfig.modelId || this.options.model || 'claude-3-5-sonnet-20240620'; // Prefer model from runtime config
    // API key comes from constructor options (originally from RuntimeProviderConfig.adapterOptions)
    const apiKey = this.options.apiKey;

    // Other parameters can come from adapter defaults or RuntimeProviderConfig.adapterOptions
    // Note: providerConfig.adapterOptions contains the options passed during ProviderManager.getAdapter
    const adapterOpts = providerConfig.adapterOptions || {};
    const maxTokens = adapterOpts.max_tokens ?? this.options.defaultMaxTokens!;
    const temperature = adapterOpts.temperature ?? this.options.defaultTemperature!;
    const stopSequences = adapterOpts.stop_sequences; // Allow override from runtime config

    const requestBody: AnthropicRequestBody = {
      model: modelToUse,
      messages: messages,
      max_tokens: maxTokens,
      temperature: temperature,
    };

    if (system) {
      requestBody.system = system;
    }
    if (stopSequences) {
      requestBody.stop_sequences = stopSequences;
    }
    if (stream) {
        requestBody.stream = true;
    }
    // Add tool definitions to requestBody.tools if using Anthropic's native tool support
    // based on tools passed in options or derived from prompt context.

    const apiUrl = 'https://api.anthropic.com/v1/messages';

    const generator = async function*(this: AnthropicAdapter): AsyncIterable<StreamEvent> {
        try {
            const response = await fetch(apiUrl, {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                    'x-api-key': apiKey, // Use API key from constructor options
                    'anthropic-version': this.options.anthropicVersion!,
                    // 'anthropic-beta': 'tools-2024-05-16', // Enable if using native tool support with latest models
                },
                body: JSON.stringify(requestBody),
            });

            if (!response.ok) {
                const errorBody = await response.text();
                console.error(`Anthropic API Error (${response.status}): ${errorBody}`);
                yield { type: 'ERROR', data: new Error(`Anthropic API Error (${response.status}): ${errorBody}`), threadId, traceId, sessionId };
                yield { type: 'END', data: null, threadId, traceId, sessionId }; // Ensure END is yielded
                return;
            }

            // --- Handle Streaming Response ---
            if (stream && response.body) {
                const reader = response.body.pipeThrough(new TextDecoderStream()).getReader();
                let buffer = '';
                let messageStopReason: string | null = null;
                let finalUsageData: any = null;
                let thinkingTokens = 0; // Track thinking tokens if possible/needed

                while (true) {
                    const { value, done } = await reader.read();
                    if (done) break;

                    buffer += value;
                    const lines = buffer.split('\n');
                    buffer = lines.pop() || ''; // Keep incomplete line

                    for (const line of lines) {
                        if (line.startsWith('data: ')) {
                            const dataContent = line.substring(6).trim();
                            try {
                                const jsonData = JSON.parse(dataContent);
                                const type = jsonData.type;

                                // Determine tokenType based on callContext
                                const tokenTypeBase = callContext === 'AGENT_THOUGHT' ? 'AGENT_THOUGHT' : 'FINAL_SYNTHESIS';

                                if (type === 'content_block_delta' && jsonData.delta?.type === 'text_delta') {
                                    const textDelta = jsonData.delta.text;
                                    // Anthropic stream doesn't easily distinguish thinking vs response within a call
                                    // Rely solely on callContext for now.
                                    const tokenType = `${tokenTypeBase}_LLM_RESPONSE`;
                                    yield { type: 'TOKEN', data: textDelta, threadId, traceId, sessionId, tokenType: tokenType as StreamEvent['tokenType'] };
                                } else if (jsonData.type === 'content_block_start' && jsonData.content_block?.type === 'tool_use') {
                                    // Start of a tool_use block. Agent might want this as structured data.
                                    // For token streaming, this might be a signal.
                                    // Let's yield a placeholder token or specific event if UI needs it.
                                    // Or, the agent core would aggregate these.
                                } else if (jsonData.type === 'content_block_delta' && jsonData.delta?.type === 'input_json_delta') {
                                    // Streaming tool input.
                                    // Could yield as special TOKEN or accumulate.
                                } else if (jsonData.type === 'message_start') {
                                    finalUsageData = jsonData.message?.usage ?? finalUsageData;
                                } else if (jsonData.type === 'message_delta') {
                                    finalUsageData = { ...(finalUsageData ?? {}), ...jsonData.usage };
                                    messageStopReason = jsonData.delta?.stop_reason ?? messageStopReason;
                                } else if (jsonData.type === 'message_stop') {
                                    // Stream finished signal from Anthropic
                                    if (finalUsageData || messageStopReason) {
                                        const metadata: LLMMetadata = {
                                            inputTokens: finalUsageData?.input_tokens,
                                            outputTokens: finalUsageData?.output_tokens,
                                            thinkingTokens: thinkingTokens > 0 ? thinkingTokens : undefined,
                                            stopReason: messageStopReason,
                                            providerRawUsage: finalUsageData,
                                            traceId: traceId,
                                        };
                                        yield { type: 'METADATA', data: metadata, threadId, traceId, sessionId };
                                    }
                                    yield { type: 'END', data: null, threadId, traceId, sessionId };
                                    return; // Exit generator
                                }

                            } catch (parseError: any) {
                                console.warn(`Failed to parse Anthropic stream chunk: ${dataContent}`, parseError);
                                yield { type: 'ERROR', data: new Error(`Stream parse error: ${parseError.message}`), threadId, traceId, sessionId };
                            }
                        }
                    }
                }
                // If loop finishes without message_stop (unlikely but possible), yield END
                yield { type: 'END', data: null, threadId, traceId, sessionId };

            // --- Handle Non-Streaming Response ---
            } else {
                const responseData: AnthropicResponse = await response.json();
                let responseContentText = '';
                let toolUseCalls: any[] = []; // Collect tool calls if any

                 if (responseData.content) {
                     for (const block of responseData.content) {
                         if (block.type === 'text') {
                             responseContentText += block.text;
                         } else if (block.type === 'tool_use') {
                             toolUseCalls.push({
                                type: 'tool_use', // Keep it structured
                                id: block.id,
                                name: block.name,
                                input: block.input
                             });
                         }
                     }
                 }

                 const tokenTypeBase = callContext === 'AGENT_THOUGHT' ? 'AGENT_THOUGHT' : 'FINAL_SYNTHESIS';
                 const tokenType = `${tokenTypeBase}_LLM_RESPONSE`;

                 // If there are tool calls, yield them as structured data. Otherwise, yield text.
                 // The Agent Core will need to decide how to buffer/parse this.
                 // For simplicity, if tool calls exist, we yield the raw content blocks.
                 // If only text, we yield the string.
                 const dataToYield = toolUseCalls.length > 0 ? responseData.content : responseContentText;
                 yield { type: 'TOKEN', data: dataToYield, threadId, traceId, sessionId, tokenType: tokenType as StreamEvent['tokenType'] };

                const usage = responseData.usage;
                if (usage || responseData.stop_reason) {
                    const metadata: LLMMetadata = {
                        inputTokens: usage?.input_tokens,
                        outputTokens: usage?.output_tokens,
                        stopReason: responseData.stop_reason,
                        providerRawUsage: usage,
                        traceId: traceId,
                    };
                    yield { type: 'METADATA', data: metadata, threadId, traceId, sessionId };
                }
                yield { type: 'END', data: null, threadId, traceId, sessionId };
            }

        } catch (error: any) {
            console.error(`${this.providerName} adapter error in generator:`, error);
            yield { type: 'ERROR', data: error instanceof Error ? error : new Error(String(error)), threadId, traceId, sessionId };
            yield { type: 'END', data: null, threadId, traceId, sessionId }; // Ensure END is yielded even after error
        }
    }.bind(this); // Bind the generator function to the class instance

    return generator(); // Return the async generator
  }
}
</code></pre>
                    <h4 class="text-lg font-medium mt-6 mb-4 text-gray-800">Explanation:</h4>
                    <ol class="list-decimal list-inside space-y-3 text-gray-700">
                        <li><strong>Implements `ProviderAdapter`:</strong> Adheres to the updated contract.</li>
                        <li><strong>Constructor:</strong> Takes `AnthropicAdapterConstructorOptions` (like API key). These options are provided by the `ProviderManager` when it instantiates the adapter based on `RuntimeProviderConfig.adapterOptions`.</li>
                        <li><strong>`formatMessages` Helper (Updated):</strong>
                            <ul class="list-disc list-inside ml-6 space-y-2 mt-2">
                                <li>Accepts `ArtStandardPrompt` as input.</li>
                                <li>Maps ART standard roles (`system`, `user`, `assistant`, `tool_request`, `tool_result`) to Anthropic roles (`user`, `assistant`) and content structures (including `tool_use` and `tool_result` block types).</li>
                                <li>Handles merging consecutive messages of the same mapped role.</li>
                                <li>Ensures conversation starts with a `user` message.</li>
                            </ul>
                        </li>
                        <li><strong>`call` Method (Updated):</strong>
                            <ul class="list-disc list-inside ml-6 space-y-2 mt-2">
                                <li>Accepts `ArtStandardPrompt` and `CallOptions` (which includes `providerConfig`).</li>
                                <li>Returns `Promise<AsyncIterable<StreamEvent>>`.</li>
                                <li>Uses an `async function*` generator to yield `StreamEvent`s.</li>
                                <li>Calls `formatMessages` to translate the prompt.</li>
                                <li>Determines model, API key, and other parameters primarily from the options set in the constructor (`this.options`, derived from `RuntimeProviderConfig.adapterOptions`) and the `providerConfig` passed in `CallOptions`.</li>
                                <li>Constructs the Anthropic API request body, including `stream: true` if requested in `CallOptions`.</li>
                                <li>Makes the `fetch` request.</li>
                                <li><strong>Streaming Logic:</strong> If `stream` is true and `response.body` exists:
                                    <ul>
                                        <li>Uses `TextDecoderStream` to read the response body.</li>
                                        <li>Parses Server-Sent Events (SSE) format typical of Anthropic streams.</li>
                                        <li>Yields `TOKEN` events for `content_block_delta` -> `text_delta`, determining `tokenType` based on `callContext`.</li>
                                        <li>Handles `tool_use` related stream events.</li>
                                        <li>Yields `METADATA` events based on `message_delta` or `message_stop`.</li>
                                        <li>Handles other stream event types (`message_start`, `message_stop`).</li>
                                        <li>Yields `ERROR` events on API or parsing errors.</li>
                                        <li>Yields `END` event when the stream completes or stops.</li>
                                    </ul>
                                </li>
                                 <li><strong>Non-Streaming Logic:</strong> If `stream` is false:
                                    <ul>
                                        <li>Reads the full JSON response.</li>
                                        <li>Extracts text content and any `tool_use` blocks. If `tool_use` blocks exist, yields the raw content array for the Agent Core to parse; otherwise, yields the aggregated text.</li>
                                        <li>Yields a `METADATA` event with usage info.</li>
                                        <li>Yields the `END` event.</li>
                                    </ul>
                                </li>
                                <li>Includes robust error handling within the generator.</li>
                            </ul>
                        </li>
                    </ol>
                 </div>
                 <div id="scenario-3-integration" class="mb-8">
                    <h3 class="text-xl font-medium mb-4 text-gray-800">5.3. Integrating the <code>AnthropicAdapter</code></h3>
                     <p class="mb-4 text-gray-700">Integrating a custom adapter is straightforward with the `ProviderManager`. You simply register the adapter *class* in the `ProviderManagerConfig` when calling `createArtInstance`. ART handles the instantiation when needed.</p>
<pre><code class="language-typescript">// --- Integration Example ---
import { AnthropicAdapter } from './adapters/AnthropicAdapter';
// Import other necessary components (Storage, Agent Core, Tools)
import { createArtInstance, ProviderManagerConfig, AgentFactoryConfig, IndexedDBStorageAdapter, PESAgent, CalculatorTool, ArtInstance, RuntimeProviderConfig } from 'art-framework';

async function setupWithCustomAdapter(): Promise&lt;ArtInstance&gt; {

    // Define the ProviderManagerConfig, including the custom adapter
    const providerConfig: ProviderManagerConfig = {
      availableProviders: [
        {
          name: 'anthropic', // Unique identifier for this provider configuration
          adapter: AnthropicAdapter, // Pass the adapter CLASS
        },
        // You could also register built-in adapters here if needed
        // { name: 'openai', adapter: OpenAIAdapter },
      ],
      // Optional global limits
      // maxParallelApiInstancesPerProvider: 2,
    };

    // Define the overall ART configuration
    const config: AgentFactoryConfig = {
      storage: new IndexedDBStorageAdapter({ dbName: 'artWithAnthropic' }), // Pass storage adapter instance
      providers: providerConfig, // Pass the provider config
      agentCore: PESAgent, // Use the default agent core
      tools: [new CalculatorTool()], // Include any tools
    };

    // Create the ART instance
    const art = await createArtInstance(config);
    console.log("ART instance created with Anthropic adapter registered.");
    return art;
}

// --- Runtime Usage (Conceptual) ---

async function useAnthropic(art: ArtInstance, threadId: string, query: string) {
    // Define the runtime configuration for this specific call
    const runtimeConfig: RuntimeProviderConfig = {
        providerName: 'anthropic', // Must match the name registered in ProviderManagerConfig
        modelId: 'claude-3-opus-20240229', // Specify the desired model
        adapterOptions: {
            apiKey: 'YOUR_ANTHROPIC_API_KEY', // Provide necessary options for the adapter constructor
            // Add other Anthropic-specific options if needed
            // defaultTemperature: 0.5
        }
    };

    // Persist this choice for the thread (optional but common)
    await art.stateManager.setThreadConfigValue(threadId, 'runtimeProviderConfig', runtimeConfig);

    // Make the call - the agent core will load the runtimeProviderConfig from state
    // Alternatively, pass it directly via configOverrides:
    // const response = await art.process({ query, threadId, configOverrides: { runtimeProviderConfig: runtimeConfig } });
    const response = await art.process({ query, threadId });
    // Access the final response message content
    console.log("Response from Anthropic:", response.response.content);
}

</code></pre>
                    <h4 class="text-lg font-medium mt-6 mb-4 text-gray-800">How it Works Now:</h4>
                    <ol class="list-decimal list-inside space-y-3 text-gray-700">
                        <li><strong>Registration:</strong> `createArtInstance` receives the `ProviderManagerConfig` which includes the `AnthropicAdapter` class associated with the name 'anthropic'. The `ProviderManager` is initialized with this mapping.</li>
                        <li><strong>Runtime Selection:</strong> When `art.process` is called, the `PESAgent` (or whichever `IAgentCore` is used) loads the `runtimeProviderConfig` from the `ThreadConfig` (which was set by the application, e.g., in `useAnthropic`).</li>
                        <li><strong>Instantiation:</strong> The `PESAgent` passes this `runtimeProviderConfig` within `CallOptions` to the `ReasoningEngine`. The `ReasoningEngine` calls `providerManager.getAdapter(runtimeConfig)`.</li>
                        <li><strong>ProviderManager Logic:</strong> The `ProviderManager` finds the registered class (`AnthropicAdapter`) for the name 'anthropic'. It creates a *new instance* of `AnthropicAdapter`, passing `runtimeConfig.adapterOptions` (containing the API key, etc.) to its constructor. It manages the lifecycle of this instance (pooling, caching based on config).</li>
                        <li><strong>Execution:</strong> The `ReasoningEngine` receives the managed adapter instance and calls its `call` method with the prompt and options. The `AnthropicAdapter` instance uses its configured options (API key) to communicate with the Anthropic API.</li>
                        <li><strong>Release:</strong> After the `call` completes (or the stream ends), the `ReasoningEngine` releases the adapter instance back to the `ProviderManager`.</li>
                    </ol>
                    <p class="mt-4 text-gray-700">This approach cleanly separates adapter implementation from instantiation and configuration, allowing flexible runtime selection and management of multiple providers.</p>
                 </div>
            </section>

             <section id="scenario-4" class="mb-16 fade-in-section">
                <h2 class="text-2xl md:text-3xl font-semibold mb-6 border-b pb-3 text-sky-700">6. Scenario 4: Adding a Custom Storage Adapter (DuckDB WASM Example)</h2>
                 <p class="mb-5 text-gray-700">
                    Let's explore using DuckDB WASM as a storage backend. DuckDB is an in-process analytical data management system, and its WASM version allows running it directly in the browser. This could enable more powerful local data storage and querying, including potential vector similarity search for RAG-like capabilities, compared to basic <code>localStorage</code> or <code>IndexedDB</code>.
                </p>
                 <p class="mb-5 text-gray-700"><strong>Goal:</strong> Implement a skeleton <code>DuckDBWasmAdapter</code> demonstrating basic CRUD and conceptual vector storage/search.</p>

                  <div id="scenario-4-simple-explanation" class="mb-10 p-4 bg-blue-50 border border-blue-200 text-blue-800 rounded-md text-sm">
                     <h3 class="text-lg font-medium mb-2 text-blue-900">6.0 Simplified Explanation for Developers</h3>
                     <p class="mb-2">Imagine ART, our smart assistant, needs a place to keep its notes and memories (like conversation history or agent state). By default, it might use a simple notebook (IndexedDB) or just remember things short-term (in-memory). But you want it to use a more robust, cloud-based filing cabinet (like Supabase) for long-term storage, while still using a quick notepad (in-memory) for temporary notes to speed things up.</p>
                     <ol class="list-decimal list-inside mt-3 space-y-1">
                         <li><strong>Creating Your Filing Cabinet Connector (Your Custom Supabase Adapter):</strong> You need to build a special connector that knows how to talk to your Supabase filing cabinet. This connector is your custom <strong>Storage Adapter</strong>. You'll write code that implements ART's standard `StorageAdapter` blueprint. This blueprint requires your connector to have methods for basic filing operations:
                             <ul class="list-disc list-inside ml-6 text-xs space-y-1">
                                 <li>`get`: How to find a specific note in the filing cabinet.</li>
                                 <li>`set`: How to save or update a note in the filing cabinet.</li>
                                 <li>`delete`: How to throw away a note.</li>
                                 <li>`query`: How to find multiple notes based on certain criteria.</li>
                                 <li>`init` (optional): How to set up the connection to the filing cabinet when ART starts.</li>
                                 <li>`clearCollection` / `clearAll` (optional): How to empty specific drawers or the whole cabinet.</li>
                             </ul>
                             Your code for the Supabase adapter will use the Supabase client library to perform these operations against your Supabase database.
                         </li>
                         <li><strong>Setting Up the Quick Notepad (Using InMemoryStorageAdapter):</strong> ART already comes with a simple in-memory notepad (`InMemoryStorageAdapter`). This adapter is very fast because it just keeps notes in the assistant's short-term memory, but the notes are lost when the assistant is turned off (the browser tab is closed).</li>
                         <li><strong>Connecting the Notepad and the Filing Cabinet (Creating a Caching Adapter):</strong> This is where you get clever. You can create *another* custom adapter, let's call it `CachingStorageAdapter`. This adapter won't talk directly to a storage system itself. Instead, it will *use* both the `InMemoryStorageAdapter` (the notepad) and your `SupabaseAdapter` (the filing cabinet connector).
                              <ul class="list-disc list-inside ml-6 text-xs space-y-1">
                                 <li>When ART asks the `CachingStorageAdapter` to `get` a note, it first checks the notepad (`InMemoryStorageAdapter`). If the note is there, it returns it quickly.</li>
                                 <li>If the note is *not* in the notepad, the `CachingStorageAdapter` then asks the filing cabinet connector (`SupabaseAdapter`) to `get` it from Supabase. If found, it returns the note *and* saves a copy in the notepad for next time.</li>
                                 <li>When ART asks the `CachingStorageAdapter` to `set` or `delete` a note, it performs the operation on *both* the notepad (`InMemoryStorageAdapter`) and the filing cabinet connector (`SupabaseAdapter`) to keep them in sync.</li>
                                 <li>The `query` method might be more complex, potentially querying Supabase and then populating the cache.</li>
                             </ul>
                         </li>
                          <li><strong>Giving the Combined Setup to the Assistant:</strong> When you set up the ART assistant using `createArtInstance`, you provide your `CachingStorageAdapter` as the main `storage` component in the configuration. Your `CachingStorageAdapter` instance will be created with instances of the `InMemoryStorageAdapter` and your `SupabaseAdapter` inside it.</li>
                     </ol>
                      <p class="mt-3 mb-2">So, to use Supabase with in-memory caching:</p>
                      <ul class="list-disc list-inside mt-2 space-y-1">
                         <li>You create a custom `SupabaseAdapter` class that implements ART's `StorageAdapter` interface and talks to your Supabase database.</li>
                         <li>You create a `CachingStorageAdapter` class that also implements `StorageAdapter`. Its constructor takes instances of a primary storage adapter (your `SupabaseAdapter`) and a cache adapter (`InMemoryStorageAdapter`). Its methods implement the caching logic (read from cache, fallback to primary, write to both).</li>
                         <li>In your application's setup code, you create instances: `const supabaseAdapter = new SupabaseAdapter(...)`, `const inMemoryAdapter = new InMemoryStorageAdapter()`, `const cachingAdapter = new CachingStorageAdapter(supabaseAdapter, inMemoryAdapter)`.</li>
                         <li>You pass the `cachingAdapter` instance in the `storage` part of the configuration when calling `createArtInstance`.</li>
                     </ul>
                     <p class="mt-3 mb-2">You don't need to change any of ART's core files. You build custom components that adhere to ART's standard interfaces and wire them together during your application's initialization.</p>
                     <h4 class="text-lg font-medium mt-4 mb-2 text-blue-900">How to Create and Use Your Custom Storage Adapter:</h4>
                      <ol class="list-decimal list-inside mt-2 space-y-1">
                         <li><strong>Create Your Adapter File(s):</strong> Create new file(s) in your application's project, perhaps in a folder like `storage-adapters` or `data`. For example, `supabase-adapter.ts` and `caching-adapter.ts`.</li>
                         <li><strong>Import Necessary ART Components:</strong> Inside your adapter files, import the required types and interfaces from `art-framework`. Key imports for a storage adapter include:
                             <ul class="list-disc list-inside ml-6 text-xs space-y-1">
                                 <li>`StorageAdapter`: The interface your adapter class must implement.</li>
                                 <li>`FilterOptions`: The type for query options.</li>
                                 <li>You might also need types for the data you are storing (e.g., `ConversationMessage`, `AgentState`, `Observation`) if you want type safety within your adapter, although the `StorageAdapter` interface uses generic types (`&lt;T&gt;`).</li>
                             </ul>
                         </li>
                         <li><strong>Implement Your Adapter Class(es):</strong> Create the class(es) that implement the `StorageAdapter` interface.
                             <ul class="list-disc list-inside ml-6 text-xs space-y-1">
                                 <li>For the `SupabaseAdapter`, implement the `get`, `set`, `delete`, and `query` methods using the Supabase client library to interact with your database. Implement `init` if you need to establish the Supabase connection asynchronously.</li>
                                 <li>For the `CachingStorageAdapter`, implement the `get`, `set`, `delete`, and `query` methods by coordinating calls to the injected primary and cache adapters (e.g., check cache on `get`, write to both on `set`).</li>
                             </ul>
                         </li>
                         <li><strong>Import and Pass to `createArtInstance`:</strong> In the file where you initialize ART, import your custom adapter class(es). In the configuration object passed to `createArtInstance`, create instances of your custom adapters and pass the top-level adapter (e.g., your `CachingStorageAdapter`) in the `storage` part:
<pre><code class="language-typescript">import { createArtInstance, InMemoryStorageAdapter, OpenAIAdapter, ProviderManagerConfig } from 'art-framework';
import { SupabaseAdapter } from './storage-adapters/supabase-adapter'; // Import your Supabase adapter
import { CachingStorageAdapter } from './storage-adapters/caching-adapter'; // Import your Caching adapter

// Assuming SupabaseAdapter constructor takes options like URL and Key
const supabaseAdapter = new SupabaseAdapter({ url: 'YOUR_SUPABASE_URL', apiKey: 'YOUR_SUPABASE_API_KEY' });
const inMemoryAdapter = new InMemoryStorageAdapter(); // Use the built-in in-memory adapter

// Instantiate your caching adapter with the primary and cache adapters
const cachingAdapter = new CachingStorageAdapter(supabaseAdapter, inMemoryAdapter);

const providerConfig: ProviderManagerConfig = { // Define provider config
  availableProviders: [
    {
      name: 'openai', // Or your chosen provider name
      adapter: OpenAIAdapter, // Or your chosen adapter CLASS
      // adapterOptions: { /* Default options if any */ }
    }
  ]
};

const config = {
  storage: cachingAdapter, // Pass the caching adapter instance
  providers: providerConfig, // Pass the provider configuration
  // ... other config (agentCore, tools)
};

const art = await createArtInstance(config);
</code></pre>
                        </li>
                    </ol>
                      <p class="mt-3">By following these steps, you can seamlessly integrate your custom storage solution(s) with ART without modifying the framework's core code.</p>

                 </div>

                 <p class="mb-8 p-4 bg-yellow-50 border border-yellow-200 text-yellow-800 rounded-md text-sm">
                    <strong>Disclaimer:</strong> Integrating DuckDB WASM is significantly more complex than `localStorage` or `IndexedDB`. It involves asynchronous initialization, managing WASM bundles, understanding SQL, and potentially handling vector embeddings and similarity calculations. This example provides a conceptual structure.
                </p>

                 <div id="scenario-4-imports" class="mb-10">
                    <h3 class="text-xl font-medium mb-4 text-gray-800">6.1. Necessary Imports & Explanations</h3>
<pre><code class="language-typescript">// --- ART Storage Adapter Creation Imports ---
import {
  // The interface that a custom storage adapter must implement
  StorageAdapter,
  // Type defining options for querying data (filtering, sorting, limits)
  FilterOptions
} from 'art-framework';

// --- DuckDB WASM Imports ---
// You would typically install @duckdb/duckdb-wasm
import * as duckdb from '@duckdb/duckdb-wasm';
// Import specific types if needed, e.g., from duckdb-wasm
// import { AsyncDuckDB, AsyncDuckDBConnection } from '@duckdb/duckdb-wasm';

// --- Vector Embedding Imports (Conceptual) ---
// You would need a library or function to generate embeddings
// e.g., using Transformers.js or calling an embedding API
// import { pipeline } from '@xenova/transformers'; // Example
// const extractor = await pipeline('feature-extraction', 'Xenova/all-MiniLM-L6-v2');
// async function getEmbedding(text: string): Promise<number[]> {
//   const output = await extractor(text, { pooling: 'mean', normalize: true });
//   return Array.from(output.data as Float32Array);
// }
</code></pre>
                    <h4 class="text-lg font-medium mt-6 mb-4 text-gray-800">Explanation of Storage Adapter Imports:</h4>
                    <div class="space-y-4 text-gray-700">
                        <div>
                            <p><strong><code>StorageAdapter</code></strong></p>
                            <p class="mb-2">The blueprint for creating a custom way for ART to save and load its data (like chat history or agent memory) using DuckDB WASM.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                The interface your custom storage class must implement. Requires implementing methods for basic CRUD operations:
                                <ul class="list-disc list-inside ml-4">
                                    <li><code>async get&lt;T&gt;(collection: string, id: string): Promise&lt;T | null&gt;</code>: Retrieve a single item by ID.</li>
                                    <li><code>async set&lt;T&gt;(collection: string, id: string, data: T): Promise&lt;void&gt;</code>: Save (create or update) an item.</li>
                                    <li><code>async delete(collection: string, id: string): Promise&lt;void&gt;</code>: Delete an item by ID.</li>
                                    <li><code>async query&lt;T&gt;(collection: string, filterOptions: FilterOptions): Promise&lt;T[]&gt;</code>: Retrieve multiple items based on filter criteria.</li>
                                    <li>Optional: <code>async init?(config?: any): Promise&lt;void&gt;</code> (for async setup), <code>async clearCollection?(collection: string): Promise&lt;void&gt;</code>, <code>async clearAll?(): Promise&lt;void&gt;</code>. The implementation will translate these generic operations into DuckDB SQL commands executed via the WASM instance.</li>
                                </ul>
                            </details>
                        </div>
                         <div>
                            <p><strong><code>FilterOptions</code></strong></p>
                            <p class="mb-2">Describes how to filter, sort, or limit the data when asking the storage adapter for multiple items. Mapping this to SQL, especially for vector similarity, is the main challenge.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                Interface used as input for the <code>StorageAdapter.query</code> method. May include properties like:
                                <ul class="list-disc list-inside ml-4">
                                    <li><code>filters?: Array&lt;{ field: string; operator: string; value: any }&gt;</code>: Criteria to match items (e.g., <code>{ field: 'role', operator: '==', value: 'USER' }</code>).</li>
                                    <li><code>sortBy?: string</code>: Field name to sort by.</li>
                                    <li><code>sortDirection?: 'asc' | 'desc'</code>: Sorting order.</li>
                                    <li><code>limit?: number</code>: Maximum number of items to return.</li>
                                    <li><code>offset?: number</code>: Number of items to skip (for pagination).</li>
                                    <li>The <code>query</code> implementation in the DuckDB adapter will need to parse these options and construct appropriate <code>WHERE</code>, <code>ORDER BY</code>, and <code>LIMIT</code>/<code>OFFSET</code> clauses in SQL. Supporting complex filters or vector similarity searches (e.g., using a custom operator like <code>&lt;=&gt;</code> if using an extension, or calculating distance manually) requires specific logic.</li>
                                </ul>
                            </details>
                        </div>
                        <div>
                            <p><strong><code>@duckdb/duckdb-wasm</code></strong></p>
                            <p class="mb-2">The library providing the DuckDB WASM engine and browser integration.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                Used for initializing the database (<code>duckdb.selectBundle</code>, <code>db.instantiate</code>, <code>db.open</code>), establishing connections (<code>db.connect()</code>), and executing SQL queries (<code>connection.query()</code>, <code>connection.send()</code>, <code>connection.prepare()</code>, etc.). Requires careful handling of asynchronous initialization and potentially large WASM bundles. Consider using specific backends like OPFS (<code>db.registerFileURL</code>) for better persistence.
                            </details>
                        </div>
                        <div>
                            <p><strong>Vector Embedding Library (Conceptual)</strong></p>
                            <p class="mb-2">A library or function to convert text data (like conversation messages or state content) into numerical vectors (embeddings) for similarity search.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                Needed if implementing vector search capabilities. This is separate from DuckDB itself but crucial for the RAG use case. Embeddings would be generated before <code>set</code>ting data and used during <code>query</code> for similarity calculations. Libraries like <code>Transformers.js</code> can run embedding models client-side.
                            </details>
                        </div>
                    </div>
                 </div>

                 <div id="scenario-4-implementation" class="mb-10">
                    <h3 class="text-xl font-medium mb-4 text-gray-800">6.2. Implementing <code>DuckDBWasmAdapter</code> (Skeleton)</h3>
<pre><code class="language-typescript">// src/adapters/DuckDBWasmAdapter.ts
import { StorageAdapter, FilterOptions } from 'art-framework';
import * as duckdb from '@duckdb/duckdb-wasm';

// --- Vector Embedding Placeholder ---
async function getEmbedding(text: string): Promise&lt;number[]&gt; {
  // Placeholder: Replace with actual embedding generation
  console.warn("Using placeholder embedding function!");
  // Simple hash-based placeholder vector (NOT suitable for real use)
  let hash = 0;
  for (let i = 0; i &lt; text.length; i++) {
    hash = (hash &lt;&lt; 5) - hash + text.charCodeAt(i);
    hash |= 0; // Convert to 32bit integer
  }
  // Create a dummy vector based on the hash
  const vec = Array(16).fill(0); // Small dimension for example
  for(let i=0; i&lt;vec.length; i++) {
      vec[i] = (hash >> (i*2)) & 3; // Simple mapping
  }
  return vec;
}
// --- End Placeholder ---


// Define table schemas conceptually
const TABLE_SCHEMAS: Record&lt;string, string&gt; = {
  conversations: `(id VARCHAR PRIMARY KEY, threadId VARCHAR, role VARCHAR, content TEXT, timestamp BIGINT, embedding FLOAT[16])`, // Added embedding
  state: `(id VARCHAR PRIMARY KEY, threadId VARCHAR, config JSON, agentState JSON)`, // Store JSON directly
  observations: `(id VARCHAR PRIMARY KEY, threadId VARCHAR, traceId VARCHAR, type VARCHAR, timestamp BIGINT, content JSON, metadata JSON)`,
  // Add other collections as needed
};
const EMBEDDING_DIMENSION = 16; // Match the schema

export class DuckDBWasmAdapter implements StorageAdapter {
  private db: duckdb.AsyncDuckDB | null = null;
  private connection: duckdb.AsyncDuckDBConnection | null = null;
  private dbPath: string; // Path for storing DB file if using specific backend
  private initPromise: Promise&lt;void&gt; | null = null; // Prevent race conditions

  constructor(options: { dbPath?: string } = {}) {
      // dbPath might be used with specific backends like OPFS
      this.dbPath = options.dbPath || 'art_duckdb.db';
  }

  // Modified init to handle concurrent calls
  async init(): Promise&lt;void&gt; {
    if (!this.initPromise) {
        this.initPromise = this._initialize();
    }
    return this.initPromise;
  }

  private async _initialize(): Promise&lt;void&gt; {
    if (this.db) return; // Already initialized

    console.log("Initializing DuckDB WASM...");
    try {
      const JSDELIVR_BUNDLES = duckdb.getJsDelivrBundles();
      const bundle = await duckdb.selectBundle(JSDELIVR_BUNDLES);
      const worker_url = URL.createObjectURL(
        new Blob([`importScripts("${bundle.mainWorker!}");`], { type: 'text/javascript' })
      );
      const worker = new Worker(worker_url);
      const logger = new duckdb.ConsoleLogger(); // Or implement custom logger
      this.db = new duckdb.AsyncDuckDB(logger, worker);
      await this.db.instantiate(bundle.mainModule, bundle.pthreadWorker);
      URL.revokeObjectURL(worker_url);

      // Optional: Register a specific file persistence backend if needed (e.g., OPFS)
      // Requires specific browser support and setup
      // await this.db.registerFileURL(this.dbPath, `/${this.dbPath}`, duckdb.DuckDBDataProtocol.BROWSER_FSACCESS, false);

      await this.db.open({
          // path: this.dbPath, // Use if registered above
          query: {
              // Configure WASM specifics if needed, e.g., memory limits
              // initialMemory: '...',
          }
      });
      this.connection = await this.db.connect();
      console.log("DuckDB WASM Initialized and Connected.");

      // Ensure tables exist
      await this.ensureTables();

    } catch (error) {
      console.error("DuckDB WASM Initialization failed:", error);
      this.initPromise = null; // Reset promise on failure
      throw error;
    }
  }

  private async ensureTables(): Promise&lt;void&gt; {
      if (!this.connection) throw new Error("DuckDB connection not available.");
      console.log("Ensuring tables exist...");
      // Consider installing extensions like 'json' if not bundled
      // await this.connection.query(`INSTALL json; LOAD json;`);
      for (const [tableName, schema] of Object.entries(TABLE_SCHEMAS)) {
          try {
              await this.connection.query(`CREATE TABLE IF NOT EXISTS ${tableName} ${schema};`);
              console.log(`Table ${tableName} ensured.`);
          } catch(e) {
              console.error(`Failed to ensure table ${tableName}:`, e);
              throw e;
          }
      }
  }

  private async ensureConnection(): Promise&lt;duckdb.AsyncDuckDBConnection&gt; {
      await this.init(); // Ensure initialization is complete
      if (!this.connection) {
          throw new Error("Failed to establish DuckDB connection after init.");
      }
      return this.connection;
  }

  async get&lt;T&gt;(collection: string, id: string): Promise&lt;T | null&gt; {
    const conn = await this.ensureConnection();
    try {
      // Use prepared statements for safety
      const stmt = await conn.prepare(`SELECT * FROM ${collection} WHERE id = $1`);
      // Use arrow format for potentially better type handling with JSON
      const results = await stmt.query(id);
      await stmt.close(); // Close statement
      if (results.numRows > 0) {
        const row = results.get(0)?.toJSON();
        // DuckDB might return JSON columns as strings, parse them
        return this.parseJsonColumns(collection, row) as T;
      }
      return null;
    } catch (error) {
      console.error(`DuckDB get error in ${collection}:`, error);
      return null; // Or throw? Depends on desired error handling
    }
  }

  async set&lt;T extends { id: string, content?: string }&gt;(collection: string, id: string, data: T): Promise&lt;void&gt; {
    const conn = await this.ensureConnection();
    const schema = TABLE_SCHEMAS[collection];
    if (!schema) throw new Error(`Unknown collection: ${collection}`);

    // Prepare data for insertion (handle JSON, generate embedding)
    const values: any[] = [];
    const placeholders: string[] = [];
    const columns: string[] = [];

    let embedding: number[] | null = null;
    if (collection === 'conversations' && data.content && schema.includes('embedding')) {
        embedding = await getEmbedding(data.content); // Generate embedding
    }

    // Dynamically build based on schema and data properties
    // This is simplified; a real implementation needs robust mapping & type handling
    const columnDefs = schema.substring(1, schema.length - 1).split(',').map(s => s.trim().split(' ')[0]);

    for (const col of columnDefs) {
        if (col === 'embedding') {
            if (embedding) {
                columns.push(col);
                values.push(embedding); // DuckDB WASM might handle array types directly or need list_value syntax
                placeholders.push(`$${values.length}`);
            }
        } else if (col in data) {
            columns.push(col);
            let value = (data as any)[col];
            // Stringify JSON fields
            if (schema.includes(`${col} JSON`) && typeof value === 'object') {
                value = JSON.stringify(value);
            }
            values.push(value);
            placeholders.push(`$${values.length}`);
        } else if (col === 'id') { // Ensure ID is always included if not in data explicitly
             columns.push('id');
             values.push(id);
             placeholders.push(`$${values.length}`);
        }
    }


    const sql = `INSERT OR REPLACE INTO ${collection} (${columns.join(', ')}) VALUES (${placeholders.join(', ')})`;

    try {
      // Use prepared statements for insertion/replacement
      const stmt = await conn.prepare(sql);
      await stmt.send(...values); // Use send for operations not returning rows
      await stmt.close();
    } catch (error) {
      console.error(`DuckDB set error in ${collection}:`, error);
      throw error; // Re-throw to signal failure
    }
  }

  async delete(collection: string, id: string): Promise&lt;void&gt; {
    const conn = await this.ensureConnection();
    try {
      const stmt = await conn.prepare(`DELETE FROM ${collection} WHERE id = $1`);
      await stmt.send(id);
      await stmt.close();
    } catch (error) {
      console.error(`DuckDB delete error in ${collection}:`, error);
      throw error;
    }
  }

  async query&lt;T&gt;(collection: string, filterOptions: FilterOptions): Promise&lt;T[]&gt; {
    const conn = await this.ensureConnection();
    let sql = `SELECT * FROM ${collection}`;
    const params: any[] = [];
    let paramIndex = 1;

    // --- Basic Filtering ---
    if (filterOptions.filters && filterOptions.filters.length > 0) {
      const whereClauses = filterOptions.filters
        .map((filter) => {
            // Basic equality check - needs expansion for other operators
            if (filter.operator === '==') {
                params.push(filter.value);
                return `${filter.field} = $${paramIndex++}`;
            }
            // TODO: Add support for other operators like '!=', '>', '&lt;', 'in', etc.
            console.warn(`Unsupported filter operator: ${filter.operator}`);
            return null; // Ignore unsupported filters
        })
        .filter(clause => clause !== null); // Remove nulls from ignored filters

      if (whereClauses.length > 0) {
          sql += ` WHERE ${whereClauses.join(' AND ')}`;
      }
    }

    // --- Vector Similarity Search (Conceptual) ---
    // This requires a specific setup in DuckDB (e.g., vss extension)
    // or manual calculation. Let's assume a filter operator 'vector_similarity'.
    const vectorFilter = filterOptions.filters?.find(f => f.operator === 'vector_similarity');
    if (vectorFilter && collection === 'conversations' && TABLE_SCHEMAS[collection].includes('embedding')) {
        // Assuming vectorFilter.value is the query embedding (number[])
        // Assuming vectorFilter.field is 'embedding'
        const queryEmbedding = vectorFilter.value as number[];
        // Example using hypothetical list_dot_product (needs extension or UDF)
        // Or calculate distance manually if needed.
        // This SQL is conceptual and depends heavily on DuckDB setup.
        // sql = `SELECT *, list_dot_product(embedding, list_value(${queryEmbedding.join(',')})) AS similarity FROM ${collection}`;
        // sql += ` ORDER BY similarity DESC`; // Order by similarity
        console.warn("Vector similarity search requested but not fully implemented in this skeleton.");
        // Add placeholder WHERE clause if needed based on filtering logic
    } else {
        // --- Basic Sorting ---
        if (filterOptions.sortBy) {
            sql += ` ORDER BY ${filterOptions.sortBy} ${filterOptions.sortDirection === 'desc' ? 'DESC' : 'ASC'}`;
        }
    }


    // --- Pagination ---
    if (filterOptions.limit !== undefined) {
      sql += ` LIMIT $${paramIndex++}`;
      params.push(filterOptions.limit);
    }
    if (filterOptions.offset !== undefined) {
      sql += ` OFFSET $${paramIndex++}`;
      params.push(filterOptions.offset);
    }

    try {
      console.log("Executing DuckDB Query:", sql, params);
      const stmt = await conn.prepare(sql);
      const results = await stmt.query(...params);
      await stmt.close();
      // Parse JSON columns for all results
      return results.toArray().map(arrowRecord => this.parseJsonColumns(collection, arrowRecord.toJSON()) as T);
    } catch (error) {
      console.error(`DuckDB query error in ${collection}:`, error);
      return []; // Return empty on error, or re-throw
    }
  }

   async clearCollection(collection: string): Promise&lt;void&gt; {
       const conn = await this.ensureConnection();
       try {
           const stmt = await conn.prepare(`DELETE FROM ${collection}`);
           await stmt.send();
           await stmt.close();
       } catch (error) {
           console.error(`DuckDB clearCollection error for ${collection}:`, error);
           throw error;
       }
   }

   async clearAll(): Promise&lt;void&gt; {
       const conn = await this.ensureConnection();
       try {
           for (const tableName of Object.keys(TABLE_SCHEMAS)) {
               const stmt = await conn.prepare(`DELETE FROM ${tableName}`);
               await stmt.send();
               await stmt.close();
           }
       } catch (error) {
           console.error(`DuckDB clearAll error:`, error);
           throw error;
       }
   }

   // Helper to parse columns that should be JSON
   private parseJsonColumns(collection: string, row: any): any {
       if (!row) return null;
       const schema = TABLE_SCHEMAS[collection];
       if (!schema) return row;

       const jsonFields = ['config', 'agentState', 'content', 'metadata']; // Fields potentially stored as JSON strings
       for (const field of jsonFields) {
           // Check if schema defines field as JSON and if current value is string
           if (schema.includes(`${field} JSON`) && typeof row[field] === 'string') {
               try {
                   row[field] = JSON.parse(row[field]);
               } catch (e) {
                   console.warn(`Failed to parse JSON field ${field} in collection ${collection}`, e);
                   // Keep as string if parsing fails
               }
           }
       }
       return row;
   }

   async close(): Promise&lt;void&gt; {
       if (this.initPromise) {
           await this.initPromise; // Ensure init is done before closing
       }
       if (this.connection) {
           console.log("Closing DuckDB connection...");
           await this.connection.close();
           this.connection = null;
       }
       if (this.db) {
           console.log("Terminating DuckDB instance...");
           await this.db.terminate();
           this.db = null;
       }
       this.initPromise = null; // Reset init promise
       console.log("DuckDB terminated.");
   }
}
</code></pre>
                    <h4 class="text-lg font-medium mt-6 mb-4 text-gray-800">Explanation:</h4>
                    <ol class="list-decimal list-inside space-y-3 text-gray-700">
                        <li><strong>Implement <code>StorageAdapter</code>:</strong> Fulfills the contract.</li>
                        <li><strong>DuckDB WASM Setup:</strong>
                            <ul class="list-disc list-inside ml-6 space-y-2 mt-2">
                                <li>Imports <code>@duckdb/duckdb-wasm</code>.</li>
                                <li>The <code>init</code> method handles the complex asynchronous loading of the WASM bundle, worker instantiation, database opening, and connection establishment. Uses <code>initPromise</code> to prevent race conditions on concurrent calls.</li>
                                <li><code>ensureTables</code> creates the necessary tables (including conceptual <code>embedding</code> column) if they don't exist.</li>
                                <li><code>ensureConnection</code> is a helper to guarantee initialization.</li>
                            </ul>
                        </li>
                         <li><strong>CRUD Methods:</strong>
                            <ul class="list-disc list-inside ml-6 space-y-2 mt-2">
                                <li>Translate operations into SQL using <strong>prepared statements</strong> (`prepare`, `query`, `send`) for security and efficiency.</li>
                                <li>Handles JSON stringification/parsing for relevant columns.</li>
                                <li>Includes conceptual embedding generation during <code>set</code> for the <code>conversations</code> table.</li>
                            </ul>
                        </li>
                        <li><strong><code>query</code> Method:</strong>
                             <ul class="list-disc list-inside ml-6 space-y-2 mt-2">
                                <li>Constructs SQL <code>SELECT</code> query.</li>
                                <li>Maps simple equality filters from <code>FilterOptions</code> to <code>WHERE</code> clauses using parameterized queries.</li>
                                <li>Includes a *conceptual placeholder* for vector similarity search, noting its complexity and dependency on potential DuckDB extensions (like <code>vss</code>) or manual calculations.</li>
                                <li>Adds basic <code>ORDER BY</code>, <code>LIMIT</code>, and <code>OFFSET</code>.</li>
                                <li><strong>Limitation:</strong> Explicitly notes that complex filtering and efficient vector search are advanced topics.</li>
                            </ul>
                        </li>
                        <li><strong>Cleanup:</strong> Includes an async <code>close</code> method to properly terminate the DB connection and worker, ensuring it waits for initialization if pending.</li>
                    </ol>
                 </div>

                 <div id="scenario-4-integration" class="mb-8">
                    <h3 class="text-xl font-medium mb-4 text-gray-800">6.3. Integrating the <code>DuckDBWasmAdapter</code></h3>
                    <p class="mb-4 text-gray-700">You integrate a custom storage adapter by passing an *instance* of it in the `storage` field of the configuration object given to `createArtInstance`.</p>
                    <p class="mb-4 text-gray-700"><strong>Example Integration Snippet:</strong></p>
<pre><code class="language-typescript">// --- Integration Example ---
import { DuckDBWasmAdapter } from './adapters/DuckDBWasmAdapter';
import { OpenAIAdapter, ProviderManagerConfig, PESAgent, CalculatorTool, createArtInstance, ArtInstance } from 'art-framework';

async function setupDuckDbArt(): Promise&lt;ArtInstance&gt; {
    // 1. Instantiate your custom storage adapter
    const storageAdapter = new DuckDBWasmAdapter(/* options */);
    // IMPORTANT: Initialize DuckDB WASM (or ensure it's called before first use)
    await storageAdapter.init();

    // 2. Define Provider Configuration
    const providerConfig: ProviderManagerConfig = {
        availableProviders: [
          { name: 'openai', adapter: OpenAIAdapter }
        ]
    };

    // 3. Define ART Configuration
    const config = {
        storage: storageAdapter, // Pass the initialized adapter instance
        providers: providerConfig,
        agentCore: PESAgent,
        tools: [new CalculatorTool()]
    };

    // 4. Create ART Instance
    const artInstance = await createArtInstance(config);

    // Optional: Add cleanup hook for DuckDB if your app lifecycle allows
    // window.addEventListener('beforeunload', () => storageAdapter.close());

    return artInstance;
}
</code></pre>
                    <h4 class="text-lg font-medium mt-6 mb-4 text-gray-800">How it Works Now:</h4>
                    <ul class="list-disc list-inside space-y-3 text-gray-700">
                        <li><strong>Node 1 (Developer Interface):</strong> You define `DuckDBWasmAdapter`, instantiate it, call its `init()` method, and pass the instance in the `storage` field when calling `createArtInstance`.</li>
                        <li><strong>Node 2 (Core Orchestration):</strong> The internal Repositories (like `ConversationRepository`, `StateRepository`) receive the `DuckDBWasmAdapter` instance via dependency injection. All internal calls to save or load data (e.g., `ConversationManager.getMessages`, `StateManager.loadThreadContext`) are routed through the Repositories to your adapter's `query`, `get`, `set`, or `delete` methods.</li>
                        <li><strong>Node 3 (External Dependencies & Interactions):</strong> Your adapter interacts with the DuckDB WASM engine, executing SQL commands to manage data stored potentially in the browser's file system (e.g., OPFS) or memory. If vector search is implemented, it also handles embedding generation and similarity calculations.</li>
                    </ul>
                 </div>
            </section>

            <section id="scenario-5" class="mb-16 fade-in-section">
                <h2 class="text-2xl md:text-3xl font-semibold mb-6 border-b pb-3 text-sky-700">7. Scenario 5: Adding a Custom Agent Pattern (Advanced Usage)</h2>
                <p class="mb-5 text-gray-700">
                    Let's implement the ReAct (Reason -> Act -> Observe) agent pattern and allow the user to switch between PES and ReAct in the chatbot UI.
                </p>
                <p class="mb-8 text-gray-700"><strong>Goal:</strong> Create a <code>ReActAgent</code> class, integrate it, and add UI controls for switching.</p>

                <div id="scenario-5-imports" class="mb-10">
                    <h3 class="text-xl font-medium mb-4 text-gray-800">7.1. Necessary Imports & Explanations</h3>
                    <p class="mb-4 text-gray-700">In addition to imports from previous scenarios, you need these for creating a custom agent core:</p>
<pre><code class="language-typescript">// --- ART Agent Core Creation Imports ---
import {
  // The interface that a custom agent core must implement
  IAgentCore,
  // Input properties for the process method
  AgentProps,
  // Output type for the process method
  AgentFinalResponse,
  // --- Interfaces for Dependencies Injected into the Agent Core ---
  // These define the components your custom agent will use internally
  StateManager,
  ConversationManager,
  ToolRegistry,
  PromptManager, // Handles prompt assembly using fragments and validation
  ReasoningEngine, // Returns AsyncIterable<StreamEvent>
  OutputParser, // Parses aggregated LLM output
  ObservationManager,
  ToolSystem,
  UISystem, // Needed for broadcasting stream events
  // Other needed types
  ToolSchema, ParsedToolCall, ToolResult, ArtStandardPrompt, StreamEvent, LLMMetadata, ExecutionMetadata,
  ConversationMessage, MessageRole, PromptContext, // Added PromptContext, ConversationMessage, MessageRole
  // Types for provider selection
  RuntimeProviderConfig, CallOptions
} from 'art-framework';
</code></pre>
                     <h4 class="text-lg font-medium mt-6 mb-4 text-gray-800">Explanation of Agent Core Imports:</h4>
                     <div class="space-y-4 text-gray-700">
                         <div>
                            <p><strong><code>IAgentCore</code></strong></p>
                            <p class="mb-2">The main blueprint for creating a new "thinking style" or reasoning process for the agent. If you want the agent to think differently than the default "Plan -> Use Tools -> Answer" style, you implement this.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                The core interface for custom agent logic. Your class must implement <code>IAgentCore</code>. Key requirements:
                                <ul class="list-disc list-inside ml-4">
                                    <li>Implement <code>async process(props: AgentProps): Promise&lt;AgentFinalResponse&gt;</code>: This method *is* your agent's brain. It receives the <code>AgentProps</code> (query, threadId, etc.) and must orchestrate all steps (loading data, constructing `ArtStandardPrompt` objects, using `PromptManager` for fragments/validation, determining `RuntimeProviderConfig`, creating `CallOptions`, calling LLMs via `ReasoningEngine`, handling the `AsyncIterable<StreamEvent>` response, parsing output via `OutputParser`, calling tools via `ToolSystem`, saving data, logging observations, pushing stream events via `UISystem`) according to your custom logic (e.g., a ReAct loop) and return the final response including the `ConversationMessage` and aggregated `ExecutionMetadata`.</li>
                                    <li>Define a <code>constructor</code> that accepts a single argument: an object containing instances of the necessary ART subsystems (dependencies) defined by the interfaces below (e.g., `constructor(private deps: { stateManager: StateManager, reasoningEngine: ReasoningEngine, uiSystem: UISystem, ... })`). The `AgentFactory` will automatically provide (inject) these dependencies when it instantiates your custom agent core based on the `config.agentCore` setting.</li>
                                </ul>
                            </details>
                        </div>
                         <div>
                            <p><strong>Dependency Interfaces (`StateManager`, `ConversationManager`, etc.)</strong></p>
                            <p class="mb-2">These are the built-in helpers and managers that ART gives to your custom agent brain so it doesn't have to reinvent everything (like how to talk to the LLM, use tools, remember history, log events, or broadcast UI updates). Your custom `process` method will use these helpers.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                These interfaces define the contracts for the core ART subsystems injected into your <code>IAgentCore</code> constructor. You'll use their methods within your <code>process</code> implementation:
                                <ul class="list-disc list-inside ml-4">
                                    <li><code>StateManager</code>: Use `.loadThreadContext(threadId)` to get <code>ThreadConfig</code> and <code>AgentState</code>. Use `.saveStateIfModified(threadId)` to persist state changes. Use `.isToolEnabled(threadId, toolName)` for checks. Use `getThreadConfigValue(threadId, 'runtimeProviderConfig')` to retrieve the default provider config for the thread.</li>
                                    <li><code>ConversationManager</code>: Use `.getMessages(threadId, options)` to retrieve history. Use `.addMessages(threadId, messages)` to save new user/assistant messages.</li>
                                    <li><code>ToolRegistry</code>: Use `.getAvailableTools({ enabledForThreadId })` to get <code>ToolSchema[]</code> for prompt context. Use `.getToolExecutor(toolName)` if needed (though <code>ToolSystem</code> is usually preferred).</li>
                                    <li><code>PromptManager</code>: Provides reusable text fragments and validation. Use `.getFragment(name, context)` to retrieve instruction blocks. Use `.validatePrompt(promptObject)` to ensure the `ArtStandardPrompt` object constructed by your agent logic is valid before sending it to the `ReasoningEngine`.</li>
                                     <li><code>ReasoningEngine</code>: Use `.call(prompt, callOptions)` to interact with the LLM, passing the *validated* `ArtStandardPrompt` object. The `callOptions` object (type `CallOptions`) must include the `RuntimeProviderConfig` (specifying provider name, model, API key, etc.) along with other parameters like `stream`, `threadId`, `traceId`. The `ReasoningEngine` uses this config to get the correct adapter instance from the `ProviderManager`. The method returns a `Promise<AsyncIterable<StreamEvent>>`, which your agent must consume.</li>
                                    <li><code>OutputParser</code>: Use `.parsePlanningOutput(...)`, `.parseSynthesisOutput(...)` (for PES-like flows) or potentially define/use custom methods to extract structured data (like thoughts, actions, final answers) from the LLM's raw response content (assembled from the stream).</li>
                                    <li><code>ObservationManager</code>: Use `.record(observationData)` frequently within your `process` logic to log key steps (start/end, LLM calls, tool calls, custom steps like 'thought' or 'action', and new <code>LLM_STREAM_...</code> events) for debugging and UI feedback via sockets.</li>
                                    <li><code>ToolSystem</code>: Use `.executeTools(parsedToolCalls, threadId, traceId)` to run one or more tools identified by your agent's logic. It handles retrieving the executor, validating input against the schema, calling `execute`, and returning `ToolResult[]`.</li>
                                    <li><code>UISystem</code>: Use `.getLLMStreamSocket()` to access the socket for broadcasting real-time <code>StreamEvent</code>s (received from the `ReasoningEngine`'s stream) to the UI. Also use `.getConversationSocket()` and `.getObservationSocket()`.</li>
                                </ul>
                            </details>
                        </div>
                    </div>
                </div>

                <div id="scenario-5-implementation" class="mb-10">
                    <h3 class="text-xl font-medium mb-4 text-gray-800">7.2. Implementing the <code>ReActAgent</code> (with Streaming & Provider Selection)</h3>
                    <p class="mb-4 text-gray-700">This requires defining the ReAct loop logic within the <code>process</code> method, including determining the `RuntimeProviderConfig`, creating `CallOptions`, constructing the `ArtStandardPrompt` object (possibly using fragments from `PromptManager` and validating it), handling the asynchronous stream from the `ReasoningEngine`, and pushing stream events via the `UISystem`.</p>
<pre><code class="language-typescript">// src/agents/ReActAgent.ts

import {
  IAgentCore, AgentProps, AgentFinalResponse, StateManager, ConversationManager, ToolRegistry,
  PromptManager, ReasoningEngine, OutputParser, ObservationManager, ToolSystem, UISystem,
  ObservationType, ConversationMessage, MessageRole, ToolSchema, ParsedToolCall, ToolResult,
  ArtStandardPrompt, StreamEvent, LLMMetadata, ExecutionMetadata, PromptContext,
  RuntimeProviderConfig, CallOptions // Ensure necessary types are imported
} from 'art-framework';

// Define a structure for the parsed ReAct step output
interface ReActStepOutput {
  thought: string;
  action?: string; // Tool name
  actionInput?: any; // Arguments for the tool
  finalAnswer?: string; // Final answer if found
  rawLLMOutput: string; // Aggregated output from the step's LLM call
}

// Define the structure for the ReAct scratchpad entry for building prompts
interface ReActScratchpadEntryForPrompt {
    thought: string;
    action?: string;
    actionInputJson?: string; // Stringified JSON for the template
    observation: string;
}
// Define the structure for internal scratchpad tracking
interface ReActInternalScratchpadEntry {
    thought: string;
    action?: string;
    actionInput?: any;
    observation: string;
}


export class ReActAgent implements IAgentCore {
    constructor(private deps: {
        stateManager: StateManager;
        conversationManager: ConversationManager;
        toolRegistry: ToolRegistry;
        promptManager: PromptManager;
        reasoningEngine: ReasoningEngine;
        outputParser: OutputParser; // May need custom ReAct parsing methods
        observationManager: ObservationManager;
        toolSystem: ToolSystem;
        uiSystem: UISystem;
    }) {}

    private parseReActOutput(llmOutput: string): ReActStepOutput {
        const thoughtMatch = llmOutput.match(/Thought:\s*([\s\S]*?)(?=Action:|Final Answer:|$)/i);
        const actionMatch = llmOutput.match(/Action:\s*(\w+)/i);
        const inputMatch = llmOutput.match(/Action Input:\s*({[\s\S]*?}|[\s\S]*?)(?=Thought:|Observation:|$)/i);
        const finalAnswerMatch = llmOutput.match(/Final Answer:\s*([\s\S]*)/i);

        let actionInput: any = null;
        if (inputMatch && inputMatch[1]) {
            const potentialInput = inputMatch[1].trim();
            try {
                actionInput = JSON.parse(potentialInput);
            } catch (e) {
                actionInput = potentialInput;
            }
        }
        return {
            thought: thoughtMatch ? thoughtMatch[1].trim() : "Could not parse thought.",
            action: actionMatch ? actionMatch[1].trim() : undefined,
            actionInput: actionInput,
            finalAnswer: finalAnswerMatch ? finalAnswerMatch[1].trim() : undefined,
            rawLLMOutput: llmOutput
        };
    }

    // This agent constructs the ArtStandardPrompt object directly
    private constructReActPromptObject(
        query: string,
        history: ConversationMessage[],
        tools: ToolSchema[],
        scratchpad: ReActInternalScratchpadEntry[],
        systemPromptText: string
    ): ArtStandardPrompt {
        const messages: ArtStandardPrompt = [];

        // System Prompt
        let toolDescriptions = tools.map(t => `Name: ${t.name}, Description: ${t.description}, Input Schema: ${JSON.stringify(t.inputSchema)}`).join('\n');
        if (!toolDescriptions) toolDescriptions = "No tools available.";
        messages.push({
            role: 'system',
            content: `${systemPromptText}\nYou are a helpful assistant that thinks step-by-step using the ReAct framework. Available tools:\n${toolDescriptions}\nAlways respond with the ReAct format:\nThought: [Your reasoning]\nAction: [tool_name or Final Answer:]\nAction Input: [Arguments as JSON object or the final answer content]`
        });

        // History (simplified for this example, could be more elaborate)
        history.forEach(msg => messages.push({ role: msg.role, content: msg.content }));

        // Current Query
        messages.push({ role: 'user', content: `User Query: ${query}` });

        // Scratchpad as Assistant messages
        let assistantScratchpadContent = "ReAct Scratchpad:\n";
        scratchpad.forEach((step, index) => {
            assistantScratchpadContent += `Thought ${index + 1}: ${step.thought}\n`;
            if (step.action) {
                assistantScratchpadContent += `Action ${index + 1}: ${step.action}\nAction Input ${index + 1}: ${JSON.stringify(step.actionInput)}\nObservation ${index + 1}: ${step.observation}\n`;
            }
        });
        assistantScratchpadContent += `Thought:`; // Prompt LLM to start with its thought

        messages.push({ role: 'assistant', content: assistantScratchpadContent });
        return messages;
    }


    async process(props: AgentProps): Promise<AgentFinalResponse> {
        const { query, threadId, userId, sessionId, configOverrides, executionContext } = props;
        const traceId = `react-trace-${Date.now()}`;
        const llmStreamSocket = this.deps.uiSystem.getLLMStreamSocket();

        await this.deps.observationManager.record({ type: ObservationType.PROCESS_START, threadId, traceId, content: { agentType: 'ReAct', query } });

        const context = await this.deps.stateManager.loadThreadContext(threadId, configOverrides);
        const initialHistory = await this.deps.conversationManager.getMessages(threadId);
        const tools = await this.deps.toolRegistry.getAvailableTools({ enabledForThreadId: threadId });
        const systemPromptText = context.threadConfig.systemPrompt || "You are a helpful assistant.";

        let runtimeConfig: RuntimeProviderConfig | undefined = context.threadConfig.runtimeProviderConfig;
        if (!runtimeConfig) {
             throw new Error(`No RuntimeProviderConfig found for thread ${threadId}. Please configure a provider.`);
        }

        const internalScratchpad: ReActInternalScratchpadEntry[] = [];
        let step = 0;
        const maxSteps = 7;
        let aggregatedLlmMetadata: LLMMetadata | undefined = undefined;

        while (step < maxSteps) {
            step++;
            await this.deps.observationManager.record({ type: 'REACT_STEP' as ObservationType, threadId, traceId, content: { step } });

            // 2. Construct Prompt Object & Validate
            const promptObject = this.constructReActPromptObject(query, initialHistory, tools, internalScratchpad, systemPromptText);
            const validatedPrompt = await this.deps.promptManager.validatePrompt(promptObject); // Validate the constructed object

            // 3. Call LLM and process stream
            const callOptions: CallOptions = {
                providerConfig: runtimeConfig,
                threadId, traceId, sessionId, userId,
                stream: true,
                callContext: 'AGENT_THOUGHT'
            };
            await this.deps.observationManager.record({ type: ObservationType.LLM_REQUEST, threadId, traceId, content: { phase: `react_step_${step}`, prompt: validatedPrompt, options: callOptions } });

            const stream = await this.deps.reasoningEngine.call(validatedPrompt, callOptions);
            let llmResponseBuffer = '';
            let stepLlmMetadata: LLMMetadata | undefined = undefined;

            for await (const event of stream) {
                 llmStreamSocket.notify(event); // Push all events to UI
                switch (event.type) {
                    case 'TOKEN':
                        llmResponseBuffer += event.data;
                        break;
                    case 'METADATA':
                         await this.deps.observationManager.record({ type: ObservationType.LLM_STREAM_METADATA, content: event.data, threadId, traceId, sessionId });
                         stepLlmMetadata = { ...(stepLlmMetadata || {}), ...event.data as LLMMetadata };
                         break;
                    case 'ERROR':
                        await this.deps.observationManager.record({ type: ObservationType.LLM_STREAM_ERROR, content: event.data, threadId, traceId, sessionId });
                        console.error(`ReAct Agent LLM Stream Error (Step ${step}):`, event.data);
                        throw new Error(`LLM Stream Error during ReAct step ${step}: ${(event.data as Error).message || event.data}`);
                    case 'END':
                         await this.deps.observationManager.record({ type: ObservationType.LLM_STREAM_END, threadId, traceId, sessionId });
                         break;
                }
            }
            // Aggregate metadata (simple overwrite for last step's, can be more complex)
            if (stepLlmMetadata) aggregatedLlmMetadata = { ...aggregatedLlmMetadata, ...stepLlmMetadata };
            await this.deps.observationManager.record({ type: ObservationType.LLM_RESPONSE, threadId, traceId, content: { phase: `react_step_${step}`, response: llmResponseBuffer, metadata: stepLlmMetadata } });

            const parsedOutput = this.parseReActOutput(llmResponseBuffer);
            await this.deps.observationManager.record({ type: 'thought' as ObservationType, threadId, traceId, content: parsedOutput.thought });

            if (parsedOutput.finalAnswer) {
                 await this.deps.observationManager.record({ type: ObservationType.SYNTHESIS_OUTPUT, threadId, traceId, content: { responseText: parsedOutput.finalAnswer } });
                 await this.deps.observationManager.record({ type: ObservationType.PROCESS_END, threadId, traceId, content: { status: 'success' } });

                 const finalAiMessage: ConversationMessage = { id: `react-final-${Date.now()}`, role: MessageRole.ASSISTANT, content: parsedOutput.finalAnswer, timestamp: Date.now(), threadId };
                 await this.deps.conversationManager.addMessages(threadId, [finalAiMessage]); // Add assistant message
                 await this.deps.stateManager.saveStateIfModified(threadId);
                 return { response: finalAiMessage, metadata: { traceId, llmMetadata: aggregatedLlmMetadata } };
            }

             let observationResultText = "No valid action taken or action was 'Final Answer' without content.";
             if (parsedOutput.action && parsedOutput.action.toLowerCase() !== 'final answer' && parsedOutput.actionInput !== undefined) {
                const toolName = parsedOutput.action;
                const toolInput = parsedOutput.actionInput;
                await this.deps.observationManager.record({ type: ObservationType.TOOL_START, threadId, traceId, metadata: { toolName: toolName, input: toolInput } });
                const toolCall: ParsedToolCall = { toolName: toolName, args: toolInput };
                try {
                    const toolResults = await this.deps.toolSystem.executeTools([toolCall], threadId, traceId, executionContext);
                    const result = toolResults[0];
                    observationResultText = JSON.stringify(result.status === 'success' ? result.output : { error: result.error });
                    await this.deps.observationManager.record({ type: ObservationType.TOOL_END, threadId, traceId, metadata: { toolName: toolName, resultStatus: result.status, output: result.output, error: result.error } });
                } catch (toolError: any) {
                     observationResultText = JSON.stringify({ error: `Failed to execute tool ${toolName}: ${toolError.message}` });
                     await this.deps.observationManager.record({ type: ObservationType.TOOL_END, threadId, traceId, metadata: { toolName: toolName, resultStatus: 'error', error: toolError.message } });
                }
            }
             internalScratchpad.push({
                 thought: parsedOutput.thought,
                 action: parsedOutput.action,
                 actionInput: parsedOutput.actionInput,
                 observation: observationResultText
             });
            await this.deps.observationManager.record({ type: 'observation' as ObservationType, threadId, traceId, content: observationResultText });
        }

        const failureText = "Reached maximum thinking steps without a final answer.";
        await this.deps.observationManager.record({ type: ObservationType.PROCESS_END, threadId, traceId, content: { status: 'max_steps_reached' } });
        const failureMessage: ConversationMessage = { id: `react-maxstep-${Date.now()}`, role: MessageRole.SYSTEM, content: failureText, timestamp: Date.now(), threadId };
        await this.deps.conversationManager.addMessages(threadId, [failureMessage]);
        await this.deps.stateManager.saveStateIfModified(threadId);
        return { response: failureMessage, metadata: { traceId, llmMetadata: aggregatedLlmMetadata } };
    }
}
</code></pre>
                    <h4 class="text-lg font-medium mt-6 mb-4 text-gray-800">Explanation:</h4>
                    <ol class="list-decimal list-inside space-y-3 text-gray-700">
                        <li><strong>Implements `IAgentCore`:</strong> Adheres to the contract.</li>
                        <li><strong>Constructor Dependencies:</strong> Includes `UISystem` for accessing the `LLMStreamSocket`.</li>
                         <li><strong>`process` Method:</strong> Contains the core ReAct loop:
                            <ul class="list-disc list-inside ml-6 space-y-2 mt-2">
                                <li>Loads context/history/tools.</li>
                                <li><strong>`RuntimeProviderConfig`:</strong> Checks `configOverrides` first, then falls back to the thread's default config loaded via `StateManager`. Throws an error if no provider is configured.</li>
                                <li><strong>Loop:</strong>
                                    <ul>
                                        <li>Constructs the `ArtStandardPrompt` object for ReAct directly (e.g., using `constructReActPromptObject`).</li>
                                        <li>Validates the prompt object using `this.deps.promptManager.validatePrompt()`.</li>
                                        <li>Creates `CallOptions`, importantly including the determined `runtimeProviderConfig`, `stream: true`, and `callContext: 'AGENT_THOUGHT'`.</li>
                                        <li>Calls `ReasoningEngine.call` with the validated prompt and options.</li>
                                        <li><strong>Consumption:</strong> Iterates through the `AsyncIterable<StreamEvent>`.
                                            <ul>
                                                <li>Pushes *every* event to `uiSystem.getLLMStreamSocket().notify(event)`.</li>
                                                <li>Buffers `TOKEN` data into `llmResponseBuffer`.</li>
                                                <li>Logs `LLM_STREAM_METADATA`, `LLM_STREAM_ERROR`, `LLM_STREAM_END` via `ObservationManager`.</li>
                                                <li>Aggregates metadata for the current step and overall execution.</li>
                                            </ul>
                                        </li>
                                         <li>Logs the aggregated `LLM_RESPONSE` after the stream ends.</li>
                                        <li>Parses the *buffered* `llmResponseBuffer` for Thought, Action, Action Input, or Final Answer (using `parseReActOutput`).</li>
                                        <li>Logs the parsed `thought`.</li>
                                        <li>If "Final Answer:", logs synthesis/end, saves history (including the AI's final message), and returns the `AgentFinalResponse` with the `ConversationMessage` and aggregated metadata.</li>
                                        <li>If "Action:", executes the tool using `ToolSystem`, logs start/end, and captures the result/error as the `observationResultText`.</li>
                                        <li>Adds the {thought, action, input, observation} to the `internalScratchpad` for the next loop iteration. Logs the `observation`.</li>
                                    </ul>
                                </li>
                                <li>Handles reaching max steps, saves a system failure message, and returns.</li>
                            </ul>
                        </li>
                         <li><strong>Custom Logic:</strong> Requires a robust `parseReActOutput` function and a `constructReActPromptObject` method tailored to the specific LLM's expected ReAct format when constructing the `ArtStandardPrompt` object.</li>
                    </ol>
                </div>

                <div id="scenario-5-integration" class="mb-8">
                     <h3 class="text-xl font-medium mb-4 text-gray-800">7.3. Integrating ReAct and Agent Switching into the Chatbot</h3>
                    <p class="mb-4 text-gray-700">Modify the <code>ArtChatbot</code> component:</p>
<pre><code class="language-typescript jsx">
// src/components/ArtChatbot.tsx
import React, { useState, useEffect, useRef, useCallback } from 'react';
// --- ART Imports ---
import {
  createArtInstance, ArtInstance, AgentProps, AgentFinalResponse,
  ConversationMessage, MessageRole, Observation, ObservationType,
  StreamEvent, ProviderManagerConfig, RuntimeProviderConfig, // Core types
  PESAgent, // Default agent
  IndexedDBStorageAdapter, OpenAIAdapter, CalculatorTool // Default components
} from 'art-framework';
// --- Custom Agent/Tool Imports ---
import { ReActAgent } from '../agents/ReActAgent'; // Import custom agent (adjust path)
import { CurrentInfoTool } from '../tools/CurrentInfoTool'; // Import custom tool

// --- Add Agent Type State ---
type AgentType = 'pes' | 'react';

// Helper to generate temporary IDs
const tempId = () => `temp-${Date.now()}-${Math.random().toString(16).slice(2)}`;

const ArtChatbot: React.FC = () => {
  const [messages, setMessages] = useState<ConversationMessage[]>([]);
  const [input, setInput] = useState('');
  const [isLoading, setIsLoading] = useState(false);
  const [status, setStatus] = useState<string>('Initializing...');
  const [selectedAgent, setSelectedAgent] = useState<AgentType>('pes'); // State for agent choice
  const artInstanceRef = useRef<ArtInstance | null>(null);
  const messageListRef = useRef<HTMLDivElement>(null);
  const threadId = 'web-chatbot-thread-1';
  const [isInitializing, setIsInitializing] = useState(true); // Track initialization state
  const streamingMessageIdRef = useRef<string | null>(null); // Track the ID of the message being streamed

  // --- Auto-scrolling ---
  useEffect(() => {
    if (messageListRef.current) {
      messageListRef.current.scrollTop = messageListRef.current.scrollHeight;
    }
  }, [messages]);

  // --- ART Initialization (Modified for Agent Switching) ---
  useEffect(() => {
    let isMounted = true;
    let unsubObservation: (() => void) | null = null;
    let unsubConversation: (() => void) | null = null;
    let unsubStream: (() => void) | null = null;

    const initializeArt = async () => {
      // Clear previous instance and subscriptions
      if (unsubObservation) unsubObservation();
      if (unsubConversation) unsubConversation();
      if (unsubStream) unsubStream();
      artInstanceRef.current = null; // Helps ensure we don't use stale instance

      if (!isMounted) return;
      setIsInitializing(true);
      setStatus(`Initializing ${selectedAgent.toUpperCase()} Agent...`);

      try {
        const AgentCoreClass = selectedAgent === 'react' ? ReActAgent : PESAgent;

        // Define Provider Configuration
        const providerConfig: ProviderManagerConfig = {
            availableProviders: [
              { name: 'openai', adapter: OpenAIAdapter }
              // Add other providers like AnthropicAdapter if needed
            ]
        };

        // Define ART Factory Configuration
        const config = {
          storage: new IndexedDBStorageAdapter({ dbName: `artWebChatHistory-${selectedAgent}` }), // Separate DB per agent?
          providers: providerConfig,
          agentCore: AgentCoreClass, // Dynamically set the agent core
          tools: [
              new CalculatorTool(),
              new CurrentInfoTool() // Include custom tool
          ]
          // logger: { level: 'debug' } // Optional: Enable detailed logging
        };

        const instance = await createArtInstance(config);
        if (!isMounted) return;

        artInstanceRef.current = instance;

        // Set default provider config for the thread if not already set
        // This ensures the AgentCore has a default provider to use
        try {
            const currentProviderConfig = await instance.stateManager.getThreadConfigValue(threadId, 'runtimeProviderConfig');
            if (!currentProviderConfig) {
                const defaultRuntimeConfig: RuntimeProviderConfig = {
                    providerName: 'openai', // Default to openai
                    modelId: 'gpt-4o', // Ensure a valid default model
                    adapterOptions: { apiKey: import.meta.env.VITE_OPENAI_API_KEY || 'YOUR_API_KEY_HERE' } // Ensure API Key is present
                };
                await instance.stateManager.setThreadConfigValue(threadId, 'runtimeProviderConfig', defaultRuntimeConfig);
                console.log("Set default provider config for thread:", threadId);
            }
        } catch (configError) {
            console.error("Error setting/getting default provider config:", configError);
             if (isMounted) setStatus(`Config Error: ${configError instanceof Error ? configError.message : 'Unknown config error'}`);
             setIsInitializing(false); // Stop initialization on critical config error
             return; // Prevent further setup
        }


        setStatus('Loading history...');
        await loadMessages(); // Reload messages for the new instance/config

        // --- Re-subscribe to Observations ---
        setStatus('Connecting observers...');
        const observationSocket = instance.uiSystem.getObservationSocket();
        unsubObservation = observationSocket.subscribe(
            (observation: Observation) => {
               if (observation.threadId === threadId && isMounted && !isLoading) { // Avoid status flicker during processing
                    let newStatus = status;
                    switch (observation.type) {
                        case ObservationType.PROCESS_START: newStatus = 'Processing...'; break;
                        case ObservationType.LLM_REQUEST: newStatus = 'Thinking...'; break;
                        case ObservationType.TOOL_START: newStatus = `Using ${observation.metadata?.toolName}...`; break;
                        case ObservationType.TOOL_END: newStatus = 'Tool finished.'; break;
                        case ObservationType.PROCESS_END: newStatus = 'Ready.'; streamingMessageIdRef.current = null; break;
                        case ObservationType.LLM_STREAM_END: newStatus = 'Receiving final response...'; break;
                        case ObservationType.LLM_STREAM_ERROR: newStatus = 'Stream error.'; streamingMessageIdRef.current = null; break;
                        case 'thought' as ObservationType: newStatus = 'Thinking (ReAct)...'; break;
                        case 'observation' as ObservationType: newStatus = 'Observing (ReAct)...'; break;
                        default: break; // Keep current status for other observations
                    }
                    setStatus(newStatus);
               }
            },
            undefined, // Subscribe to all types for simplicity here
            { threadId: threadId }
        );

        // --- Re-subscribe to Conversation (Final Messages) ---
         const conversationSocket = instance.uiSystem.getConversationSocket();
         unsubConversation = conversationSocket.subscribe(
             (message: ConversationMessage) => {
                 if (message.threadId === threadId && isMounted) {
                     console.log("Received final message via socket:", message);
                     setMessages(prev => {
                         const existingIndex = prev.findIndex(m => m.id === streamingMessageIdRef.current && m.role === MessageRole.ASSISTANT);
                         if (existingIndex > -1) {
                             const updatedMessages = [...prev];
                             updatedMessages[existingIndex] = message; // Replace temp with final
                             return updatedMessages;
                         } else if (!prev.some(m => m.id === message.id)) {
                             return [...prev, message].sort((a, b) => a.timestamp - b.timestamp); // Add if new
                         }
                         return prev;
                     });
                     streamingMessageIdRef.current = null; // Clear streaming ID
                 }
             },
             undefined, { threadId: threadId }
         );

        // --- Re-subscribe to LLM Stream ---
        const streamSocket = instance.uiSystem.getLLMStreamSocket();
        unsubStream = streamSocket.subscribe(
            (event: StreamEvent) => {
                if (event.threadId === threadId && isMounted) {
                    if (event.type === 'TOKEN' &&
                    (event.tokenType === 'FINAL_SYNTHESIS_LLM_RESPONSE' || // PES final answer
                        event.tokenType?.startsWith('FINAL_SYNTHESIS') || // Catch ReAct final answer tokens
                        (selectedAgent === 'react' && event.tokenType === 'AGENT_THOUGHT_LLM_RESPONSE')) // ReAct agent might stream final answer during thought
                    ) {
                        setMessages(prev => {
                            const currentStreamingId = streamingMessageIdRef.current;
                            if (!currentStreamingId) {
                                const newStreamingMessage: ConversationMessage = {
                                    id: tempId(), role: MessageRole.ASSISTANT, content: event.data,
                                    timestamp: Date.now(), threadId: threadId, metadata: { streaming: true }
                                };
                                streamingMessageIdRef.current = newStreamingMessage.id;
                                return [...prev, newStreamingMessage];
                            } else {
                                return prev.map(msg => msg.id === currentStreamingId ? { ...msg, content: msg.content + event.data } : msg);
                            }
                        });
                    } else if (event.type === 'ERROR') {
                        console.error("Stream Error:", event.data);
                        setStatus('Stream Error');
                        setMessages(prev => [...prev, { id: tempId(), role: MessageRole.SYSTEM, content: `Stream Error: ${ (event.data as Error).message || String(event.data)}`, timestamp: Date.now(), threadId: threadId }]);
                        streamingMessageIdRef.current = null;
                    }
                }
            },
            undefined, // Stream type filter - undefined means subscribe to all types
            { threadId: threadId } // Filter object
        );


        if (isMounted) setStatus('Ready.');

      } catch (error) {
        console.error(`Failed to initialize ${selectedAgent.toUpperCase()} ART:`, error);
        if (isMounted) setStatus(`Initialization Error: ${error instanceof Error ? error.message : 'Unknown error'}`);
      } finally {
         if (isMounted) setIsInitializing(false);
      }
    };

    initializeArt(); // Initialize on mount and when selectedAgent changes

    // Cleanup function
    return () => {
      isMounted = false;
      console.log("Cleaning up ART subscriptions...");
      if (unsubObservation) unsubObservation();
      if (unsubConversation) unsubConversation();
      if (unsubStream) unsubStream();
    };
  }, [selectedAgent, threadId]); // Re-run useEffect when selectedAgent changes!

  // --- Load Messages ---
  const loadMessages = useCallback(async () => {
    setMessages([]); // Clear messages before loading
    if (!artInstanceRef.current) return;
     try {
       const history = await artInstanceRef.current.conversationManager.getMessages(threadId, { limit: 100 });
       setMessages(history.sort((a, b) => a.timestamp - b.timestamp));
     } catch (error) {
       console.error("Failed to load messages:", error);
       setStatus('Error loading history.');
     }
  }, [threadId]);

  // --- Handle Sending ---
  const handleSend = useCallback(async () => {
     if (!input.trim() || !artInstanceRef.current || isLoading || isInitializing) return;

     const userMessage: ConversationMessage = {
       id: `user-${Date.now()}`, role: MessageRole.USER, content: input,
       timestamp: Date.now(), threadId: threadId,
     };
     setMessages(prev => [...prev, userMessage]);
     streamingMessageIdRef.current = null; // Reset streaming ID before sending

     const currentInput = input;
     setInput('');
     setIsLoading(true);
     setStatus('Processing request...');

     try {
        // AgentCore will internally load the runtimeProviderConfig from thread state.
        // If you need to override it for a specific call, do it here:
        // const specificRuntimeConfig: RuntimeProviderConfig = { ... };
        const props: AgentProps = {
            query: currentInput,
            threadId: threadId,
            // configOverrides: { runtimeProviderConfig: specificRuntimeConfig }
        };

       const response: AgentFinalResponse = await artInstanceRef.current.process(props);
       console.log("ART process completed. Final Response:", response);
       // Final message display is primarily handled by ConversationSocket subscription

     } catch (error) {
       console.error("Error processing message:", error);
       const errorMsg = error instanceof Error ? error.message : 'Failed to get response';
       setMessages(prev => [...prev, { id: `error-${Date.now()}`, role: MessageRole.SYSTEM, content: `Error: ${errorMsg}`, timestamp: Date.now(), threadId: threadId }]);
       setStatus('Error occurred.');
        streamingMessageIdRef.current = null;
     } finally {
       setIsLoading(false);
     }
  }, [input, isLoading, threadId, isInitializing, selectedAgent]); // Removed 'parsedOutput' from deps as it's not defined in this scope.
  
  // --- Render Component ---
return (
    <div className="chatbot-container">
        {/* Agent Switcher UI */}
        <div style={{ padding: '5px 10px', borderBottom: '1px solid #eee', textAlign: 'center' }}>
        <label>Agent Mode: </label>
        <select 
            value={selectedAgent} 
            onChange={(e) => setSelectedAgent(e.target.value as AgentType)} 
            disabled={isInitializing || isLoading}
        >
            <option value="pes">Plan-Execute-Synthesize</option>
            <option value="react">ReAct</option>
        </select>
        {(isInitializing || isLoading) && <span style={{ marginLeft: '10px', fontSize: '0.8em' }}>{status}...</span>}
        </div>

        <div className="message-list" ref={messageListRef}>
        {messages.map((msg) => (
            <div key={msg.id} className={`message ${msg.role} ${msg.metadata?.streaming ? 'streaming' : ''}`}>
            <pre style={{ whiteSpace: 'pre-wrap', margin: 0, fontFamily: 'inherit' }}>{msg.content}</pre>
            </div>
        ))}
        </div>
        <div className="status-indicator">
        {isInitializing ? 'Initializing...' : (isLoading ? status : 'Ready.')}
        </div>
        <div className="input-area">
        <input
            type="text"
            value={input}
            onChange={(e) => setInput(e.target.value)}
            onKeyPress={(e) => e.key === 'Enter' && !isLoading && !isInitializing && handleSend()}
            disabled={isLoading || isInitializing || !artInstanceRef.current}
            placeholder={isInitializing ? 'Initializing...' : (artInstanceRef.current ? "Ask something..." : "Error - Init failed")}
        />
        <button
            onClick={handleSend}
            disabled={isLoading || isInitializing || !artInstanceRef.current || !input.trim()}
        >
            {isLoading ? '...' : 'Send'}
        </button>
        </div>
    </div>
    );

    export default ArtChatbot;
</code></pre>
                    <h4 class="text-lg font-medium mt-6 mb-4 text-gray-800">Explanation of Changes:</h4>
                     <ol class="list-decimal list-inside space-y-3 text-gray-700">
                         <li><strong>State for Agent Type:** Added <code>selectedAgent</code> state (<code>'pes'</code> or <code>'react'</code>).</li>
                         <li><strong>Agent Switcher UI:** Added a <code><select></code> dropdown to allow the user to change the <code>selectedAgent</code> state.</li>
                         <li><strong>Dynamic Initialization:** The main <code>useEffect</code> hook now has <code>selectedAgent</code> in its dependency array. This means whenever <code>selectedAgent</code> changes, the effect re-runs:
                             <ul class="list-disc list-inside ml-6 space-y-2 mt-2">
                                 <li>It determines the correct <code>AgentCoreClass</code> (<code>PESAgent</code> or <code>ReActAgent</code>) based on the state.</li>
                                 <li>Updates the `createArtInstance` config to use the new `providers` structure, registering the `OpenAIAdapter` class.</li>
                                 <li>It calls <code>createArtInstance</code> with the chosen <code>agentCore</code>.</li>
                                 <li>It reloads messages (potentially from a different DB if configured, or clears the list for the new agent context).</li>
                                 <li>It re-subscribes to the `ObservationSocket`, `ConversationSocket`, and `LLMStreamSocket` for the new instance.</li>
                                 <li>Includes logic to set a default `RuntimeProviderConfig` for the thread if one doesn't exist, ensuring the API key is present.</li>
                             </ul>
                         </li>
                         <li><strong>Loading/Disabled States:** Added <code>isInitializing</code> state and updated `disabled` conditions on input/button/select to prevent interaction while ART is initializing or processing.</li>
                         <li><strong>Stream Handling in UI:**
                             <ul class="list-disc list-inside ml-6 space-y-2 mt-2">
                                 <li>Subscribes to `LLMStreamSocket`.</li>
                                 <li>Uses `streamingMessageIdRef` to track the temporary ID of the message being built from tokens.</li>
                                 <li>On receiving a `TOKEN` event (of the correct `tokenType`, including `FINAL_SYNTHESIS_LLM_RESPONSE` or ReAct's final answer tokens), it either creates a new temporary assistant message or appends the token to the existing one.</li>
                                 <li>On receiving the final message via `ConversationSocket`, it replaces the temporary message with the final, persisted one using the ID.</li>
                             </ul>
                          </li>
                         <li><strong>Handle Send:** The `AgentCore` (PESAgent or ReActAgent) is responsible for loading the `RuntimeProviderConfig` from thread state or using overrides passed in `AgentProps`. The UI component primarily focuses on initiating the `process` call.</li>
                     </ol>
                    <h4 class="text-lg font-medium mt-6 mb-4 text-gray-800">How it Works Now:</h4>
                    <ul class="list-disc list-inside space-y-3 text-gray-700">
                        <li><strong>Node 1 (Developer Interface):** You've defined the <code>ReActAgent</code> and provided UI controls (<code><select></code>) to change the <code>selectedAgent</code> state. The `useEffect` hook dynamically sets the <code>agentCore</code> in the `config` based on this state before calling `createArtInstance`. The `AgentCore` (e.g., `ReActAgent` or `PESAgent`) is responsible for determining the `RuntimeProviderConfig` for the specific LLM call, typically by loading it from `ThreadConfig` or using an override from `AgentProps`.</li>
                        <li><strong>Node 2 (Core Orchestration):** When the user selects an agent type, `createArtInstance` builds the internal engine using the *specified* <code>IAgentCore</code> implementation. When `art.process()` is called, the framework routes the call to the currently active agent core's `process` method. This method uses the determined `RuntimeProviderConfig` within `CallOptions` when calling the `ReasoningEngine`. The `ReasoningEngine` uses this config to get the correct adapter instance from the `ProviderManager` and execute the LLM call, handling streaming and releasing the adapter.</li>
                        <li><strong>Node 3 (External Dependencies & Interactions):** The `ProviderAdapter` instance obtained from the `ProviderManager` handles the streaming communication with the LLM. Tools are invoked based on the logic within the active `IAgentCore` implementation (PES or ReAct).</li>
                    </ul>
                </div>
            </section>

            <section id="conclusion" class="mb-16 fade-in-section">
                <h2 class="text-2xl md:text-3xl font-semibold mb-6 border-b pb-3 text-sky-700">8. Conclusion</h2>
                <p class="text-gray-700">
                    ART offers a layered approach to building browser-based AI agents. Developers can start simply by configuring built-in components, progress to extending capabilities with custom tools and adapters, and finally achieve deep customization by implementing entirely new agent reasoning patterns. Recent advancements, such as the flexible prompt management system (where agent patterns construct `ArtStandardPrompt` objects, optionally using fragments from `PromptManager`, which also validates these objects), the implementation of real-time LLM response streaming, dynamic provider management, and flexible state persistence, further enhance ART's capabilities. By understanding the 3-node architecture and the different usage levels, you can effectively leverage ART to create powerful and flexible client-side AI applications.
                </p>
            </section>

        </main>
    </div>

    <script>
        // Initialize Mermaid
        mermaid.initialize({ startOnLoad: true });

        // Initialize Highlight.js
        hljs.highlightAll();

        // Sidebar Active Link Highlighting
        const sections = document.querySelectorAll('main section[id], main div[id]'); // Include divs with IDs for subsections
        const navLinks = document.querySelectorAll('#sidebar-nav > li > a'); // Direct children links
        const subNavLinks = document.querySelectorAll('#sidebar-nav ul ul a'); // Sub-links

        function changeActiveLink() {
            let index = sections.length;
            let currentId = '';

            // Find the topmost section currently visible, considering a small offset
            while(--index >= 0 && window.scrollY + 150 < sections[index].offsetTop) { /* empty loop body */ } // Increased offset slightly

             if (index >= 0) {
                currentId = sections[index].id;
             }

            // Remove active class from all links first
            navLinks.forEach((link) => link.classList.remove('active'));
            subNavLinks.forEach((link) => link.classList.remove('active'));

            if (currentId) {
                // Try finding the specific sub-link first
                let activeSubLink = document.querySelector(`.sidebar ul ul a[href="#${currentId}"]`);
                let activeMainLink = null;

                if (activeSubLink) {
                    // Highlight the sub-link
                    activeSubLink.classList.add('active');
                    // Find and highlight the parent main link
                    let tempClosestUl = activeSubLink.closest('ul.ml-4');
                    let parentLi = tempClosestUl ? tempClosestUl.closest('li') : null;
                    activeMainLink = parentLi ? parentLi.querySelector('a.sidebar-link') : null;
                } else {
                    // If no specific sub-link, find the main link
                    activeMainLink = document.querySelector(`.sidebar-link[href="#${currentId}"]`);
                }

                 // Add active class to the main link found (either direct match or parent)
                 if (activeMainLink) {
                     activeMainLink.classList.add('active');
                 }
            }
        }


        // Initial check and add listener
        changeActiveLink();
        window.addEventListener('scroll', changeActiveLink);

         // Smooth scroll for sidebar links (already handled by CSS `scroll-behavior: smooth;`)

         // Fade-in sections on scroll (Simple version)
         const observer = new IntersectionObserver((entries) => {
             entries.forEach(entry => {
                 if (entry.isIntersecting) {
                     entry.target.classList.add('visible'); // You might need to adjust CSS for .visible
                     // Optional: Stop observing after it becomes visible
                     // observer.unobserve(entry.target);
                 }
             });
         }, { threshold: 0.1 }); // Trigger when 10% visible

         document.querySelectorAll('.fade-in-section').forEach(section => {
             observer.observe(section);
         });

    </script>
</body>
</html>