<!DOCTYPE html>
<html lang="en" class="scroll-smooth">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ART Framework: Comprehensive Developer Guide</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-light.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/typescript.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/javascript.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/json.min.js"></script>

    <style>
        /* Custom Styles */
        body {
            font-family: 'Inter', sans-serif;
            background-color: #f8fafc; /* Tailwind gray-50 */
            color: #374151; /* Tailwind gray-700 */
        }

        /* Style code blocks */
        pre code.hljs {
            display: block;
            overflow-x: auto;
            padding: 1.5em;
            border-radius: 0.5rem;
            background: #f3f4f6; /* Tailwind gray-100 */
            color: #1f2937; /* Tailwind gray-800 */
            font-size: 0.9em;
            line-height: 1.6;
        }
        /* Inline code */
        code:not(pre > code) {
             background-color: #e5e7eb; /* Tailwind gray-200 */
             padding: 0.1em 0.3em;
             border-radius: 0.25rem;
             font-size: 0.9em;
        }

        /* Ensure mermaid diagram is centered and responsive */
        .mermaid {
            display: block;
            margin: 2rem auto; /* Increased margin */
            max-width: 100%;
            text-align: center;
        }
        .mermaid svg {
             max-width: 100%;
             height: auto;
        }

        /* Sidebar active link style */
        .sidebar-link.active {
            background-color: #e0f2fe; /* Tailwind sky-100 */
            color: #0c4a6e; /* Tailwind sky-800 */
            font-weight: 600;
        }

        /* Smooth scroll */
        html {
            scroll-behavior: smooth;
        }

        /* Simple fade-in animation */
        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(10px); }
            to { opacity: 1; transform: translateY(0); }
        }
        .fade-in-section {
            animation: fadeIn 0.6s ease-out forwards;
        }

        /* Details/Summary styling for Developer Notes */
        details {
            background-color: #f9fafb; /* Tailwind gray-50 */
            border: 1px solid #e5e7eb; /* Tailwind gray-200 */
            border-radius: 0.375rem; /* rounded-md */
            padding: 0.75rem 1rem; /* px-4 py-3 */
            margin-top: 0.5rem; /* mt-2 */
        }
        summary {
            font-weight: 500; /* medium */
            color: #4b5563; /* gray-600 */
            cursor: pointer;
            list-style-position: inside; /* Keeps marker aligned */
        }
         details[open] > summary {
             margin-bottom: 0.5rem; /* mb-2 */
         }
        details > *:not(summary) {
            font-size: 0.9em;
            color: #4b5563; /* gray-600 */
            line-height: 1.6;
        }
        details ul {
            margin-top: 0.5rem;
            padding-left: 1rem;
        }
         details ul li {
             margin-bottom: 0.25rem;
         }

        /* Responsive adjustments */
        @media (max-width: 768px) {
            .sidebar {
                position: static;
                width: 100%;
                height: auto;
                margin-bottom: 1rem;
                border-right: none;
                border-bottom: 1px solid #e5e7eb;
            }
            .main-content {
                margin-left: 0;
                padding-top: 1rem;
            }
            .sidebar ul {
                display: flex;
                flex-wrap: wrap;
                justify-content: center;
            }
             .sidebar li {
                margin-right: 0.5rem;
                margin-bottom: 0.5rem;
            }
        }

    </style>
</head>
<body class="text-gray-800">

    <div class="flex flex-col md:flex-row min-h-screen">
        <aside class="sidebar w-full md:w-64 lg:w-72 bg-white border-r border-gray-200 p-4 md:p-6 md:sticky md:top-0 md:h-screen overflow-y-auto">
            <h1 class="text-xl lg:text-2xl font-bold text-sky-700 mb-6">ART Framework</h1>
            <nav>
                <ul class="space-y-2" id="sidebar-nav">
                    <li>
                        <a href="#introduction" class="sidebar-link block px-4 py-2 rounded-md text-gray-700 hover:bg-gray-100 transition duration-150">1. Introduction</a>
                        <ul class="ml-4 mt-1 space-y-1 text-sm">
                            <li><a href="#usage-complexity" class="block px-3 py-1 rounded text-gray-600 hover:bg-gray-100 transition duration-150">1.1 Usage Complexity</a></li>
                        </ul>
                    </li>
                    <li>
                        <a href="#architecture" class="sidebar-link block px-4 py-2 rounded-md text-gray-700 hover:bg-gray-100 transition duration-150">2. Architecture</a>
                         <ul class="ml-4 mt-1 space-y-1 text-sm">
                            <li><a href="#streaming-architecture" class="block px-3 py-1 rounded text-gray-600 hover:bg-gray-100 transition duration-150">2.1 Streaming Architecture</a></li>
                            <li><a href="#prompt-architecture" class="block px-3 py-1 rounded text-gray-600 hover:bg-gray-100 transition duration-150">2.2 Prompt Architecture</a></li>
                            <li><a href="#state-management-architecture" class="block px-3 py-1 rounded text-gray-600 hover:bg-gray-100 transition duration-150">2.3 State Management</a></li>
                        </ul>
                    </li>
                    <li>
                        <a href="#scenario-1" class="sidebar-link block px-4 py-2 rounded-md text-gray-700 hover:bg-gray-100 transition duration-150">3. Scenario 1: React Chatbot</a>
                        <ul class="ml-4 mt-1 space-y-1 text-sm">
                            <li><a href="#scenario-1-imports" class="block px-3 py-1 rounded text-gray-600 hover:bg-gray-100 transition duration-150">3.1 Imports</a></li>
                            <li><a href="#scenario-1-component" class="block px-3 py-1 rounded text-gray-600 hover:bg-gray-100 transition duration-150">3.2 Component</a></li>
                             <li><a href="#scenario-1-workflow" class="block px-3 py-1 rounded text-gray-600 hover:bg-gray-100 transition duration-150">3.3 Internal Workflow</a></li>
                        </ul>
                    </li>
                    <li>
                        <a href="#scenario-2" class="sidebar-link block px-4 py-2 rounded-md text-gray-700 hover:bg-gray-100 transition duration-150">4. Scenario 2: Custom Tool</a>
                         <ul class="ml-4 mt-1 space-y-1 text-sm">
                            <li><a href="#scenario-2-imports" class="block px-3 py-1 rounded text-gray-600 hover:bg-gray-100 transition duration-150">4.1 Imports</a></li>
                            <li><a href="#scenario-2-implementation" class="block px-3 py-1 rounded text-gray-600 hover:bg-gray-100 transition duration-150">4.2 Implementation</a></li>
                            <li><a href="#scenario-2-integration" class="block px-3 py-1 rounded text-gray-600 hover:bg-gray-100 transition duration-150">4.3 Integration</a></li>
                        </ul>
                    </li>
                     <li>
                        <a href="#scenario-3" class="sidebar-link block px-4 py-2 rounded-md text-gray-700 hover:bg-gray-100 transition duration-150">5. Scenario 3: Custom Provider</a>
                         <ul class="ml-4 mt-1 space-y-1 text-sm">
                            <li><a href="#scenario-3-imports" class="block px-3 py-1 rounded text-gray-600 hover:bg-gray-100 transition duration-150">5.1 Imports</a></li>
                            <li><a href="#scenario-3-implementation" class="block px-3 py-1 rounded text-gray-600 hover:bg-gray-100 transition duration-150">5.2 Implementation</a></li>
                            <li><a href="#scenario-3-integration" class="block px-3 py-1 rounded text-gray-600 hover:bg-gray-100 transition duration-150">5.3 Integration</a></li>
                        </ul>
                    </li>
                     <li>
                        <a href="#scenario-4" class="sidebar-link block px-4 py-2 rounded-md text-gray-700 hover:bg-gray-100 transition duration-150">6. Scenario 4: Custom Storage</a>
                         <ul class="ml-4 mt-1 space-y-1 text-sm">
                            <li><a href="#scenario-4-imports" class="block px-3 py-1 rounded text-gray-600 hover:bg-gray-100 transition duration-150">6.1 Imports</a></li>
                            <li><a href="#scenario-4-implementation" class="block px-3 py-1 rounded text-gray-600 hover:bg-gray-100 transition duration-150">6.2 Implementation</a></li>
                            <li><a href="#scenario-4-integration" class="block px-3 py-1 rounded text-gray-600 hover:bg-gray-100 transition duration-150">6.3 Integration</a></li>
                        </ul>
                    </li>
                    <li>
                        <a href="#scenario-5" class="sidebar-link block px-4 py-2 rounded-md text-gray-700 hover:bg-gray-100 transition duration-150">7. Scenario 5: Custom Agent</a>
                         <ul class="ml-4 mt-1 space-y-1 text-sm">
                            <li><a href="#scenario-5-imports" class="block px-3 py-1 rounded text-gray-600 hover:bg-gray-100 transition duration-150">7.1 Imports</a></li>
                            <li><a href="#scenario-5-implementation" class="block px-3 py-1 rounded text-gray-600 hover:bg-gray-100 transition duration-150">7.2 Implementation</a></li>
                            <li><a href="#scenario-5-integration" class="block px-3 py-1 rounded text-gray-600 hover:bg-gray-100 transition duration-150">7.3 Integration</a></li>
                        </ul>
                    </li>
                    <li><a href="#conclusion" class="sidebar-link block px-4 py-2 rounded-md text-gray-700 hover:bg-gray-100 transition duration-150">8. Conclusion</a></li>
                </ul>
            </nav>
        </aside>

        <main class="main-content flex-1 p-6 md:p-10 lg:p-12 overflow-y-auto">

            <section id="introduction" class="mb-16 p-6 bg-white rounded-lg shadow-lg fade-in-section">
                <h2 class="text-2xl md:text-3xl font-semibold mb-6 border-b pb-3 text-sky-700">1. Introduction: What is ART?</h2>
                <p class="mb-5 text-gray-700 leading-relaxed">
                    ART (Agent Reasoning & Tooling) is a JavaScript/TypeScript framework designed specifically for building intelligent, AI-powered agents that run <strong>directly in the user's web browser</strong>. Think of it as a toolkit that helps you create applications like sophisticated chatbots, research assistants, or automated helpers without needing a separate server for the core AI logic.
                </p>
                <h3 class="text-xl font-medium mb-3 text-gray-800">Core Goals:</h3>
                <ul class="list-disc list-inside mb-5 space-y-2 text-gray-700 leading-relaxed">
                    <li><strong>Client-Side First:</strong> Runs entirely in the browser, making web-native AI apps possible.</li>
                    <li><strong>Modular:</strong> Built like LEGO bricks â€“ different parts (like memory, reasoning engine, tools) can be swapped or added.</li>
                    <li><strong>Flexible:</strong> Adaptable to different AI models, tools, and agent behaviors.</li>
                    <li><strong>Decoupled:</strong> Components work together through defined contracts (interfaces), not direct dependencies, making the system easier to manage and extend.</li>
                </ul>
                <h3 class="text-xl font-medium mb-3 text-gray-800">Who is this guide for?</h3>
                <p class="mb-5 text-gray-700 leading-relaxed">
                    This guide is for web developers who want to build applications using Large Language Models (LLMs) directly in the browser. We'll cover everything from basic setup to advanced customization, using both technical terms and simpler explanations.
                </p>

                 <div id="usage-complexity" class="mt-10">
                     <h3 class="text-xl font-medium mb-4 text-gray-800">1.1. Usage Complexity Levels</h3>
                    <p class="mb-4 text-gray-700 leading-relaxed">Developers can engage with ART at different levels of complexity, depending on their needs:</p>
                    <ul class="list-disc list-inside space-y-4 text-gray-700 leading-relaxed">
                        <li>
                            <strong>Level 1: Simple Usage (Using Built-ins)</strong>
                            <ul class="list-circle list-inside ml-6 space-y-1 text-sm mt-1">
                                <li><strong>Focus:</strong> Configuration.</li>
                                <li><strong>Activities:</strong> Select from ART's pre-built adapters (storage, reasoning), use the default agent pattern (<code>PESAgent</code>), and potentially include built-in tools. Initialize via <code>createArtInstance</code> and use <code>art.process()</code>.</li>
                                <li><strong>Goal:</strong> Quickly set up a functional agent using standard components.</li>
                            </ul>
                        </li>
                        <li>
                            <strong>Level 2: Intermediate Usage (Extending with Custom Tools/Adapters)</strong>
                             <ul class="list-circle list-inside ml-6 space-y-1 text-sm mt-1">
                                <li><strong>Focus:</strong> Extension & Integration.</li>
                                <li><strong>Activities:</strong> Includes Simple Usage activities, PLUS implementing custom <code>IToolExecutor</code> interfaces to add specific capabilities (e.g., interacting with your backend, using specific libraries) or custom <code>ProviderAdapter</code>/<code>StorageAdapter</code> interfaces for unsupported services/storage.</li>
                                <li><strong>Goal:</strong> Tailor the agent's capabilities and integrations while leveraging the core framework's orchestration.</li>
                            </ul>
                        </li>
                        <li>
                            <strong>Level 3: Advanced Usage (Implementing Custom Agent Patterns)</strong>
                             <ul class="list-circle list-inside ml-6 space-y-1 text-sm mt-1">
                                <li><strong>Focus:</strong> Core Logic Customization.</li>
                                <li><strong>Activities:</strong> Includes Intermediate Usage activities, PLUS implementing a custom <code>IAgentCore</code> interface. This involves defining a completely new reasoning loop or modifying an existing one significantly. Requires a deep understanding of how all internal ART components interact.</li>
                                <li><strong>Goal:</strong> Gain maximum control over the agent's behavior and reasoning process.</li>
                            </ul>
                        </li>
                    </ul>
                </div>
            </section>

            <section id="architecture" class="mb-16 p-6 bg-white rounded-lg shadow-lg fade-in-section">
                <h2 class="text-2xl md:text-3xl font-semibold mb-6 border-b pb-3 text-sky-700">2. Understanding ART's Architecture: The 3 Nodes</h2>
                <p class="mb-5 text-gray-700 leading-relaxed">
                    Imagine ART as having three main layers or "nodes" that work together:
                </p>
                <div class="mermaid mb-8">
flowchart LR
    A["Node 1: Developer Interface\n(Your Code & Config)"] --> B["Node 2: ART Core Orchestration\n(The Framework's Brain)"]
    B --> C["Node 3: External Dependencies & Interactions\n(The Outside World)"]
    C --> B
                </div>

                <div class="space-y-8">
                    <div>
                        <h3 class="text-xl font-medium mb-3 text-gray-800">Node 1: Developer Interface (Your Code & Config)</h3>
                        <ul class="list-disc list-inside space-y-2 text-gray-700 leading-relaxed">
                            <li><strong>What it is:</strong> This is where you, the developer, interact with ART. You write the code to set up, configure, and control the agent.</li>
                            <li><strong>What you do here:</strong> Choose which AI model to use (like GPT-4 or Gemini), decide how the agent should remember things (in memory or browser storage), select which tools it can use, pick the agent's thinking style (its "pattern"), and tell the agent when to start processing a user's request.</li>
                            <li><strong>Key ART parts involved:</strong> <code>createArtInstance</code> (the function to start ART), configuration objects, <code>ArtInstance</code> (the main object you interact with), <code>art.process()</code> (the command to make the agent think).</li>
                        </ul>
                    </div>
                    <div>
                        <h3 class="text-xl font-medium mb-3 text-gray-800">Node 2: ART Core Orchestration (The Framework's Brain)</h3>
                         <ul class="list-disc list-inside space-y-2 text-gray-700 leading-relaxed">
                            <li><strong>What it is:</strong> This is the internal engine of ART, set up based on your configuration in Node 1. It manages the entire process of understanding a request, using tools, and generating a response.</li>
                            <li><strong>What it does:</strong> Follows the chosen agent pattern (like "Plan-Execute-Synthesize" or "ReAct"), manages conversation history, keeps track of the agent's state, prepares instructions (prompts) for the AI model using blueprints and context, understands the AI's responses (including streaming tokens), coordinates tool usage, logs important events (observations), and broadcasts real-time updates to the UI.</li>
                            <li><strong>Key ART parts involved:</strong> The specific Agent Core implementation (`PESAgent`, `ReActAgent`), Managers (`StateManager`, `ConversationManager`, `ObservationManager`), Systems (`ToolSystem`, `UISystem`), Reasoning Components (`ReasoningEngine`, `PromptManager`, `OutputParser`). You usually don't interact with these directly after setup unless you're doing advanced customization or consuming UI sockets.</li>
                        </ul>
                    </div>
                    <div>
                        <h3 class="text-xl font-medium mb-3 text-gray-800">Node 3: External Dependencies & Interactions (The Outside World)</h3>
                         <ul class="list-disc list-inside space-y-2 text-gray-700 leading-relaxed">
                            <li><strong>What it is:</strong> This node represents where the ART engine connects to resources outside its core orchestration logic. These are the pluggable pieces configured in Node 1.</li>
                            <li><strong>What it does:</strong> Makes the actual calls to the LLM provider APIs (handling both traditional request/response and streaming), executes tool logic (which might involve calling other web services or using browser features), and persists/retrieves data from the chosen storage mechanism.</li>
                            <li><strong>Key Elements (Interfaces & Implementations):** Adapters (`ProviderAdapter` for LLMs, `StorageAdapter` for memory/storage), Tool Implementations (`IToolExecutor`).</li>
                        </ul>
                    </div>
                </div>

                 <div id="streaming-architecture" class="mt-10">
                    <h3 class="text-xl font-medium mb-4 text-gray-800">2.1. Core Concept: Real-time Streaming Architecture</h3>
                    <p class="mb-4 text-gray-700 leading-relaxed">To provide a more responsive and interactive user experience, ART incorporates a real-time streaming architecture for handling LLM responses. Instead of waiting for the entire response, the UI can receive and display tokens as soon as the LLM generates them.</p>

                    <h4 class="text-lg font-medium mt-6 mb-3 text-gray-800">Key Components:</h4>
                    <ul class="list-disc list-inside space-y-3 mb-6 text-gray-700 leading-relaxed">
                        <li>
                            <strong><code>ReasoningEngine.call</code> returning <code>AsyncIterable&lt;StreamEvent&gt;</code>:</strong>
                            <ul class="list-circle list-inside ml-6 space-y-1 text-sm mt-1">
                                <li><strong>Non-Developer Explanation:</strong> Instead of the agent's "brain" waiting for the LLM's complete answer, it now gets a "conveyor belt" (<code>AsyncIterable</code>). Pieces of the answer (<code>StreamEvent</code> objects) arrive on the belt one by one as the LLM thinks and writes.</li>
                                <li><strong>Developer Notes:</strong> The core <code>ReasoningEngine</code> interface's <code>call</code> method now returns a <code>Promise</code> resolving to an <code>AsyncIterable</code>. This iterable yields <code>StreamEvent</code> objects, allowing the consuming code (typically the <code>AgentCore</code>) to process tokens, metadata, errors, and end signals asynchronously as they arrive from the <code>ProviderAdapter</code>.</li>
                            </ul>
                        </li>
                         <li>
                            <strong><code>StreamEvent</code> Interface:</strong>
                            <ul class="list-circle list-inside ml-6 space-y-1 text-sm mt-1">
                                <li><strong>Non-Developer Explanation:</strong> Each item on the conveyor belt has a label (<code>StreamEvent</code>) saying what it is: a piece of text (<code>TOKEN</code>), statistics (<code>METADATA</code>), an error (<code>ERROR</code>), or the end signal (<code>END</code>). Text tokens also have a sub-label (<code>tokenType</code>) indicating if it's part of the LLM's internal "thinking" process or the final "response".</li>
                                <li><strong>Developer Notes:</strong> This interface (detailed below) standardizes the data flowing from the LLM stream. The <code>type</code> field is crucial for routing, and the <code>tokenType</code> field enables differentiating intermediate reasoning steps from the final output meant for the user. Adapters are responsible for correctly populating these fields based on provider-specific stream formats and the <code>callContext</code> option.</li>
                            </ul>
                        </li>
                         <li>
                            <strong><code>LLMStreamSocket</code> (<code>UISystem</code>):</strong>
                            <ul class="list-circle list-inside ml-6 space-y-1 text-sm mt-1">
                                <li><strong>Non-Developer Explanation:</strong> ART uses an announcement system (sockets) for different parts to communicate, especially with the UI. A new, dedicated channel (<code>LLMStreamSocket</code>) was added specifically for broadcasting the live stream events (tokens, metadata, errors, end signals) from the LLM conveyor belt to any UI components listening.</li>
                                <li><strong>Developer Notes:</strong> Accessed via <code>artInstance.uiSystem.getLLMStreamSocket()</code>. The <code>AgentCore</code> consumes the <code>AsyncIterable</code> from the <code>ReasoningEngine</code> and pushes each <code>StreamEvent</code> to this socket. UI components subscribe to this socket to receive real-time updates, decoupling the UI from the stream source and providing a consistent subscription pattern (<code>socket.subscribe(...)</code>).</li>
                            </ul>
                        </li>
                    </ul>

                     <h4 class="text-lg font-medium mt-6 mb-3 text-gray-800">Flow Overview:</h4>
                    <ol class="list-decimal list-inside space-y-3 mb-6 text-gray-700 leading-relaxed">
                        <li><strong>UI/App:** Calls <code>artInstance.process(props)</code>.</li>
                        <li><strong>Agent Core (<code>PESAgent</code>, etc.):** Calls <code>reasoningEngine.call(prompt, { stream: true, callContext: '...' })</code>.</li>
                        <li><strong>Reasoning Engine (Adapter - e.g., <code>OpenAIAdapter</code>):** Makes a streaming request to the LLM provider API.</li>
                        <li><strong>Adapter:** Receives stream chunks, parses them, determines <code>tokenType</code>, and <code>yield</code>s <code>StreamEvent</code> objects via the <code>AsyncIterable</code>.</li>
                        <li><strong>Agent Core:** Consumes the <code>AsyncIterable</code> using <code>for await...of</code>.</li>
                        <li><strong>Agent Core:** For each <code>StreamEvent</code>:
                            <ul class="list-disc list-inside ml-6 space-y-1 text-sm mt-1">
                                <li>Pushes the event to <code>uiSystem.getLLMStreamSocket().notify(event)</code>.</li>
                                <li>If it's a final response <code>TOKEN</code>, appends it to an internal buffer.</li>
                                <li>If it's <code>METADATA</code>, <code>ERROR</code>, or <code>END</code>, records it via <code>ObservationManager</code>.</li>
                                <li>Aggregates <code>METADATA</code>.</li>
                            </ul>
                        </li>
                        <li><strong>UI:** Receives <code>StreamEvent</code>s via its <code>LLMStreamSocket</code> subscription and updates the display in real-time.</li>
                        <li><strong>Agent Core (After Stream Ends):** Constructs the final <code>ConversationMessage</code> from the buffer, saves it via <code>ConversationManager</code>, and returns the <code>AgentFinalResponse</code> containing the final message and aggregated <code>ExecutionMetadata</code> (including <code>llmMetadata</code>).</li>
                        <li><strong>UI:** (Optional but recommended) Receives the final <code>ConversationMessage</code> via <code>ConversationSocket</code> subscription and replaces the temporary streamed message with the final, persisted one to ensure consistency.</li>
                    </ol>

                     <h4 class="text-lg font-medium mt-6 mb-3 text-gray-800">Thinking vs. Response Tokens (<code>tokenType</code>):</h4>
                     <ul class="list-disc list-inside space-y-2 mb-6 text-gray-700 leading-relaxed">
                         <li>The <code>StreamEvent.tokenType</code> field allows distinguishing between tokens generated during intermediate reasoning steps (e.g., <code>AGENT_THOUGHT_LLM_RESPONSE</code>) and tokens forming the final user-facing answer (e.g., <code>FINAL_SYNTHESIS_LLM_RESPONSE</code>).</li>
                         <li>Adapters determine the <code>LLM_THINKING</code> vs <code>LLM_RESPONSE</code> part based on provider-specific markers (if available).</li>
                         <li>The Agent Core provides the <code>AGENT_THOUGHT</code> vs <code>FINAL_SYNTHESIS</code> context via <code>CallOptions.callContext</code>.</li>
                         <li>The UI can use <code>tokenType</code> to visually differentiate these tokens (e.g., showing thinking steps faded or in a separate area).</li>
                     </ul>

                     <h4 class="text-lg font-medium mt-6 mb-3 text-gray-800">Metadata Delivery (<code>LLMMetadata</code>):</h4>
                     <ul class="list-disc list-inside space-y-2 mb-6 text-gray-700 leading-relaxed">
                          <li>Detailed LLM statistics (token counts, timing) are packaged into <code>LLMMetadata</code> objects.</li>
                          <li>Adapters yield these as <code>METADATA</code> <code>StreamEvent</code>s (either during the stream if the provider supports it, or after the stream ends based on the final response/usage info).</li>
                          <li>These events are broadcast via <code>LLMStreamSocket</code> for potential real-time display.</li>
                          <li>They are also logged as discrete observations (`LLM_STREAM_METADATA`) via `ObservationManager`.</li>
                          <li>Finally, the metadata from all relevant LLM calls within an execution cycle is aggregated and included in the `AgentFinalResponse.metadata.llmMetadata` field.</li>
                     </ul>

                    <div id="streaming-interfaces">
                        <h4 class="text-lg font-medium mt-6 mb-4 text-gray-800">Streaming-Related Interfaces and Types</h4>
                        <p class="mb-4 text-gray-700 leading-relaxed">To support real-time streaming and detailed metadata, the following interfaces and types were introduced or modified:</p>
                        <div class="space-y-4">
                            <div>
                                <p><strong><code>StreamEvent</code> Interface:**</p>
<pre><code class="language-typescript">// Non-Developer: Defines the "label" attached to each piece coming from the LLM stream, saying what it is.
// Developer Notes: Represents a single event from the LLM stream iterator. Should be defined in src/types/index.ts.
interface StreamEvent {
  type: 'TOKEN' | 'METADATA' | 'ERROR' | 'END'; // Kind of event
  data: any; // The actual content (string for TOKEN, LLMMetadata for METADATA, Error for ERROR)
  tokenType?: 'LLM_THINKING' | 'LLM_RESPONSE' | 'AGENT_THOUGHT_LLM_THINKING' | 'AGENT_THOUGHT_LLM_RESPONSE' | 'FINAL_SYNTHESIS_LLM_THINKING' | 'FINAL_SYNTHESIS_LLM_RESPONSE'; // Specific kind of token
  threadId: string; // Links event to the conversation thread. Essential for routing and context.
  traceId: string; // Links event to the specific agent.process() call. Essential for debugging and correlation.
  sessionId?: string; // Links event to a specific UI tab/window. Recommended for multi-session scenarios.
}
</code></pre>
                            </div>
                             <div>
                                <p><strong><code>LLMMetadata</code> Interface:**</p>
<pre><code class="language-typescript">// Non-Developer: Defines what kind of statistics we want to capture about the LLM's work (like token counts).
// Developer Notes: Structure for holding parsed metadata from LLM responses/streams. Should be defined in src/types/index.ts.
interface LLMMetadata {
  inputTokens?: number;
  outputTokens?: number;
  thinkingTokens?: number; // If available from provider
  timeToFirstTokenMs?: number;
  totalGenerationTimeMs?: number;
  stopReason?: string; // e.g., 'stop_sequence', 'max_tokens', 'end_turn'
  providerRawUsage?: any; // Optional raw usage data from provider for extensibility
  // Developer Note: Include traceId if this object might be stored or passed independently.
  traceId?: string;
}
</code></pre>
                            </div>
                             <div>
                                <p><strong><code>CallOptions</code> Interface:**</p>
<pre><code class="language-typescript">// Non-Developer: Adds switches to turn streaming on/off and tells the LLM call whether it's for an internal "thought" or the final answer.
// Developer Notes: Extends options passed to ReasoningEngine.call. Defined in src/types/index.ts.
interface CallOptions {
  // ... existing fields (threadId, traceId, userId, sessionId, onThought, etc.)
  stream?: boolean; // Request streaming response. Adapters MUST check this.
  callContext?: 'AGENT_THOUGHT' | 'FINAL_SYNTHESIS' | string; // Provides context for tokenType determination by the adapter. Agent Core MUST provide this.
}
</code></pre>
                            </div>
                            <div>
                                <p><strong><code>ExecutionMetadata</code> Interface:**</p>
<pre><code class="language-typescript">// Non-Developer: Adds a place to store the final LLM statistics associated with the agent's overall response.
// Developer Notes: Extends the metadata returned by agent.process(). Defined in src/types/index.ts.
interface ExecutionMetadata {
  // ... existing fields
  llmMetadata?: LLMMetadata; // Aggregated metadata from LLM calls in the execution. Agent Core MUST populate this.
}
</code></pre>
                            </div>
                             <div>
                                <p><strong><code>ObservationType</code> Enum:**</p>
<pre><code class="language-typescript">// Non-Developer: Adds new categories to the agent's logbook specifically for important streaming events.
// Developer Notes: New enum values for ObservationManager.record(). Defined in src/types/index.ts.
enum ObservationType {
  // ... existing types
  LLM_STREAM_START = 'LLM_STREAM_START', // Optional: Logged by Agent Core when iterator consumption begins.
  LLM_STREAM_METADATA = 'LLM_STREAM_METADATA', // Logged by Agent Core upon receiving METADATA event. Content should be LLMMetadata.
  LLM_STREAM_END = 'LLM_STREAM_END', // Logged by Agent Core upon receiving END event.
  LLM_STREAM_ERROR = 'LLM_STREAM_ERROR', // Logged by Agent Core upon receiving ERROR event. Content should be Error object or message.
}
</code></pre>
                            </div>
                        </div>
                    </div>
                </div>

                 <div id="prompt-architecture" class="mt-10">
                     <h3 class="text-xl font-medium mb-4 text-gray-800">2.2. Core Concept: Prompt Management Architecture</h3>
                    <p class="mb-4 text-gray-700 leading-relaxed">ART's prompt management system has been refactored to provide greater flexibility, decouple agent patterns from the core framework, and give developers more control over prompt content.</p>

                    <h4 class="text-lg font-medium mt-6 mb-3 text-gray-800">Rationale:</h4>
                     <ul class="list-disc list-inside space-y-2 mb-6 text-gray-700 leading-relaxed">
                        <li><strong>Support Custom Agent Patterns:</strong> Decouple core prompt assembly from specific agent patterns (like PES) to allow developers to create and integrate arbitrary agent flows without modifying the ART framework.</li>
                        <li><strong>Developer Control over Content:</strong> Enable developers to define and control key prompt content elements, such as system prompts (via `ThreadConfig` or dynamic context), tool presentation (via descriptions/schemas in their tool definitions, interpreted by agent blueprints), and custom data relevant to their specific agent logic (via `PromptContext`).</li>
                        <li><strong>Provider Agnosticism:</strong> Achieve true decoupling between agent logic/prompt structure and provider-specific API requirements by introducing a standardized intermediate format (`ArtStandardPrompt`).</li>
                        <li><strong>Clear Responsibilities:</strong> Establish clear boundaries: Agent Patterns define structure (blueprints) and provide context; `PromptManager` assembles into the standard format; Adapters translate the standard format to the provider API.</li>
                        <li><strong>Alignment:</strong> Align with established framework best practices that favor configuration, standardization, and template-driven customization over hardcoded, monolithic logic.</li>
                    </ul>

                    <h4 class="text-lg font-medium mt-6 mb-3 text-gray-800">Proposed Architecture Overview:</h4>
                    <div class="mermaid mb-8">
graph TD
    subgraph AgentLogic["Agent Pattern Logic"]
        A[Agent Step Logic] -->|Decides prompt needed| B(Retrieves Blueprint/Template)
        A -->|Gathers data| C{PromptContext}
        C -->|Includes| Q(Query)
        C -->|Includes| SP(System Prompt from Config)
        C -->|Includes| H(History from Storage)
        C -->|Includes| TS(Tool Schemas from Registry)
        C -->|Includes| TR(Tool Results if applicable)
        C -->|Includes| CD(Custom Data)
        A -->|Calls| PM(PromptManager.assemblePrompt)
        B -->|blueprint| PM
        C -->|context| PM
    end

    subgraph ARTCore["ART Core"]
        PM -->|Uses Mustache.js| ASP(ArtStandardPrompt)
        PM -->|Returns| A
        ASP -->|Passed to| RE(ReasoningEngine)
        RE -->|Forwards| PA(ProviderAdapter)
        PA -->|Translates| APIFormat(Provider API Payload)
    end

    PA -->|Sends| ExtAPI[External LLM API]
                    </div>

                    <h4 class="text-lg font-medium mt-6 mb-3 text-gray-800">Key Concepts:</h4>
                    <ul class="list-disc list-inside space-y-3 mb-6 text-gray-700 leading-relaxed">
                        <li><strong><code>ArtStandardPrompt</code> Format:** A canonical, provider-agnostic message array format (e.g., <code>[{ role: 'system' | 'user' | 'assistant' | 'tool_request' | 'tool_result', content: string | object }]</code>). This replaces the previous <code>FormattedPrompt</code> type alias.</li>
                        <li><strong>Agent Pattern Responsibility:** Defines prompt "blueprints" (templates targeting <code>ArtStandardPrompt</code>) and gathers all necessary <code>PromptContext</code> data (query, system prompt string, history, tool schemas, tool results, custom data). Blueprints are considered intrinsic to the agent pattern's logic and are not selected or modified via <code>ThreadConfig</code>. Built-in agents define default system prompt strings, which can be overridden by <code>ThreadConfig</code>.</li>
                        <li><strong>Core <code>PromptManager</code>:** A stateless assembler with an <code>assemblePrompt(blueprint: string | object, context: PromptContext): Promise&lt;ArtStandardPrompt&gt;</code> method. It uses an internal templating engine (like Mustache.js) to produce the <code>ArtStandardPrompt</code> object based on the agent-provided blueprint and context.</li>
                        <li><strong>Core <code>ReasoningEngine</code>:** Receives the <code>ArtStandardPrompt</code> from <code>PromptManager</code> and passes it directly to the selected <code>ProviderAdapter</code>.</li>
                        <li><strong>Provider Adapters:** Each adapter is responsible for **translating** the received <code>ArtStandardPrompt</code> object into the specific API format required by the target LLM provider (e.g., mapping roles, structuring content, handling tool calls/results according to that API's specification).</li>
                    </ul>
                    <p class="mb-4 text-gray-700 leading-relaxed">This architecture ensures that developers can customize prompt behavior significantly by controlling the inputs (blueprints, configuration, context) without needing to modify the core <code>PromptManager</code> assembly logic or the provider-specific <code>Adapter</code> translation logic.</p>

                     <h4 class="text-lg font-medium mt-6 mb-3 text-gray-800">Example Data Flow (`PromptContext` -> `ArtStandardPrompt`):</h4>
                    <ol class="list-decimal list-inside space-y-3 text-gray-700 leading-relaxed">
                        <li><em>Agent Logic (e.g., PES Planning):</em> Gathers <code>query</code>, <code>history</code>, <code>availableTools</code>, <code>systemPrompt</code> (from <code>ThreadConfig</code> or agent default) into a <code>PromptContext</code> object. Retrieves its specific <code>planningBlueprint</code> (Mustache template string).</li>
                        <li><em>Agent Logic Calls:</em> <code>promptManager.assemblePrompt(planningBlueprint, planningContext)</code>.</li>
                        <li><em><code>PromptManager</code> Execution:</em> Uses Mustache.js to render the <code>planningBlueprint</code> using data from <code>planningContext</code>. This involves iterating over history, tools, etc., as defined in the blueprint. The output is a JSON string representing the <code>ArtStandardPrompt</code> array structure defined in the blueprint. <code>PromptManager</code> parses this JSON string into the final <code>ArtStandardPrompt</code> object.</li>
                        <li><em><code>PromptManager</code> Output:</em> Returns the assembled <code>ArtStandardPrompt</code> (an array of <code>ArtStandardMessage</code> objects), e.g., <code>[{ role: 'system', content: '...', ... }, { role: 'user', content: '...', ... }, ...]</code>.</li>
                        <li><em>Agent Logic:</em> Passes the resulting <code>ArtStandardPrompt</code> to <code>ReasoningEngine</code>.</li>
                    </ol>
                </div>

                 <div id="state-management-architecture" class="mt-10">
                    <h3 class="text-xl font-medium mb-4 text-gray-800">2.3. Core Concept: State Management Architecture</h3>
                    <p class="mb-4 text-gray-700 leading-relaxed">ART's state management system is responsible for keeping track of important information beyond just the conversation history. This includes thread-specific configurations and the agent's internal state, which helps maintain context and manage complex tasks across turns.</p>

                    <h4 class="text-lg font-medium mt-6 mb-3 text-gray-800">Components of State Management:</h4>
                    <p class="mb-4 text-gray-700 leading-relaxed">ART's state management involves a few key components working together:</p>
                    <ul class="list-disc list-inside space-y-3 mb-6 text-gray-700 leading-relaxed">
                        <li>
                            <strong>The State Manager (<code>StateManager</code>):</strong> This is the primary interface within the ART core that agent implementations interact with. Its role is to handle the *logic* of state management. It knows *what* state is needed for a particular conversation or task and *when* to load or save it. It provides methods like <code>loadThreadContext(threadId)</code> to retrieve configuration and state for a specific thread and <code>saveStateIfModified(threadId)</code> to persist changes.
                        </li>
                        <li>
                            <strong>The State Repository (<code>IStateRepository</code>):</strong> This component acts as a librarian between the <code>StateManager</code> and the actual storage mechanism. It defines *how* to find and organize the specific state information the <code>StateManager</code> asks for. It has methods like <code>getState(threadId)</code> or <code>saveState(threadId, state)</code>. However, the librarian doesn't handle the physical storage details; it delegates these operations to the <code>StorageAdapter</code>.
                        </li>
                        <li>
                            <strong>The Storage Adapter (<code>StorageAdapter</code>):</strong> This is the layer responsible for the *actual saving and loading* of state data to and from a physical storage medium. It knows *how* to interact with the specific storage backend being used (e.g., IndexedDB, a database, a custom API). The <code>IStateRepository</code> tells the <code>StorageAdapter</code> *what* data to store or retrieve (e.g., "save this piece of state data with this ID in the 'state' collection"), and the <code>StorageAdapter</code> handles the *how* (e.g., "write this JSON object to the 'state' table in Supabase").
                        </li>
                    </ul>

                    <h4 class="text-lg font-medium mt-6 mb-3 text-gray-800">Extensibility through the Storage Adapter:</h4>
                    <p class="mb-4 text-gray-700 leading-relaxed">A key aspect of ART's state management flexibility for developers using the npm package is the ability to provide a custom <code>StorageAdapter</code>. While the <code>StateManager</code> and <code>IStateRepository</code> interfaces and their default implementations are part of the core framework, the <code>IStateRepository</code> is designed to work with *any* component that implements the <code>StorageAdapter</code> interface.</p>
                    <p class="mb-4 text-gray-700 leading-relaxed">This means you, as the application developer, have full control over where and how the state is persisted by implementing your own <code>StorageAdapter</code> that connects to your desired storage backend.</p>

                    <h4 class="text-lg font-medium mt-6 mb-3 text-gray-800">How Custom Storage Adapters Enable Flexible State Management:</h4>
                    <p class="mb-4 text-gray-700 leading-relaxed">By creating a custom <code>StorageAdapter</code>, you can integrate ART's state management with virtually any storage solution. This allows for:</p>
                     <ul class="list-disc list-inside space-y-2 mb-6 text-gray-700 leading-relaxed">
                        <li><strong>Persistent State:</strong> Using databases (like Supabase, PostgreSQL, etc.) or other persistent storage mechanisms to ensure conversation settings and agent state are saved across application sessions.</li>
                        <li><strong>Caching:</strong> Implementing caching layers (like using <code>InMemoryStorageAdapter</code> as a cache in front of a persistent backend) to improve performance by quickly accessing frequently used state data.</li>
                        <li><strong>Custom Logic:</strong> Adding any custom logic needed for your specific storage requirements within your adapter, such as data transformation, encryption, or integration with specific APIs.</li>
                    </ul>
                    <p class="mb-4 text-gray-700 leading-relaxed">When you initialize ART using <code>createArtInstance</code>, you provide your custom <code>StorageAdapter</code> instance in the configuration. The ART factory will then ensure that the core state management components use your adapter for all state persistence and loading operations.</p>

                     <h4 class="text-lg font-medium mt-6 mb-3 text-gray-800">How to Create and Use Your Custom Storage Adapter:</h4>
                     <ol class="list-decimal list-inside space-y-3 text-gray-700 leading-relaxed">
                         <li><strong>Create Your Adapter File(s):</strong> Create new file(s) in your application's project, perhaps in a folder like <code>storage-adapters</code> or <code>data</code>. For example, <code>supabase-adapter.ts</code> and <code>caching-adapter.ts</code>.</li>
                         <li><strong>Import Necessary ART Components:** Inside your adapter files, import the required types and interfaces from <code>art-framework</code>. Key imports for a storage adapter include:
                             <ul class="list-disc list-inside ml-6 space-y-1 text-sm mt-1">
                                 <li><code>StorageAdapter</code>: The interface your adapter class must implement.</li>
                                 <li><code>FilterOptions</code>: The type defining options for querying data.</li>
                                 <li>You might also need types for the data you are storing (e.g., <code>ConversationMessage</code>, <code>AgentState</code>, <code>Observation</code>) if you want type safety within your adapter, although the <code>StorageAdapter</code> interface uses generic types (<code>&lt;T&gt;</code>).</li>
                             </ul>
                         </li>
                         <li><strong>Implement Your Adapter Class(es):** Create the class(es) that implement the <code>StorageAdapter</code> interface.
                             <ul class="list-disc list-inside ml-6 space-y-1 text-sm mt-1">
                                 <li>For a backend-specific adapter (like a <code>SupabaseAdapter</code>), implement the <code>get</code>, <code>set</code>, <code>delete</code>, and <code>query</code> methods using the client library for your chosen backend to interact with your database or storage service. Implement <code>init</code> if you need to establish the connection asynchronously.</li>
                                 <li>For a caching adapter (like a <code>CachingStorageAdapter</code>), implement the <code>get</code>, <code>set</code>, <code>delete</code>, and <code>query</code> methods by coordinating calls to the injected primary and cache adapters (e.g., check cache on <code>get</code>, write to both on <code>set</code>).</li>
                             </ul>
                         </li>
                          <li><strong>Import and Pass to <code>createArtInstance</code>:** In the file where you initialize ART, import your custom adapter class(es). In the configuration object passed to <code>createArtInstance</code>, create instances of your custom adapters and pass the top-level adapter (e.g., your <code>CachingStorageAdapter</code>) in the <code>storage</code> part:</li>
                     </ol>
<pre><code class="language-typescript">import { createArtInstance, InMemoryStorageAdapter } from 'art-framework';
import { SupabaseAdapter } from './storage-adapters/supabase-adapter'; // Import your Supabase adapter
import { CachingStorageAdapter } from './storage-adapters/caching-adapter'; // Import your Caching adapter

// Assuming SupabaseAdapter constructor takes options like URL and Key
const supabaseAdapter = new SupabaseAdapter({ url: 'YOUR_SUPABASE_URL', apiKey: 'YOUR_SUPABASE_API_KEY' });
const inMemoryAdapter = new InMemoryStorageAdapter(); // Use the built-in in-memory adapter

// Instantiate your caching adapter with the primary and cache adapters
const cachingAdapter = new CachingStorageAdapter(supabaseAdapter, inMemoryAdapter);

const config = {
  storage: cachingAdapter, // Pass the caching adapter instance
  reasoning: { /* ... */ },
  // ... other config (agentCore, tools)
};

const art = await createArtInstance(config);
</code></pre>
                    <p class="mt-4 text-gray-700 leading-relaxed">By following these steps, you can seamlessly integrate your custom storage solution(s) with ART's state management system without modifying the framework's core code, providing the flexibility to handle various persistence and caching requirements.</p>
                </div>

            </section>

            <section id="scenario-1" class="mb-16 p-6 bg-white rounded-lg shadow-lg fade-in-section">
                <h2 class="text-2xl md:text-3xl font-semibold mb-6 border-b pb-3 text-sky-700">3. Scenario 1: Building a Feature-Rich React Chatbot (Simple Usage)</h2>
                <p class="mb-5 text-gray-700 leading-relaxed">
                    Let's build a chatbot component for a React website using only ART's built-in features. We'll aim to showcase several core ART capabilities, including the new real-time streaming.
                </p>
                <p class="mb-8 text-gray-700 leading-relaxed"><strong>Goal:</strong> A chat interface where users can talk to an AI powered by OpenAI, with conversation history saved in the browser, real-time token streaming, and status feedback using ART's observation system.</p>

                <div id="scenario-1-imports" class="mb-10">
                    <h3 class="text-xl font-medium mb-4 text-gray-800">3.1. Necessary Imports & Explanations</h3>
<pre><code class="language-typescript">// src/components/ArtChatbot.tsx
import React, { useState, useEffect, useRef, useCallback } from 'react';

// --- ART Core Imports ---
import {
  // The main factory function to initialize ART
  createArtInstance,
  // Type definition for the initialized ART object
  ArtInstance,
  // Type for the properties needed to call the agent's process method
  AgentProps,
  // Type for the final response object from the agent
  AgentFinalResponse,
  // Type representing a single message in the conversation
  ConversationMessage,
  // Enum defining message roles (USER, ASSISTANT, SYSTEM, TOOL)
  MessageRole,
  // Type representing an internal event/observation within ART
  Observation,
  // Enum defining different types of observations (PROCESS_START, LLM_REQUEST, etc.)
  ObservationType,
  // The default Plan-Execute-Synthesize agent pattern implementation
  PESAgent,
  // Interfaces for core components (needed for type hints, less for direct use here)
  StorageAdapter, ProviderAdapter, ReasoningEngine, IToolExecutor, IAgentCore,
  StateManager, ConversationManager, ToolRegistry, ObservationManager, UISystem,
  // New/Updated types for streaming and metadata
  StreamEvent, LLMMetadata, ExecutionMetadata
} from 'art-framework'; // Assuming 'art-framework' is the installed package name

// --- ART Adapter Imports (Developer Choices) ---
import {
  // Storage adapter that uses the browser's IndexedDB for persistence
  IndexedDBStorageAdapter,
  // Storage adapter that uses temporary browser memory (data lost on refresh)
  // InMemoryStorageAdapter, // Alternative, uncomment if preferred
} from 'art-framework'; // Adapters are usually exported from the main package too

import {
  // Reasoning provider adapter for OpenAI models (GPT-3.5, GPT-4, etc.)
  OpenAIAdapter,
  // Reasoning provider adapter for Google Gemini models
  // GeminiAdapter, // Alternative, uncomment if preferred
  // Reasoning provider adapter for Anthropic Claude models
  // AnthropicAdapter,
} from 'art-framework';

// --- ART Built-in Tool Imports (Optional) ---
import {
  // A simple tool that can evaluate mathematical expressions
  CalculatorTool
} from 'art-framework';
</code></pre>
                    <h4 class="text-lg font-medium mt-6 mb-4 text-gray-800">Explanation of Imports:</h4>
                    <div class="space-y-4 text-gray-700 leading-relaxed">
                        <div>
                            <p><strong><code>createArtInstance</code></strong></p>
                            <p class="mb-2">This is the main function you use to start the ART framework. Think of it as the "ignition key" â€“ you give it instructions (configuration), and it builds and starts the ART engine for you.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                An asynchronous factory function (`async function createArtInstance(config: AgentFactoryConfig): Promise&lt;ArtInstance&gt;`). Takes `config` (object conforming to the `AgentFactoryConfig` interface: `{ storage: StorageAdapter | StorageConfig, reasoning: ReasoningConfig, tools?: IToolExecutor[], agentCore?: new (deps: any) => IAgentCore, logger?: { level?: LogLevel } }`). Uses `AgentFactory` internally to instantiate and inject dependencies for all core components (Managers, Systems, Repositories, Adapters). Returns a `Promise` resolving to the fully initialized `ArtInstance` object. Typically called once at application setup. Note: `storage` can now accept a pre-instantiated `StorageAdapter`.
                            </details>
                        </div>
                         <div>
                            <p><strong><code>ArtInstance</code></strong></p>
                            <p class="mb-2">This describes the main control panel you get after starting ART. It's the object that lets you interact with the initialized framework, primarily by telling it to process user messages.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                A TypeScript interface defining the public API returned by <code>createArtInstance</code>. Key properties:
                                <ul class="list-disc list-inside ml-4">
                                    <li><code>process(props: AgentProps): Promise&lt;AgentFinalResponse&gt;</code>: The core method to run the agent's reasoning cycle.</li>
                                    <li><code>conversationManager: ConversationManager</code>: Access methods like <code>getMessages</code>, <code>addMessages</code>.</li>
                                    <li><code>stateManager: StateManager</code>: Access methods like <code>loadThreadContext</code>, <code>setThreadConfig</code>, <code>getAgentState</code>, <code>setAgentState</code>, <code>isToolEnabled</code>.</li>
                                    <li><code>toolRegistry: ToolRegistry</code>: Access methods like <code>registerTool</code>, <code>getToolExecutor</code>, <code>getAvailableTools</code>.</li>
                                    <li><code>observationManager: ObservationManager</code>: Access methods like <code>record</code>, <code>getObservations</code>.</li>
                                    <li><code>uiSystem: UISystem</code>: Access methods like <code>getObservationSocket</code>, <code>getConversationSocket</code>, and <code>getLLMStreamSocket</code> (for streaming) to get subscription interfaces.</li>
                                </ul>
                            </details>
                        </div>
                         <div>
                            <p><strong><code>AgentProps</code></strong></p>
                            <p class="mb-2">Describes the information you need to give the agent each time you want it to respond (your message and which chat it belongs to).</p>
                            <details>
                                <summary>Developer Notes</summary>
                                Interface for the input object to <code>ArtInstance.process()</code>.
                                <ul class="list-disc list-inside ml-4">
                                    <li>Required: <code>query: string</code> (user input), <code>threadId: string</code> (conversation ID).</li>
                                    <li>Optional: <code>configOverrides?: Partial&lt;ThreadConfig&gt;</code> (temporarily override settings like model or enabled tools for this call), <code>executionContext?: Record&lt;string, any&gt;</code> (pass arbitrary data into the execution context, accessible by tools), <code>userId?: string</code> (associate the request with a user), <code>sessionId?: string</code> (identify UI session for stream routing).</li>
                                </ul>
                            </details>
                        </div>
                         <div>
                            <p><strong><code>AgentFinalResponse</code></strong></p>
                            <p class="mb-2">Describes the information the agent gives back after processing your request (its reply and some tracking info).</p>
                            <details>
                                <summary>Developer Notes</summary>
                                Interface for the output object from <code>ArtInstance.process()</code>.
                                <ul class="list-disc list-inside ml-4">
                                    <li>Core Properties: <code>responseText: string</code>, <code>responseId: string</code>, <code>threadId: string</code>, <code>traceId: string</code>.</li>
                                    <li>Optional/Contextual: <code>llmResponse?: any</code> (raw output from the final LLM call), <code>toolResults?: ToolResult[]</code> (results if tools were used), <code>plan?: string</code>, <code>intent?: string</code>.</li>
                                    <li><code>metadata: ExecutionMetadata</code>: Contains detailed metadata about the execution cycle, including aggregated LLM statistics (<code>llmMetadata</code>).</li>
                                </ul>
                            </details>
                        </div>
                         <div>
                            <p><strong><code>ConversationMessage</code></strong></p>
                            <p class="mb-2">How each chat bubble's information (who sent it, what it says, when) is organized.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                Interface representing a message. Properties: <code>id: string</code>, <code>role: MessageRole</code>, <code>content: string</code>, <code>timestamp: number</code>, <code>threadId: string</code>, <code>metadata?: Record&lt;string, any&gt;</code>. Used by <code>ConversationManager</code> (via <code>StorageAdapter</code>) and often directly in UI rendering logic.
                            </details>
                        </div>
                         <div>
                            <p><strong><code>MessageRole</code></strong></p>
                            <p class="mb-2">Labels to know if a message is from the User, the AI Assistant, the System (e.g., errors, info), or a Tool (results).</p>
                            <details>
                                <summary>Developer Notes</summary>
                                TypeScript enum: <code>USER</code>, <code>ASSISTANT</code>, <code>SYSTEM</code>, <code>TOOL</code>. Crucial for structuring prompts for the LLM (differentiating user input from previous AI responses) and for UI display logic.
                            </details>
                        </div>
                         <div>
                            <p><strong><code>Observation</code></strong></p>
                            <p class="mb-2">A notification about something happening inside the agent's brain while it's working (like "Thinking..." or "Using calculator...").</p>
                            <details>
                                <summary>Developer Notes</summary>
                                Interface for internal events. Properties: <code>id: string</code>, <code>timestamp: number</code>, <code>threadId: string</code>, <code>traceId?: string</code>, <code>type: ObservationType</code>, <code>content: any</code>, <code>metadata?: Record&lt;string, any&gt;</code>. Emitted by <code>ObservationManager</code> and broadcast via <code>UISystem</code>'s <code>ObservationSocket</code>. Useful for real-time UI updates (status indicators) and debugging.
                            </details>
                        </div>
                         <div>
                            <p><strong><code>ObservationType</code></strong></p>
                            <p class="mb-2">Labels for the different types of internal notifications (like "Started thinking", "Asking the AI", "Finished using a tool").</p>
                            <details>
                                <summary>Developer Notes</summary>
                                TypeScript enum listing event types (e.g., <code>PROCESS_START</code>, <code>LLM_REQUEST</code>, <code>LLM_RESPONSE</code>, <code>TOOL_START</code>, <code>TOOL_END</code>, <code>PLANNING_OUTPUT</code>, <code>SYNTHESIS_OUTPUT</code>, <code>PROCESS_END</code>, <code>REACT_STEP</code>, <code>thought</code>, <code>action</code>, <code>observation</code>). Includes new types for discrete streaming events: <code>LLM_STREAM_START</code>, <code>LLM_STREAM_METADATA</code>, <code>LLM_STREAM_END</code>, <code>LLM_STREAM_ERROR</code>. Used to categorize <code>Observation</code> events and filter subscriptions on the <code>ObservationSocket</code>.
                            </details>
                        </div>
                         <div>
                            <p><strong><code>StreamEvent</code></strong></p>
                            <p class="mb-2">Represents a single piece of information arriving from the LLM's real-time stream (like a word, statistics, or an end signal).</p>
                            <details>
                                <summary>Developer Notes</summary>
                                Interface defining the structure of events yielded by the <code>ReasoningEngine.call</code> async iterable. Properties: <code>type</code> ('TOKEN', 'METADATA', 'ERROR', 'END'), <code>data</code> (content), <code>tokenType</code> (classification like 'LLM_THINKING', 'FINAL_SYNTHESIS_LLM_RESPONSE'), <code>threadId</code>, <code>traceId</code>, <code>sessionId</code>. Consumed by the Agent Core and pushed to the <code>LLMStreamSocket</code>.
                            </details>
                        </div>
                          <div>
                            <p><strong><code>LLMMetadata</code></strong></p>
                            <p class="mb-2">A structured way to hold detailed statistics about an LLM call (token counts, timing, etc.).</p>
                            <details>
                                <summary>Developer Notes</summary>
                                Interface defining the structure for LLM statistics. Properties: <code>inputTokens?</code>, <code>outputTokens?</code>, <code>thinkingTokens?</code>, <code>timeToFirstTokenMs?</code>, <code>totalGenerationTimeMs?</code>, <code>stopReason?</code>, <code>providerRawUsage?</code>, <code>traceId?</code>. Delivered via <code>StreamEvent</code> (type 'METADATA') and aggregated into <code>ExecutionMetadata.llmMetadata</code>.
                            </details>
                        </div>
                         <div>
                            <p><strong><code>PESAgent</code></strong></p>
                            <p class="mb-2">The specific "thinking style" the agent will use by default (Plan -> Use Tools -> Answer).</p>
                            <details>
                                <summary>Developer Notes</summary>
                                Concrete class implementing <code>IAgentCore</code>. Instantiated by <code>AgentFactory</code> if specified in <code>config.agentCore</code> or if <code>agentCore</code> is omitted. Receives dependencies (<code>StateManager</code>, <code>ReasoningEngine</code>, <code>ToolSystem</code>, <code>UISystem</code>, `PromptManager`, etc.) in its constructor. Its <code>process</code> method orchestrates the Plan-Execute-Synthesize flow, using the `PromptManager` with internal blueprints, interacting with the injected dependencies, and handling the consumption and processing of the <code>AsyncIterable&lt;StreamEvent&gt;</code> from the <code>ReasoningEngine</code>.
                            </details>
                        </div>
                         <div>
                            <p><strong><code>PromptManager</code></strong></p>
                            <p class="mb-2">This component is now a stateless assembler that takes a prompt "blueprint" (template) and a <code>PromptContext</code> object to generate a standardized <code>ArtStandardPrompt</code> (an array of messages). It no longer has hardcoded prompt logic tied to specific agent patterns.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                The <code>PromptManager</code> interface now has an <code>assemblePrompt(blueprint: string | object, context: PromptContext): Promise&lt;ArtStandardPrompt&gt;</code> method. Agent implementations are responsible for providing the appropriate blueprint and gathering the necessary data into the <code>PromptContext</code>.
                            </details>
                        </div>
                        <div>
                            <p><strong><code>ReasoningEngine</code></strong></p>
                            <p class="mb-2">This component is responsible for interacting with the configured <code>ProviderAdapter</code>. Its <code>call</code> method now returns a <code>Promise&lt;AsyncIterable&lt;StreamEvent&gt;&gt;</code>, allowing for real-time streaming of LLM responses.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                The <code>ReasoningEngine</code> interface's <code>call</code> method signature has been updated. Agent implementations consume this <code>AsyncIterable</code> to process the stream events.
                            </details>
                        </div>
                         <div>
                            <p><strong><code>UISystem</code></strong></p>
                            <p class="mb-2">This system provides access to communication channels for the UI. It now includes a dedicated <code>LLMStreamSocket</code> for broadcasting real-time <code>StreamEvent</code>s from the LLM.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                The <code>UISystem</code> interface includes a <code>getLLMStreamSocket(): LLMStreamSocket</code> method. UI components subscribe to this socket to receive and display streaming tokens and other stream events.
                            </details>
                        </div>
                         <div>
                            <p><strong><code>IndexedDBStorageAdapter</code> / <code>InMemoryStorageAdapter</code></strong></p>
                            <p class="mb-2">How the agent remembers the conversation. <code>IndexedDB</code> is like saving to a file (remembers after closing), <code>InMemory</code> is like writing on a whiteboard (erased when closed).</p>
                            <details>
                                <summary>Developer Notes</summary>
                                Concrete classes implementing <code>StorageAdapter</code> (<code>get</code>, <code>set</code>, <code>delete</code>, <code>query</code>). Can be passed as a configuration object (<code>{ type: 'indexedDB', dbName: '...' }</code>) or as a pre-instantiated instance to `createArtInstance`. Used by internal Repositories. <code>IndexedDB</code> provides persistence across browser sessions; <code>InMemory</code> does not.
                            </details>
                        </div>
                         <div>
                            <p><strong><code>OpenAIAdapter</code> / <code>GeminiAdapter</code> / <code>AnthropicAdapter</code></strong></p>
                            <p class="mb-2">The specific translator the agent uses to talk to a particular AI brain (like OpenAI's GPT, Google's Gemini, or Anthropic's Claude).</p>
                            <details>
                                <summary>Developer Notes</summary>
                                Concrete classes implementing <code>ProviderAdapter</code> (extends <code>ReasoningEngine</code>). Selected via <code>config.reasoning.provider</code>. Constructor takes an options object (e.g., <code>{ apiKey: string, model?: string, baseURL?: string, defaultParams?: object }</code>) derived from <code>config.reasoning</code>. Implements the <code>call(prompt, options)</code> method, which now returns <code>Promise&lt;AsyncIterable&lt;StreamEvent&gt;&gt;</code> to support streaming. Adapters must check <code>options.stream</code> and <code>options.callContext</code> to handle streaming requests correctly, yielding <code>StreamEvent</code> objects. Used by the core <code>ReasoningEngine</code> component.
                            </details>
                        </div>
                         <div>
                            <p><strong><code>CalculatorTool</code></strong></p>
                            <p class="mb-2">A specific skill the agent can use, like a pocket calculator.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                Concrete class implementing <code>IToolExecutor</code>. Provides <code>schema</code> (<code>name</code>, <code>description</code>, <code>inputSchema</code>) and <code>execute(input, context)</code>. Instances are passed in <code>config.tools</code>. Registered with <code>ToolRegistry</code> and executed by <code>ToolSystem</code> when planned by the <code>IAgentCore</code>.
                            </details>
                        </div>
                    </div>
                </div>

                <div id="scenario-1-component" class="mb-8">
                     <h3 class="text-xl font-medium mb-4 text-gray-800">3.2. React Component Implementation</h3>
<pre><code class="language-typescript">// src/components/ArtChatbot.tsx
import React, { useState, useEffect, useRef, useCallback } from 'react';
import {
  createArtInstance, ArtInstance, AgentProps, AgentFinalResponse,
  ConversationMessage, MessageRole, Observation, ObservationType, PESAgent,
  StreamEvent // Import StreamEvent for socket subscription
} from 'art-framework';
import { IndexedDBStorageAdapter } from 'art-framework'; // Or InMemoryStorageAdapter
import { OpenAIAdapter } from 'art-framework'; // Or GeminiAdapter, etc.
import { CalculatorTool } from 'art-framework';

// Basic CSS (add this to a corresponding CSS file or use styled-components/tailwind)
/*
.chatbot-container { max-width: 600px; margin: auto; border: 1px solid #ccc; border-radius: 8px; display: flex; flex-direction: column; height: 70vh; }
.message-list { flex-grow: 1; overflow-y: auto; padding: 10px; display: flex; flex-direction: column; }
.message { margin-bottom: 10px; padding: 8px 12px; border-radius: 15px; max-width: 80%; word-wrap: break-word; }
.message.USER { background-color: #dcf8c6; align-self: flex-end; border-bottom-right-radius: 0; }
.message.ASSISTANT { background-color: #f1f0f0; align-self: flex-start; border-bottom-left-radius: 0; }
.message.SYSTEM, .message.TOOL { background-color: #e0e0e0; font-style: italic; font-size: 0.9em; align-self: center; text-align: center; }
.message.ASSISTANT.streaming { background-color: #e6f7ff; /* Lighter blue for streaming */ }
.input-area { display: flex; padding: 10px; border-top: 1px solid #ccc; }
.input-area input { flex-grow: 1; padding: 10px; border: 1px solid #ccc; border-radius: 20px; margin-right: 10px; }
.input-area button { padding: 10px 15px; border: none; background-color: #007bff; color: white; border-radius: 20px; cursor: pointer; }
.input-area button:disabled { background-color: #aaa; cursor: not-allowed; }
.status-indicator { padding: 5px 10px; font-size: 0.8em; color: #666; text-align: center; height: 20px; }
*/

// Helper to generate temporary IDs
const tempId = () => `temp-${Date.now()}-${Math.random().toString(16).slice(2)}`;

const ArtChatbot: React.FC = () => {
  const [messages, setMessages] = useState&lt;ConversationMessage[]&gt;([]);
  const [input, setInput] = useState('');
  const [isLoading, setIsLoading] = useState(false);
  const [status, setStatus] = useState&lt;string&gt;('Initializing...'); // For observation feedback
  const artInstanceRef = useRef&lt;ArtInstance | null&gt;(null);
  const messageListRef = useRef&lt;HTMLDivElement>(null); // For auto-scrolling
  const threadId = 'web-chatbot-thread-1'; // Consistent ID for this chat instance
  const streamingMessageIdRef = useRef<string | null>(null); // Track the ID of the message being streamed

  // --- Auto-scrolling ---
  useEffect(() => {
    if (messageListRef.current) {
      messageListRef.current.scrollTop = messageListRef.current.scrollHeight;
    }
  }, [messages]);

  // --- ART Initialization ---
  useEffect(() => {
    let isMounted = true; // Prevent state updates on unmounted component
    let unsubObservation: (() => void) | null = null;
    let unsubConversation: (() => void) | null = null;
    let unsubStream: (() => void) | null = null; // Subscription for LLM stream

    const initializeArt = async () => {
      if (!artInstanceRef.current) {
        try {
          setStatus('Initializing ART Engine...');
          const config = {
            storage: {
              type: 'indexedDB',
              dbName: 'artWebChatHistory'
            },
            reasoning: {
              provider: 'openai',
              apiKey: import.meta.env.VITE_OPENAI_API_KEY || 'YOUR_OPENAI_API_KEY', // Use env var
              model: 'gpt-4o'
            },
            agentCore: PESAgent, // Explicitly using the default
            tools: [new CalculatorTool()] // Include the calculator
          };

          const instance = await createArtInstance(config);
          if (!isMounted) return; // Check if component unmounted during async init

          artInstanceRef.current = instance;
          setStatus('Loading history...');
          await loadMessages(); // Load history after successful init

          // --- Subscribe to Observations (UI Feedback) ---
          setStatus('Connecting observers...');
          const observationSocket = instance.uiSystem.getObservationSocket();
          unsubObservation = observationSocket.subscribe(
            (observation: Observation) => {
              if (observation.threadId === threadId && isMounted) {
                // Simple status updates based on observations
                let newStatus = status;
                switch (observation.type) {
                  case ObservationType.LLM_REQUEST: newStatus = 'Asking AI...'; break;
                  // LLM_RESPONSE might be less relevant now with streaming end signals
                  // case ObservationType.LLM_RESPONSE: newStatus = 'AI replied.'; break;
                  case ObservationType.TOOL_START: newStatus = `Using ${observation.metadata?.toolName}...`; break;
                  case ObservationType.TOOL_END: newStatus = 'Tool finished.'; break;
                  case ObservationType.PROCESS_START: newStatus = 'Processing request...'; break;
                  case ObservationType.PROCESS_END:
                      newStatus = 'Ready.';
                      streamingMessageIdRef.current = null; // Clear streaming ID on process end
                      break;
                  // Add new stream observations if needed for status
                  case ObservationType.LLM_STREAM_START: newStatus = 'Receiving response...'; break;
                  case ObservationType.LLM_STREAM_END: newStatus = 'Response received.'; break;
                  case ObservationType.LLM_STREAM_ERROR: newStatus = 'Stream error.'; break;
                }
                 // Avoid overwriting status if currently streaming
                 if (!isLoading) {
                     setStatus(newStatus);
                 }
              }
            },
            // Subscribe to relevant types
            [
              ObservationType.PROCESS_START, ObservationType.LLM_REQUEST, /* LLM_RESPONSE */
              ObservationType.TOOL_START, ObservationType.TOOL_END, ObservationType.PROCESS_END,
              ObservationType.LLM_STREAM_START, ObservationType.LLM_STREAM_METADATA,
              ObservationType.LLM_STREAM_END, ObservationType.LLM_STREAM_ERROR
            ],
            { threadId: threadId } // Filter for this specific chat thread
          );

           // --- Subscribe to Conversation (Update UI with Final Messages) ---
           const conversationSocket = instance.uiSystem.getConversationSocket();
           unsubConversation = conversationSocket.subscribe(
             (message: ConversationMessage) => {
               if (message.threadId === threadId && isMounted) {
                 console.log("Received final message via socket:", message);
                 setMessages(prev => {
                   // Replace the temporary streaming message with the final one
                   const existingIndex = prev.findIndex(m => m.id === streamingMessageIdRef.current && m.role === MessageRole.ASSISTANT);
                   if (existingIndex > -1) {
                     const updatedMessages = [...prev];
                     updatedMessages[existingIndex] = message;
                     return updatedMessages;
                   } else if (!prev.some(m => m.id === message.id)) {
                     // Add if not already present (e.g., user message from another client)
                     return [...prev, message].sort((a, b) => a.timestamp - b.timestamp);
                   }
                   return prev;
                 });
                  streamingMessageIdRef.current = null; // Clear after receiving final message
               }
             },
             undefined, // No role filter
             { threadId: threadId }
           );


          // --- Subscribe to LLM Stream (Real-time Token Updates) ---
          const streamSocket = instance.uiSystem.getLLMStreamSocket();
          unsubStream = streamSocket.subscribe(
            (event: StreamEvent) => {
              if (event.threadId === threadId && isMounted) {
                if (event.type === 'TOKEN' && event.tokenType === 'FINAL_SYNTHESIS_LLM_RESPONSE') { // Only display final response tokens
                  setMessages(prev => {
                    const currentStreamingId = streamingMessageIdRef.current;
                    if (!currentStreamingId) {
                      // Start of a new streaming message
                      const newStreamingMessage: ConversationMessage = {
                        id: tempId(), // Use temporary ID
                        role: MessageRole.ASSISTANT,
                        content: event.data,
                        timestamp: Date.now(),
                        threadId: threadId,
                        metadata: { streaming: true } // Mark as streaming
                      };
                      streamingMessageIdRef.current = newStreamingMessage.id;
                      return [...prev, newStreamingMessage];
                    } else {
                      // Append token to existing streaming message
                      return prev.map(msg =>
                        msg.id === currentStreamingId
                          ? { ...msg, content: msg.content + event.data }
                          : msg
                      );
                    }
                  });
                } else if (event.type === 'END') {
                    // Handled by PROCESS_END observation or ConversationSocket update
                    // streamingMessageIdRef.current = null; // Clear streaming ID
                } else if (event.type === 'METADATA') {
                    console.log("Stream Metadata:", event.data);
                    // Optionally display metadata
                } else if (event.type === 'ERROR') {
                    console.error("Stream Error:", event.data);
                    setStatus('Stream Error');
                    // Optionally display error message in UI
                    setMessages(prev => [...prev, {
                         id: tempId(), role: MessageRole.SYSTEM, content: `Stream Error: ${event.data.message || event.data}`,
                         timestamp: Date.now(), threadId: threadId
                    }]);
                     streamingMessageIdRef.current = null; // Clear streaming ID on error
                }
              }
            },
            { threadId: threadId } // Filter stream events for this thread
          );


          if (isMounted) setStatus('Ready.');

        } catch (error) {
          console.error("Failed to initialize ART:", error);
          if (isMounted) setStatus(`Initialization Error: ${error instanceof Error ? error.message : 'Unknown error'}`);
        }
      }
    };

    initializeArt();

    // Cleanup function
    return () => {
      isMounted = false;
      console.log("Cleaning up ART subscriptions...");
      if (unsubObservation) unsubObservation();
      if (unsubConversation) unsubConversation();
      if (unsubStream) unsubStream(); // Unsubscribe from stream socket
    };
  }, [threadId]); // Rerun if threadId changes (it doesn't in this example)

  // --- Load Messages ---
  const loadMessages = useCallback(async () => {
    if (!artInstanceRef.current) return;
    try {
      setIsLoading(true); // Use isLoading for history loading too
      const history = await artInstanceRef.current.conversationManager.getMessages(threadId, { limit: 100 });
      setMessages(history.sort((a, b) => a.timestamp - b.timestamp)); // Sort oldest to newest
    } catch (error) {
      console.error("Failed to load messages:", error);
      setStatus('Error loading history.');
    } finally {
      setIsLoading(false);
    }
  }, [threadId]);

  // --- Handle Sending ---
  const handleSend = useCallback(async () => {
    if (!input.trim() || !artInstanceRef.current || isLoading) return;

    const userMessage: ConversationMessage = {
      id: `user-${Date.now()}`, // Consider using UUIDs for production
      role: MessageRole.USER,
      content: input,
      timestamp: Date.now(),
      threadId: threadId,
    };

    // Add user message optimistically
    setMessages(prev => [...prev, userMessage]);
    // Clear streaming ref in case a previous stream was interrupted
    streamingMessageIdRef.current = null;

    const currentInput = input; // Capture input before clearing
    setInput('');
    setIsLoading(true);
    setStatus('Sending to ART...'); // Status will be updated by observations/stream

    try {
      const props: AgentProps = {
        query: currentInput,
        threadId: threadId,
      };
      // process() now resolves AFTER the stream is complete and the final message is saved.
      // The UI updates come via the LLMStreamSocket and ConversationSocket.
      const response: AgentFinalResponse = await artInstanceRef.current.process(props);

      console.log("ART process completed. Final Response:", response);
      // We don't add the final AI message here anymore, it should arrive via ConversationSocket
      // after being persisted by the AgentCore.

    } catch (error) {
      console.error("Error processing message:", error);
      const errorMessage: ConversationMessage = {
        id: `error-${Date.now()}`,
        role: MessageRole.SYSTEM,
        content: `Error: ${error instanceof Error ? error.message : 'Failed to get response'}`,
        timestamp: Date.now(),
        threadId: threadId,
      };
      setMessages(prev => [...prev, errorMessage]);
      setStatus('Error occurred.');
       streamingMessageIdRef.current = null; // Clear streaming ID on error
    } finally {
      setIsLoading(false); // Set loading false only after process() finishes
    }
  }, [input, isLoading, threadId]);

  // --- Render Component ---
  return (
    &lt;div className="chatbot-container"&gt;
      &lt;div className="message-list" ref={messageListRef}&gt;
        {messages.map((msg) => (
          &lt;div key={msg.id} className={`message ${msg.role} ${msg.metadata?.streaming ? 'streaming' : ''}`}&gt;
            {/* Simple rendering, consider markdown parsing for content */}
            &lt;pre style={{ whiteSpace: 'pre-wrap', margin: 0, fontFamily: 'inherit' }}&gt;{msg.content}&lt;/pre&gt;
          &lt;/div&gt;
        ))}
      &lt;/div&gt;
      &lt;div className="status-indicator"&gt;{isLoading ? status : 'Ready.'}&lt;/div&gt; {/* Show status while loading */}
      &lt;div className="input-area"&gt;
        &lt;input
          type="text"
          value={input}
          onChange={(e) => setInput(e.target.value)}
          onKeyPress={(e) => e.key === 'Enter' && !isLoading && handleSend()}
          disabled={isLoading || !artInstanceRef.current}
          placeholder={artInstanceRef.current ? "Ask something..." : "Initializing..."}
        /&gt;
        &lt;button onClick={handleSend} disabled={isLoading || !artInstanceRef.current || !input.trim()}&gt;
          {isLoading ? '...' : 'Send'}
        &lt;/button&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  );
};

export default ArtChatbot;

```</pre>
                    <h4 class="text-lg font-medium mt-6 mb-4 text-gray-800">Explanation of Features Used:</h4>
                    <ol class="list-decimal list-inside space-y-3 text-gray-700 leading-relaxed">
                        <li><strong>Initialization (`createArtInstance`):** Sets up ART with chosen adapters (`IndexedDBStorageAdapter`, `OpenAIAdapter`), the default `PESAgent`, and a built-in `CalculatorTool`.</li>
                        <li><strong>Conversation Management (`conversationManager.getMessages`):** Loads previous messages from storage when the component mounts, providing history. Messages are saved implicitly by the `PESAgent` after `process` completes.</li>
                        <li><strong>State Management (`StateManager`):** Used internally by ART to load thread configuration (like the OpenAI model specified) and potentially save agent state between turns (though this simple example doesn't explicitly manipulate `AgentState`).</li>
                        <li><strong>Reasoning (`OpenAIAdapter`, `PESAgent`):** Handles the core logic of understanding the query, planning (potentially deciding to use the calculator), and synthesizing the response. This now involves requesting and consuming an `AsyncIterable<StreamEvent>` from the `ReasoningEngine` for both planning and synthesis steps, allowing for token streaming and metadata handling.</li>
                        <li><strong>Tools (`CalculatorTool`, `ToolSystem`):** The calculator is available. If the user asks "What is 5*5?", the `PESAgent` should plan to use it, the `ToolSystem` will execute it, and the result will inform the final answer.</li>
                        <li><strong>Storage (`IndexedDBStorageAdapter`):** Ensures conversation history persists even if the user closes and reopens the browser tab.</li>
                        <li><strong>Observations & UI Sockets (`uiSystem.getObservationSocket`, `uiSystem.getLLMStreamSocket`, `uiSystem.getConversationSocket`):** The UI subscribes to the `ObservationSocket` for discrete events (like tool start/end, process start/end) and the new `LLMStreamSocket` for real-time `StreamEvent`s (tokens, metadata, errors, end signals) from the LLM. It also subscribes to the `ConversationSocket` to receive the final, persisted message after streaming is complete, ensuring UI consistency. This provides detailed real-time feedback and enables token-by-token display.</li>
                    </ol>
                     <p class="mt-6 text-gray-700 leading-relaxed">
                        This component provides a solid foundation, demonstrating the core ART features working together in a practical application, including the new streaming and enhanced observation capabilities.
                    </p>
                </div>

                 <div id="scenario-1-workflow" class="mt-10">
                    <h3 class="text-xl font-medium mb-4 text-gray-800">3.3. Detailed Internal Workflow: `art.process()` with PESAgent</h3>
                    <p class="mb-4 text-gray-700 leading-relaxed">When you call `art.process()` using the default Plan-Execute-Synthesize (PES) agent, a sequence of steps occurs internally to understand your request, potentially use tools, and generate a response. Hereâ€™s a breakdown, with both technical details and simpler explanations:</p>
                    <ol class="list-decimal list-inside space-y-4 text-gray-700 leading-relaxed">
                        <li>
                            <strong>Call Received:</strong> <code>PESAgent.process(props)</code> starts.
                            <p class="text-sm text-gray-600 ml-4 italic">In simple terms: The agent receives your query and gets ready to work.</p>
                        </li>
                        <li>
                            <strong>Record Start:</strong> Log <code>PROCESS_START</code> observation.
                            <p class="text-sm text-gray-600 ml-4 italic">In simple terms: The agent makes a note that it has started processing a new request.</p>
                        </li>
                        <li>
                            <strong>Load Context:</strong> Fetch <code>ThreadConfig</code> and <code>AgentState</code> via <code>StateManager</code>.
                            <p class="text-sm text-gray-600 ml-4 italic">In simple terms: The agent retrieves settings and memory for this conversation.</p>
                        </li>
                        <li>
                            <strong>Load History:</strong> Fetch recent <code>ConversationMessage</code>s via <code>ConversationManager</code>.
                            <p class="text-sm text-gray-600 ml-4 italic">In simple terms: The agent looks up the recent chat history.</p>
                        </li>
                        <li>
                            <strong>Get Available Tools:</strong> Fetch enabled <code>ToolSchema</code>s via <code>ToolRegistry</code> (using <code>StateManager</code> to check permissions).
                            <p class="text-sm text-gray-600 ml-4 italic">In simple terms: The agent checks which tools it can use.</p>
                        </li>
                        <li>
                            <strong>Create Planning Prompt:</strong> Use <code>PromptManager.assemblePrompt(planningBlueprint, planningContext)</code> to combine query, history, system prompt, and tool schemas into a standardized <code>ArtStandardPrompt</code> asking the LLM to plan.
                            <p class="text-sm text-gray-600 ml-4 italic">In simple terms: The agent prepares instructions (using a template) asking the AI brain to figure out a plan, including whether tools are needed.</p>
                        </li>
                        <li>
                            <strong>Execute Planning LLM Call & Stream Processing:</strong>
                            <ul class="list-disc list-inside ml-6 space-y-1 text-sm mt-1">
                                <li>Log <code>LLM_REQUEST</code> observation.</li>
                                <li>Call <code>ReasoningEngine.call(planningPrompt, { stream: true, callContext: 'AGENT_THOUGHT' })</code>.</li>
                                <li>Consume the returned <code>AsyncIterable&lt;StreamEvent&gt;</code>.</li>
                                <li>For each <code>StreamEvent</code>:
                                    <ul class="list-circle list-inside ml-6 space-y-1 text-xs mt-1">
                                        <li>Push event to <code>LLMStreamSocket</code>.</li>
                                        <li>Buffer <code>TOKEN</code> data.</li>
                                        <li>Log <code>LLM_STREAM_METADATA</code>, <code>LLM_STREAM_ERROR</code>, <code>LLM_STREAM_END</code> observations.</li>
                                        <li>Aggregate <code>METADATA</code>.</li>
                                    </ul>
                                </li>
                                <li>Log <code>LLM_RESPONSE</code> observation (with aggregated content & metadata) after stream ends.</li>
                            </ul>
                            <p class="text-sm text-gray-600 ml-4 italic mt-1">In simple terms: The agent sends the planning instructions to the AI brain, requesting a streaming response. It receives the plan piece by piece, broadcasts these pieces to the UI, logs important stream events, and collects statistics.</p>
                        </li>
                        <li>
                            <strong>Parse Planning Output:</strong> Use <code>OutputParser</code> to extract intent, plan description, and <code>ParsedToolCall[]</code> from the *aggregated* planning LLM response content.
                            <p class="text-sm text-gray-600 ml-4 italic">In simple terms: The agent reads the complete plan (assembled from the streamed tokens) and understands which tools to use.</p>
                        </li>
                        <li>
                            <strong>Record Plan:</strong> Log <code>PLANNING_OUTPUT</code> observation.
                            <p class="text-sm text-gray-600 ml-4 italic">In simple terms: The agent notes down the plan.</p>
                        </li>
                        <li>
                            <strong>Execute Tools (if <code>ParsedToolCall[]</code> is not empty):</strong>
                            <ul class="list-disc list-inside ml-6 space-y-1 text-sm mt-1">
                                <li>Call <code>ToolSystem.executeTools()</code>.</li>
                                <li><code>ToolSystem</code> validates, logs <code>TOOL_START</code>, calls <code>executor.execute()</code>, logs <code>TOOL_END</code>.</li>
                                <li>Log <code>TOOL_EXECUTION_COMPLETE</code> observation.</li>
                            </ul>
                             <p class="text-sm text-gray-600 ml-4 italic mt-1">In simple terms: If needed, the agent runs the tools specified in the plan and records the results.</p>
                        </li>
                        <li>
                            <strong>Create Synthesis Prompt:</strong> Use <code>PromptManager.assemblePrompt(synthesisBlueprint, synthesisContext)</code> to combine original query, plan, tool results, history, and system prompt into an <code>ArtStandardPrompt</code> asking for the final response.
                            <p class="text-sm text-gray-600 ml-4 italic">In simple terms: The agent prepares final instructions (using another template) asking the AI brain to write the actual answer you will see, including any tool results.</p>
                        </li>
                        <li>
                            <strong>Execute Synthesis LLM Call & Stream Processing:</strong>
                            <ul class="list-disc list-inside ml-6 space-y-1 text-sm mt-1">
                                <li>Log <code>LLM_REQUEST</code> observation.</li>
                                <li>Call <code>ReasoningEngine.call(synthesisPrompt, { stream: true, callContext: 'FINAL_SYNTHESIS' })</code>.</li>
                                <li>Consume the returned <code>AsyncIterable&lt;StreamEvent&gt;</code>.</li>
                                <li>For each <code>StreamEvent</code>:
                                    <ul class="list-circle list-inside ml-6 space-y-1 text-xs mt-1">
                                        <li>Push event to <code>LLMStreamSocket</code> (only `FINAL_SYNTHESIS_LLM_RESPONSE` tokens are typically displayed by UI).</li>
                                        <li>Buffer <code>TOKEN</code> data for the final response.</li>
                                        <li>Log <code>LLM_STREAM_...</code> observations.</li>
                                        <li>Aggregate <code>METADATA</code>.</li>
                                    </ul>
                                </li>
                                <li>Log <code>LLM_RESPONSE</code> observation after stream ends.</li>
                            </ul>
                            <p class="text-sm text-gray-600 ml-4 italic mt-1">In simple terms: The agent sends the final instructions, gets the answer piece by piece, broadcasts it to the UI, logs events, and collects stats.</p>
                        </li>
                        <li>
                            <strong>Parse Synthesis Output:</strong> Use <code>OutputParser</code> to extract the final <code>responseText</code> from the *aggregated* synthesis LLM response content.
                            <p class="text-sm text-gray-600 ml-4 italic">In simple terms: The agent extracts the complete final chat message from the streamed tokens.</p>
                        </li>
                        <li>
                            <strong>Record Synthesis:</strong> Log <code>SYNTHESIS_OUTPUT</code> observation.
                            <p class="text-sm text-gray-600 ml-4 italic">In simple terms: The agent notes down the final answer.</p>
                        </li>
                        <li>
                            <strong>Save History:</strong> Persist user query and final AI response via <code>ConversationManager</code>.
                            <p class="text-sm text-gray-600 ml-4 italic">In simple terms: The agent saves the final exchange to the chat history. (This triggers the `ConversationSocket` notification to the UI).</p>
                        </li>
                        <li>
                            <strong>Save State:</strong> Persist any changes to <code>AgentState</code> via <code>StateManager</code>.
                            <p class="text-sm text-gray-600 ml-4 italic">In simple terms: The agent saves any changes to its internal memory.</p>
                        </li>
                        <li>
                            <strong>Record End:</strong> Log <code>PROCESS_END</code> observation.
                            <p class="text-sm text-gray-600 ml-4 italic">In simple terms: The agent notes it has finished.</p>
                        </li>
                        <li>
                            <strong>Return Result:</strong> Return <code>AgentFinalResponse</code> object, including the final <code>responseText</code> and aggregated <code>ExecutionMetadata.llmMetadata</code>.
                            <p class="text-sm text-gray-600 ml-4 italic">In simple terms: The agent sends the final response object (containing the text and stats) back to the application code.</p>
                        </li>
                    </ol>
                </div>
            </section>

            <section id="scenario-2" class="mb-16 p-6 bg-white rounded-lg shadow-lg fade-in-section">
                <h2 class="text-2xl md:text-3xl font-semibold mb-6 border-b pb-3 text-sky-700">4. Scenario 2: Adding a Custom Tool (Intermediate Usage)</h2>

                <div class="mb-6 p-4 bg-blue-50 border border-blue-200 text-blue-800 rounded-md text-sm leading-relaxed">
                    <h4 class="font-semibold mb-2">Simplified Explanation for Developers:</h4>
                    <p>Imagine ART is like a highly capable smart assistant you've hired for your application. This assistant comes with some built-in abilities (like a calculator), but its real power is that you can easily teach it *new* skills that you create yourself.</p>
                    <ol class="list-decimal list-inside mt-2 space-y-1">
                        <li><strong>Creating Your Custom Skill (Your Tool):</strong> You define the new skill by writing code for it and describing what it does (its `schema`). ART provides a standard format (`IToolExecutor`) so the assistant understands how to use any skill you create.</li>
                        <li><strong>Giving the Skill to the Assistant:** When you set up the ART assistant (using <code>createArtInstance</code>), you give it a configuration list of *all* the skills (tools) you want it to have. You add your new custom skill to this list.</li>
                        <li><strong>The Assistant Learns Your Skill:** ART reads the list and adds your skill to its internal "skill library" (`ToolRegistry`).</li>
                    </ol>
                    <p class="mt-2">So, you create your tool code, ensure it follows ART's `IToolExecutor` rules, and then tell ART about it in the configuration when you start it up. No need to modify ART's core code!</p>
                </div>


                <p class="mb-5 text-gray-700 leading-relaxed">
                    Now, let's extend our chatbot by adding a custom tool that provides current information like date, time, and approximate location/locale.
                </p>
                 <p class="mb-8 text-gray-700 leading-relaxed"><strong>Goal:</strong> Create a <code>CurrentInfoTool</code> and integrate it into the ART configuration.</p>

                <div id="scenario-2-imports" class="mb-10">
                     <h3 class="text-xl font-medium mb-4 text-gray-800">4.1. Necessary Imports & Explanations</h3>
                     <p class="mb-4 text-gray-700 leading-relaxed">In addition to the imports from Scenario 1, you'll need these specifically for creating a tool:</p>
<pre><code class="language-typescript">// --- ART Tool Creation Imports ---
import {
  // The interface that every tool must implement
  IToolExecutor,
  // The type defining the tool's description, name, and input/output schemas
  ToolSchema,
  // The type defining the structure of the result returned by a tool's execute method
  ToolResult,
  // The type providing context (like threadId, traceId) to the tool's execute method
  ExecutionContext
} from 'art-framework';
</code></pre>
                    <h4 class="text-lg font-medium mt-6 mb-4 text-gray-800">Explanation of Tool Imports:</h4>
                     <div class="space-y-4 text-gray-700 leading-relaxed">
                         <div>
                            <p><strong><code>IToolExecutor</code></strong></p>
                            <p class="mb-2">The blueprint or set of rules your custom skill needs to follow so ART knows how to use it.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                The core interface for creating custom tools. Your tool class must implement this. Key requirements:
                                <ul class="list-disc list-inside ml-4">
                                    <li>Implement a readonly <code>schema</code> property of type <code>ToolSchema</code>.</li>
                                    <li>Implement an <code>async execute(input: any, context: ExecutionContext): Promise&lt;ToolResult&gt;</code> method. This method receives validated <code>input</code> (based on <code>schema.inputSchema</code>) and the <code>context</code> object. It should perform the tool's action and return a <code>Promise</code> resolving to a <code>ToolResult</code>.</li>
                                </ul>
                            </details>
                        </div>
                         <div>
                            <p><strong><code>ToolSchema</code></strong></p>
                            <p class="mb-2">The tool's "instruction manual" for the AI â€“ its name, what it does, and what information it needs to run.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                Interface defining the tool's metadata, used by both the LLM (via prompts) and the <code>ToolSystem</code>. Properties:
                                <ul class="list-disc list-inside ml-4">
                                    <li><code>name: string</code>: The unique function name the LLM will use to call the tool (e.g., "get_current_weather"). Use snake_case.</li>
                                    <li><code>description: string</code>: Detailed explanation for the LLM about the tool's purpose, capabilities, and when it should be used. Crucial for effective tool selection by the LLM.</li>
                                    <li><code>inputSchema: object</code>: A standard JSON Schema object describing the expected structure, types (string, number, boolean, object, array), required fields, and descriptions for the <code>input</code> argument of the <code>execute</code> method. Used by <code>ToolSystem</code> to validate arguments before execution.</li>
                                </ul>
                            </details>
                        </div>
                         <div>
                            <p><strong><code>ToolResult</code></strong></p>
                            <p class="mb-2">The format for the tool's answer â€“ whether it worked, and either the result or an error message.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                Interface defining the object returned by <code>IToolExecutor.execute</code>. Properties:
                                <ul class="list-disc list-inside ml-4">
                                    <li><code>status: 'success' | 'error'</code>: Must indicate the outcome.</li>
                                    <li><code>output?: any</code>: Required if <code>status</code> is 'success'. Contains the result data. Aim for JSON-serializable data (strings, numbers, booleans, arrays, plain objects) so the LLM can easily understand and incorporate it into its response.</li>
                                    <li><code>error?: string</code>: Required if <code>status</code> is 'error'. Provides a descriptive error message for logging and potentially for the LLM to understand the failure.</li>
                                </ul>
                            </details>
                        </div>
                         <div>
                            <p><strong><code>ExecutionContext</code></strong></p>
                            <p class="mb-2">Extra information passed to your tool when it runs, like which chat it's running for, useful for tracking or context-specific logic.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                Interface for the context object passed as the second argument to <code>IToolExecutor.execute</code>. Provides runtime context. Properties:
                                <ul class="list-disc list-inside ml-4">
                                    <li><code>threadId: string</code>: The ID of the conversation thread this execution belongs to.</li>
                                    <li><code>traceId?: string</code>: The ID tracing the entire <code>ArtInstance.process</code> call, useful for correlating logs across multiple steps and tool calls within a single user request.</li>
                                    <li>May contain other properties passed down from the <code>AgentProps</code> or added by the <code>IAgentCore</code> implementation.</li>
                                </ul>
                            </details>
                        </div>
                    </div>
                </div>

                <div id="scenario-2-implementation" class="mb-10">
                    <h3 class="text-xl font-medium mb-4 text-gray-800">4.2. Implementing the <code>CurrentInfoTool</code></h3>
<pre><code class="language-typescript">// src/tools/CurrentInfoTool.ts (or define within the component file for simplicity)

import { IToolExecutor, ToolSchema, ToolResult, ExecutionContext } from 'art-framework';

export class CurrentInfoTool implements IToolExecutor {
  readonly schema: ToolSchema = {
    name: "get_current_info",
    description: "Provides the current date, time, approximate user location (requires permission), and browser language/locale.",
    inputSchema: { // No specific input needed for this tool
      type: "object",
      properties: {},
    }
  };

  async execute(input: any, context: ExecutionContext): Promise&lt;ToolResult&gt; {
    console.log(`Executing CurrentInfoTool, Trace ID: ${context.traceId}`);
    try {
      const now = new Date();
      const dateTimeInfo = {
        date: now.toLocaleDateString(),
        time: now.toLocaleTimeString(),
        timezoneOffset: now.getTimezoneOffset(), // In minutes from UTC
        isoString: now.toISOString(),
      };

      let locationInfo: any = { status: 'permission_denied_or_unavailable' };
      try {
        // Use browser Geolocation API - Requires HTTPS and user permission
        if ('geolocation' in navigator) {
          locationInfo = await new Promise((resolve, reject) => {
            navigator.geolocation.getCurrentPosition(
              (position) => {
                resolve({
                  status: 'success',
                  latitude: position.coords.latitude,
                  longitude: position.coords.longitude,
                  accuracy: position.coords.accuracy, // In meters
                });
              },
              (error) => {
                // Handle errors (PERMISSION_DENIED, POSITION_UNAVAILABLE, TIMEOUT)
                resolve({ status: 'error', code: error.code, message: error.message });
              },
              { timeout: 5000 } // Set a timeout
            );
          });
        }
      } catch (geoError) {
         console.warn("Geolocation API error:", geoError);
         // Error already captured in the promise resolution
      }


      const localeInfo = {
        language: navigator.language, // e.g., "en-US"
        languages: navigator.languages, // Array of preferred languages
      };

      // Note: Getting local currency reliably client-side is complex.
      // We'll just include the locale as a hint.

      return {
        status: "success",
        output: {
          dateTime: dateTimeInfo,
          location: locationInfo,
          locale: localeInfo,
        }
      };
    } catch (error) {
      console.error("CurrentInfoTool Error:", error);
      return { status: "error", error: error instanceof Error ? error.message : "Unknown error fetching current info" };
    }
  }
}
</code></pre>
                    <h4 class="text-lg font-medium mt-6 mb-4 text-gray-800">Explanation:</h4>
                    <ol class="list-decimal list-inside space-y-3 text-gray-700 leading-relaxed">
                        <li><strong>Implement <code>IToolExecutor</code>:</strong> The class declares it follows the tool contract.</li>
                        <li><strong>Define <code>schema</code>:</strong> Provides the name (<code>get_current_info</code>), description, and specifies no required input.</li>
                        <li><strong>Implement <code>execute</code>:</strong>
                            <ul class="list-disc list-inside ml-6 space-y-2 mt-2">
                                <li>Gets the current date/time using the <code>Date</code> object.</li>
                                <li>Attempts to get the location using the browser's <code>navigator.geolocation</code> API. This is asynchronous and requires user permission (and usually HTTPS). It handles success and error cases gracefully.</li>
                                <li>Gets browser language/locale using <code>navigator.language(s)</code>.</li>
                                <li>Bundles all collected information into the <code>output</code> field of a successful <code>ToolResult</code>.</li>
                                <li>Includes error handling for unexpected issues.</li>
                            </ul>
                        </li>
                    </ol>
                </div>

                <div id="scenario-2-integration" class="mb-8">
                    <h3 class="text-xl font-medium mb-4 text-gray-800">4.3. Integrating the Tool into the Chatbot</h3>
                    <p class="mb-4 text-gray-700 leading-relaxed">Modify the ART configuration within the <code>ArtChatbot</code> component's <code>useEffect</code> hook:</p>
<pre><code class="language-typescript">// Inside the useEffect hook in ArtChatbot.tsx

import { CurrentInfoTool } from './tools/CurrentInfoTool'; // Adjust path if needed

// ... inside initializeArt function ...
          const config = {
            storage: { /* ... */ },
            reasoning: { /* ... */ },
            agentCore: PESAgent,
            tools: [
                new CalculatorTool(),
                new CurrentInfoTool() // Add an instance of the new tool
            ]
          };

          const instance = await createArtInstance(config);
// ... rest of the initialization ...
</code></pre>
                    <h4 class="text-lg font-medium mt-6 mb-4 text-gray-800">How it Works Now:</h4>
                     <ul class="list-disc list-inside space-y-3 text-gray-700 leading-relaxed">
                        <li><strong>Node 1 (Developer Interface):** You've defined the <code>CurrentInfoTool</code> and told ART about it by adding <code>new CurrentInfoTool()</code> to the <code>tools</code> array in the configuration.</li>
                        <li><strong>Node 2 (Core Orchestration):** When the user asks something like "What time is it?" or "Where am I?", the <code>PESAgent</code> gathers the necessary <code>PromptContext</code> (including the user query, history, and available tools like <code>get_current_info</code>). It then uses the `PromptManager` with its planning blueprint and this context to create a standardized <code>ArtStandardPrompt</code>. This prompt is sent to the LLM via the `ReasoningEngine` (which handles streaming). The `OutputParser` then parses the LLM's response (from the stream) to identify the user's intent and any planned tool calls. If the LLM plans to use `get_current_info`, the `ToolSystem` finds your `CurrentInfoTool` in the `ToolRegistry` and calls its `execute` method. The results (date, time, location status, locale) are passed back to the `PESAgent`. The agent then gathers a new `PromptContext` (including the original query, plan, and tool results) and uses the `PromptManager` with its synthesis blueprint to create another `ArtStandardPrompt`. This is sent to the LLM via the `ReasoningEngine` (again, handling streaming), and the `OutputParser` extracts the final response from the stream.</li>
                        <li><strong>Node 3 (External Dependencies & Interactions):** The <code>CurrentInfoTool</code> interacts with browser APIs (`Date`, `navigator.geolocation`, `navigator.language`). If geolocation permission is granted, it interacts with the device's location services. The `ProviderAdapter` used by the `ReasoningEngine` handles communication with the external LLM API, including processing streaming responses.</li>
                    </ul>
                </div>
            </section>

            <section id="scenario-3" class="mb-16 p-6 bg-white rounded-lg shadow-lg fade-in-section">
                <h2 class="text-2xl md:text-3xl font-semibold mb-6 border-b pb-3 text-sky-700">5. Scenario 3: Adding a Custom Provider Adapter (Anthropic Example)</h2>

                 <div class="mb-6 p-4 bg-blue-50 border border-blue-200 text-blue-800 rounded-md text-sm leading-relaxed">
                    <h4 class="font-semibold mb-2">Simplified Explanation for Developers:</h4>
                    <p>Think of ART as that smart assistant again. This assistant needs to talk to different "AI brains" (Large Language Models like GPT, Gemini, Claude, Ollama, etc.) to get its work done. But each AI brain speaks a slightly different language (their API format).</p>
                     <ol class="list-decimal list-inside mt-2 space-y-1">
                         <li><strong>Creating a Translator (Your Custom Provider Adapter):</strong> If you want the ART assistant to be able to talk to a *new* AI brain it doesn't already know, you need to create a special "translator" for that specific AI brain. This translator is what we call a **Provider Adapter**. Your job is to write the code for this translator. ART provides a standard blueprint (the `ProviderAdapter` interface) that your translator must follow. This blueprint ensures that your translator knows how to:
                             <ul class="list-disc list-inside ml-6 space-y-1 text-xs mt-1">
                                 <li>Receive instructions from the ART assistant in a standard format (the `ArtStandardPrompt`).</li>
                                 <li>Translate those standard instructions into the specific language the new AI brain understands (its API format).</li>
                                 <li>Send the translated request to the new AI brain.</li>
                                 <li>Receive the response back from the new AI brain (including handling streaming responses).</li>
                                 <li>Translate the AI brain's response back into a standard format that the ART assistant can understand (`AsyncIterable<StreamEvent>`).</li>
                             </ul>
                         </li>
                         <li><strong>Giving the Translator to the Assistant:** Just like with custom tools, when you set up the ART assistant for your application using <code>createArtInstance</code>, you tell it which translator to use for its AI brain communication. In the configuration you provide to <code>createArtInstance</code>, you specify your custom Provider Adapter as the <code>reasoning</code> provider.</li>
                         <li><strong>The Assistant Uses Your Translator:** When <code>createArtInstance</code> runs, ART sees that you've specified your custom translator. From then on, whenever the ART assistant needs to talk to an AI brain (i.e., when the <code>ReasoningEngine</code> is called), it will use *your* translator to handle the communication with the specific AI brain you've set up.</li>
                     </ol>
                     <p class="mt-2">So, you create a custom Provider Adapter that acts as a translator for a specific LLM API, making sure it follows ART's standard `ProviderAdapter` blueprint. Then, when you initialize ART in your application, you tell it to use your custom adapter for reasoning. You don't need to change any of ART's core files; you just provide your new component during the setup process.</p>
                </div>

                <p class="mb-5 text-gray-700 leading-relaxed">
                    Sometimes, you might want to connect ART to an LLM provider that isn't supported out-of-the-box, like Anthropic's Claude models, or perhaps use a proxy or a self-hosted model with a unique API. This requires creating a custom Provider Adapter.
                </p>
                 <p class="mb-8 text-gray-700 leading-relaxed"><strong>Goal:</strong> Implement a functional <code>AnthropicAdapter</code> using the Anthropic Messages API, supporting streaming.</p>

                 <div id="scenario-3-imports" class="mb-10">
                     <h3 class="text-xl font-medium mb-4 text-gray-800">5.1. Necessary Imports & Explanations</h3>
<pre><code class="language-typescript">// --- ART Provider Adapter Creation Imports ---
import {
  // The base interface for LLM provider adapters
  ProviderAdapter,
  // The core interface for making LLM calls (ProviderAdapter extends this) - Now returns AsyncIterable<StreamEvent>
  ReasoningEngine,
  // Type for standardized prompts (array of messages) ART uses internally
  ArtStandardPrompt,
  ArtStandardMessage,
  // Type for options passed to the LLM call (model params, streaming flags, context, etc.)
  CallOptions,
  // Type for conversation messages (may be part of ArtStandardPrompt context)
  ConversationMessage,
  // Enum for message roles
  MessageRole,
  // Types for streaming output
  StreamEvent,
  LLMMetadata
  // ObservationManager/Type are typically not used directly within adapters
} from 'art-framework';

// --- Potentially types from Anthropic SDK if used, or define manually ---
// Example manual types for Anthropic Messages API
interface AnthropicMessage {
  role: 'user' | 'assistant';
  content: string | Array<{ type: 'text', text: string } | { type: 'tool_use', id: string, name: string, input: any } | { type: 'tool_result', tool_use_id: string, content: string, is_error?: boolean }>; // Updated content type for tool use/result
}
interface AnthropicRequestBody {
  model: string;
  messages: AnthropicMessage[];
  system?: string;
  max_tokens: number;
  temperature?: number;
  stop_sequences?: string[];
  stream?: boolean; // Added for streaming
  // ... other Anthropic params
}
interface AnthropicResponse { // For non-streaming or final aggregated response
  content: Array<{ type: 'text', text: string } | { type: 'tool_use', id: string, name: string, input: any }>; // Updated content type for tool use
  stop_reason?: string;
  usage?: { input_tokens: number, output_tokens: number };
  // ... other fields
}
</code></pre>
                    <h4 class="text-lg font-medium mt-6 mb-4 text-gray-800">Explanation of Provider Adapter Imports:</h4>
                     <div class="space-y-4 text-gray-700 leading-relaxed">
                         <div>
                             <p><strong><code>ProviderAdapter</code></strong></p>
                             <p class="mb-2">The blueprint for creating a translator between ART's general way of thinking about AI models and the specific way a particular AI provider's API works (like Anthropic).</p>
                             <details>
                                 <summary>Developer Notes</summary>
                                 The interface your custom LLM adapter class must implement. It extends <code>ReasoningEngine</code>, meaning it must primarily implement the <code>call</code> method (which now returns `Promise<AsyncIterable<StreamEvent>>`). It also requires a <code>readonly providerName: string</code> property to identify the adapter (e.g., 'anthropic').
                             </details>
                         </div>
                         <div>
                             <p><strong><code>ReasoningEngine</code></strong></p>
                             <p class="mb-2">Defines the basic capability of making a call to an AI model with a prompt. `ProviderAdapter` builds upon this.</p>
                             <details>
                                 <summary>Developer Notes</summary>
                                 The base interface defining the core `async call(prompt: ArtStandardPrompt, options: CallOptions): Promise<AsyncIterable<StreamEvent>>` method signature (updated for streaming and standardized prompt). Your `ProviderAdapter` implementation provides the concrete logic for this method, typically returning an async generator function.
                             </details>
                         </div>
                         <div>
                             <p><strong><code>ArtStandardPrompt</code> / <code>ArtStandardMessage</code></strong></p>
                             <p class="mb-2">Represents the standardized, provider-agnostic instructions prepared for the AI model by the `PromptManager`. This is now an array of `ArtStandardMessage` objects.</p>
                             <details>
                                 <summary>Developer Notes</summary>
                                 Your adapter's <code>call</code> method receives this standard format (`ArtStandardPrompt`) from the `ReasoningEngine` and is responsible for translating it into the specific message structure and format required by the target LLM provider's API (e.g., mapping roles, handling content types, structuring tool calls/results).
                             </details>
                         </div>
                         <div>
                             <p><strong><code>CallOptions</code></strong></p>
                             <p class="mb-2">Additional settings and information passed along when making the AI call, like which specific model version to use (e.g., 'claude-3-opus-20240229'), creativity settings (temperature), max response length (`maxTokens`), stop sequences, and importantly, whether to stream the response (`stream: true`) and the context of the call (`callContext`).</p>
                             <details>
                                 <summary>Developer Notes</summary>
                                 Interface for the options object passed to <code>ReasoningEngine.call</code>. Includes properties like <code>threadId</code>, <code>traceId</code>, <code>sessionId</code>, <code>model</code> (optional override), <code>temperature</code>, <code>maxTokens</code>, <code>stopSequences</code>, and crucially <code>stream?: boolean</code> and <code>callContext?: string</code>. Your adapter's <code>call</code> method must check <code>stream</code> and map relevant options to the provider API.
                             </details>
                         </div>
                         <div>
                             <p><strong><code>ConversationMessage</code>, <code>MessageRole</code></strong></p>
                             <p class="mb-2">Needed within the adapter's prompt translation logic to correctly interpret the roles and content within the received `ArtStandardPrompt` and map them to the provider's expected format.</p>
                             <details>
                                 <summary>Developer Notes</summary>
                                 Used within the adapter's `call` method (specifically in the `formatMessages` helper) during the prompt translation step before sending the request to the Anthropic API. System messages, user messages, assistant messages, tool requests, and tool results from the `ArtStandardPrompt` need careful mapping to Anthropic's structure.
                             </details>
                         </div>
                         <div>
                             <p><strong><code>StreamEvent</code>, <code>LLMMetadata</code></strong></p>
                             <p class="mb-2">These are the types your adapter's `call` method will yield via its `AsyncIterable` return value when streaming is enabled.</p>
                             <details>
                                 <summary>Developer Notes</summary>
                                 Your adapter needs to construct <code>StreamEvent</code> objects with the correct <code>type</code>, <code>data</code>, <code>tokenType</code>, and IDs. For <code>METADATA</code> events, the <code>data</code> should conform to the <code>LLMMetadata</code> interface. The adapter must parse the provider's specific stream format (e.g., Server-Sent Events) to generate these standard events.
                             </details>
                         </div>
                     </div>
                 </div>

                 <div id="scenario-3-implementation" class="mb-10">
                    <h3 class="text-xl font-medium mb-4 text-gray-800">5.2. Implementing <code>AnthropicAdapter</code> (with Streaming)</h3>
<pre><code class="language-typescript">// src/adapters/AnthropicAdapter.ts

import {
  ProviderAdapter, ArtStandardPrompt, ArtStandardMessage, CallOptions, ConversationMessage, MessageRole,
  StreamEvent, LLMMetadata, ReasoningEngine // Import necessary types
} from 'art-framework';

// Example types matching Anthropic Messages API structure
interface AnthropicMessage {
  role: 'user' | 'assistant';
  // Content can be string or complex array for tool use/results
  content: string | Array<{ type: 'text', text: string } | { type: 'tool_use', id: string, name: string, input: any } | { type: 'tool_result', tool_use_id: string, content: string, is_error?: boolean }>;
}
interface AnthropicRequestBody {
  model: string;
  messages: AnthropicMessage[];
  system?: string;
  max_tokens: number;
  temperature?: number;
  stop_sequences?: string[];
  stream?: boolean;
  // Potentially add tool definitions if using Anthropic's native tool support
  // tools?: Array<{ name: string, description: string, input_schema: object }>;
}
// Type for non-streaming or final aggregated response
interface AnthropicResponse {
  content: Array<{ type: 'text', text: string } | { type: 'tool_use', id: string, name: string, input: any }>;
  stop_reason?: string;
  usage?: { input_tokens: number, output_tokens: number };
  // ... other fields
}

interface AnthropicAdapterOptions {
  apiKey: string;
  model?: string; // e.g., 'claude-3-5-sonnet-20240620'
  defaultMaxTokens?: number;
  defaultTemperature?: number;
  anthropicVersion?: string; // e.g., '2023-06-01'
}

export class AnthropicAdapter implements ProviderAdapter {
  readonly providerName = 'anthropic';
  private options: AnthropicAdapterOptions;

  constructor(options: AnthropicAdapterOptions) {
    if (!options.apiKey) {
      throw new Error(`Anthropic adapter requires an apiKey.`);
    }
    this.options = {
        ...options,
        defaultMaxTokens: options.defaultMaxTokens ?? 1024,
        defaultTemperature: options.defaultTemperature ?? 0.7,
        anthropicVersion: options.anthropicVersion ?? '2023-06-01'
    };
  }

  // Helper to format ArtStandardPrompt messages to Anthropic format
  private formatMessages(prompt: ArtStandardPrompt): { messages: AnthropicMessage[], system?: string } {
    let systemPrompt: string | undefined = undefined;
    const anthropicMessages: AnthropicMessage[] = [];

    let lastRole: 'user' | 'assistant' | null = null;
    for (const message of prompt) {
      if (message.role === 'system') {
        systemPrompt = String(message.content); // Use the last system message content
        continue;
      }

      let role: 'user' | 'assistant';
      let content: AnthropicMessage['content'];

      // Map ART roles to Anthropic roles and content structure
      if (message.role === 'user') {
        role = 'user';
        content = String(message.content); // Simple user text
      } else if (message.role === 'assistant') {
        role = 'assistant';
        content = String(message.content); // Simple assistant text
      } else if (message.role === 'tool_request' && Array.isArray(message.content)) {
        // Assistant requests tool use
        role = 'assistant';
        content = message.content.map((toolCall: any) => ({
          type: 'tool_use',
          id: toolCall.id || `toolu_${Date.now()}`, // Ensure an ID exists
          name: toolCall.function.name,
          // Anthropic expects input as object, ART standard stores arguments string
          input: typeof toolCall.function.arguments === 'string' ? JSON.parse(toolCall.function.arguments) : toolCall.function.arguments,
        }));
      } else if (message.role === 'tool_result') {
        // User provides tool result
        role = 'user';
        // Tool results need to be wrapped in the specific Anthropic structure
        content = [{
          type: 'tool_result',
          tool_use_id: message.tool_call_id!, // tool_call_id is required
          content: typeof message.content === 'object' ? JSON.stringify(message.content) : String(message.content),
          // Optionally indicate errors
          // is_error: message.metadata?.isError // Assuming metadata might contain error flag
        }];
      } else {
        console.warn(`AnthropicAdapter: Skipping message with unhandled role: ${message.role}`);
        continue; // Skip unhandled roles
      }

      // Merge consecutive messages of the same mapped role if needed by Anthropic API rules
      if (role === lastRole) {
        console.warn(`AnthropicAdapter: Consecutive ${role} messages detected. Merging content.`);
        const lastMsg = anthropicMessages.pop()!;
        // Simple string concatenation for text, array concat for tool results/requests
        if (typeof lastMsg.content === 'string' && typeof content === 'string') {
          lastMsg.content = `${lastMsg.content}\n${content}`;
        } else if (Array.isArray(lastMsg.content) && Array.isArray(content)) {
          lastMsg.content = [...lastMsg.content, ...content];
        } else {
          // Handle mixed types? Convert to string array? Depends on API tolerance.
          // Forcing array structure:
          const lastContentArray = Array.isArray(lastMsg.content) ? lastMsg.content : [{ type: 'text', text: lastMsg.content }];
          const currentContentArray = Array.isArray(content) ? content : [{ type: 'text', text: String(content) }];
          lastMsg.content = [...lastContentArray, ...currentContentArray];
        }
        anthropicMessages.push(lastMsg);
      } else {
        anthropicMessages.push({ role, content });
        lastRole = role;
      }
    }

    // Ensure the conversation starts with a user message if possible (Anthropic requirement)
    if (anthropicMessages.length > 0 && anthropicMessages[0].role === 'assistant') {
        console.warn("AnthropicAdapter: Conversation starts with assistant message, prepending empty user message.");
        anthropicMessages.unshift({ role: 'user', content: "(Context setting: Conversation starts here)" });
    }
     // Ensure the conversation ends with a user message if the last was assistant (Anthropic requirement)
     if (anthropicMessages.length > 0 && anthropicMessages[anthropicMessages.length - 1].role === 'assistant') {
          console.warn("AnthropicAdapter: Conversation ends with assistant message. This might lead to issues if expecting immediate user input next.");
          // Depending on the use case, might need to append a placeholder user message or handle differently.
     }


    return { messages: anthropicMessages, system: systemPrompt };
  }

  // Updated to accept ArtStandardPrompt and return AsyncIterable<StreamEvent>
  async call(prompt: ArtStandardPrompt, options: CallOptions): Promise<AsyncIterable<StreamEvent>> {
    const { threadId, traceId = `anthropic-trace-${Date.now()}`, sessionId, stream, callContext } = options;

    const { messages, system } = this.formatMessages(prompt); // Use updated formatMessages
    const modelToUse = options.model || this.options.model || 'claude-3-5-sonnet-20240620';
    const maxTokens = options.maxTokens ?? this.options.defaultMaxTokens!;
    const temperature = options.temperature ?? this.options.defaultTemperature!;

    const requestBody: AnthropicRequestBody = {
      model: modelToUse,
      messages: messages,
      max_tokens: maxTokens,
      temperature: temperature,
    };

    if (system) {
      requestBody.system = system;
    }
    if (options.stopSequences) {
      requestBody.stop_sequences = options.stopSequences;
    }
    if (stream) {
        requestBody.stream = true;
    }
    // Add tool definitions to requestBody.tools if using Anthropic's native tool support
    // based on tools passed in options or derived from prompt context.

    const apiUrl = 'https://api.anthropic.com/v1/messages';

    const generator = async function*(this: AnthropicAdapter): AsyncIterable<StreamEvent> {
        try {
            const response = await fetch(apiUrl, {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                    'x-api-key': this.options.apiKey,
                    'anthropic-version': this.options.anthropicVersion!,
                    // 'anthropic-beta': 'tools-2024-04-04', // Enable if using native tool support
                },
                body: JSON.stringify(requestBody),
            });

            if (!response.ok) {
                const errorBody = await response.text();
                console.error(`Anthropic API Error (${response.status}): ${errorBody}`);
                yield { type: 'ERROR', data: new Error(`Anthropic API Error (${response.status}): ${errorBody}`), threadId, traceId, sessionId };
                yield { type: 'END', data: null, threadId, traceId, sessionId }; // Ensure END is yielded
                return;
            }

            // --- Handle Streaming Response ---
            if (stream && response.body) {
                const reader = response.body.pipeThrough(new TextDecoderStream()).getReader();
                let buffer = '';
                let messageStopReason: string | null = null;
                let finalUsageData: any = null;
                let thinkingTokens = 0; // Track thinking tokens if possible/needed

                while (true) {
                    const { value, done } = await reader.read();
                    if (done) break;

                    buffer += value;
                    const lines = buffer.split('\n');
                    buffer = lines.pop() || ''; // Keep incomplete line

                    for (const line of lines) {
                        if (line.startsWith('data: ')) {
                            const dataContent = line.substring(6).trim();
                            try {
                                const jsonData = JSON.parse(dataContent);
                                const type = jsonData.type;

                                // Determine tokenType based on callContext
                                const tokenTypeBase = callContext === 'AGENT_THOUGHT' ? 'AGENT_THOUGHT' : 'FINAL_SYNTHESIS';

                                if (type === 'content_block_delta' && jsonData.delta?.type === 'text_delta') {
                                    const textDelta = jsonData.delta.text;
                                    // Anthropic stream doesn't easily distinguish thinking vs response within a call
                                    // Rely solely on callContext for now.
                                    const tokenType = `${tokenTypeBase}_LLM_RESPONSE`;
                                    yield { type: 'TOKEN', data: textDelta, threadId, traceId, sessionId, tokenType: tokenType as StreamEvent['tokenType'] };
                                } else if (type === 'message_start') {
                                    finalUsageData = jsonData.message?.usage ?? finalUsageData;
                                } else if (type === 'message_delta') {
                                    finalUsageData = { ...(finalUsageData ?? {}), ...jsonData.usage };
                                    messageStopReason = jsonData.delta?.stop_reason ?? messageStopReason;
                                } else if (type === 'message_stop') {
                                    // Stream finished signal from Anthropic
                                    // Metadata is typically sent in the final 'message_stop' or derived from previous events
                                    if (finalUsageData || messageStopReason) {
                                        const metadata: LLMMetadata = {
                                            inputTokens: finalUsageData?.input_tokens,
                                            outputTokens: finalUsageData?.output_tokens,
                                            thinkingTokens: thinkingTokens > 0 ? thinkingTokens : undefined, // Include if tracked
                                            stopReason: messageStopReason,
                                            providerRawUsage: finalUsageData,
                                            traceId: traceId,
                                        };
                                        yield { type: 'METADATA', data: metadata, threadId, traceId, sessionId };
                                    }
                                    yield { type: 'END', data: null, threadId, traceId, sessionId };
                                    return; // Exit generator
                                }
                                // Handle tool_use deltas if needed (e.g., streaming arguments)
                                else if (type === 'content_block_start' && jsonData.content_block?.type === 'tool_use') {
                                    // Could yield a specific event for UI to indicate tool use start
                                } else if (type === 'content_block_stop') {
                                    // Tool use block finished
                                }

                            } catch (parseError: any) {
                                console.warn(`Failed to parse Anthropic stream chunk: ${dataContent}`, parseError);
                                yield { type: 'ERROR', data: new Error(`Stream parse error: ${parseError.message}`), threadId, traceId, sessionId };
                            }
                        }
                    }
                }
                // If loop finishes without message_stop (unlikely but possible), yield END
                yield { type: 'END', data: null, threadId, traceId, sessionId };

            // --- Handle Non-Streaming Response ---
            } else {
                const responseData: AnthropicResponse = await response.json();
                let responseContentText = '';
                let toolUseCalls: any[] = []; // Collect tool calls if any

                 if (responseData.content) {
                     for (const block of responseData.content) {
                         if (block.type === 'text') {
                             responseContentText += block.text;
                         } else if (block.type === 'tool_use') {
                             // Collect tool use information if needed for non-streaming response handling
                             toolUseCalls.push({ id: block.id, name: block.name, input: block.input });
                         }
                     }
                 }

                 // Determine tokenType based on callContext
                 const tokenTypeBase = callContext === 'AGENT_THOUGHT' ? 'AGENT_THOUGHT' : 'FINAL_SYNTHESIS';
                 const tokenType = `${tokenTypeBase}_LLM_RESPONSE`;

                 // Yield a single TOKEN event with the full text content
                 // If tool calls occurred, the agent core needs to handle them based on this response
                 yield { type: 'TOKEN', data: responseContentText, threadId, traceId, sessionId, tokenType: tokenType as StreamEvent['tokenType'] };
                 // Optionally yield a separate event for tool calls if needed by agent logic
                 // if (toolUseCalls.length > 0) yield { type: 'TOOL_CALLS', data: toolUseCalls, ... }

                // Yield METADATA
                const usage = responseData.usage;
                if (usage || responseData.stop_reason) {
                    const metadata: LLMMetadata = {
                        inputTokens: usage?.input_tokens,
                        outputTokens: usage?.output_tokens,
                        stopReason: responseData.stop_reason,
                        providerRawUsage: usage,
                        traceId: traceId,
                    };
                    yield { type: 'METADATA', data: metadata, threadId, traceId, sessionId };
                }
                // Yield END signal
                yield { type: 'END', data: null, threadId, traceId, sessionId };
            }

        } catch (error: any) {
            console.error(`${this.providerName} adapter error in generator:`, error);
            yield { type: 'ERROR', data: error instanceof Error ? error : new Error(String(error)), threadId, traceId, sessionId };
            yield { type: 'END', data: null, threadId, traceId, sessionId }; // Ensure END is yielded even after error
        }
    }.bind(this); // Bind the generator function to the class instance

    return generator(); // Return the async generator
  }
}
</code></pre>
                    <h4 class="text-lg font-medium mt-6 mb-4 text-gray-800">Explanation:</h4>
                    <ol class="list-decimal list-inside space-y-3 text-gray-700 leading-relaxed">
                        <li><strong>Implements `ProviderAdapter`:** Adheres to the updated contract.</li>
                        <li><strong>Constructor:** Takes standard options.</li>
                        <li><strong>`formatMessages` Helper (Updated):**
                            <ul class="list-disc list-inside ml-6 space-y-2 mt-2">
                                <li>Accepts `ArtStandardPrompt` as input.</li>
                                <li>Maps ART standard roles (`system`, `user`, `assistant`, `tool_request`, `tool_result`) to Anthropic roles (`user`, `assistant`) and content structures (including `tool_use` and `tool_result` block types).</li>
                                <li>Handles merging consecutive messages of the same mapped role.</li>
                                <li>Ensures conversation starts with a `user` message.</li>
                            </ul>
                        </li>
                        <li><strong>`call` Method (Updated):**
                            <ul class="list-disc list-inside ml-6 space-y-2 mt-2">
                                <li>Accepts `ArtStandardPrompt`.</li>
                                <li>Returns `Promise<AsyncIterable<StreamEvent>>`.</li>
                                <li>Uses an `async function*` generator to yield `StreamEvent`s.</li>
                                <li>Calls `formatMessages`.</li>
                                <li>Constructs the Anthropic API request body, including `stream: true` if requested in `CallOptions`.</li>
                                <li>Makes the `fetch` request.</li>
                                <li>**Streaming Logic:** If `stream` is true and `response.body` exists:
                                    <ul>
                                        <li>Uses `TextDecoderStream` to read the response body.</li>
                                        <li>Parses Server-Sent Events (SSE) format typical of Anthropic streams.</li>
                                        <li>Yields `TOKEN` events for `content_block_delta` -> `text_delta`, determining `tokenType` based on `callContext`.</li>
                                        <li>Yields `METADATA` events based on `message_delta` or `message_stop`.</li>
                                        <li>Handles other stream event types (`message_start`, `message_stop`, `content_block_start`, `content_block_stop`).</li>
                                        <li>Yields `ERROR` events on API or parsing errors.</li>
                                        <li>Yields `END` event when the stream completes or stops.</li>
                                    </ul>
                                </li>
                                 <li>**Non-Streaming Logic:** If `stream` is false:
                                    <ul>
                                        <li>Reads the full JSON response.</li>
                                        <li>Extracts text content and any `tool_use` blocks.</li>
                                        <li>Yields a single `TOKEN` event with the aggregated text.</li>
                                        <li>Yields a `METADATA` event with usage info.</li>
                                        <li>Yields the `END` event.</li>
                                    </ul>
                                </li>
                                <li>Includes robust error handling within the generator.</li>
                            </ul>
                        </li>
                    </ol>
                 </div>

                 <div id="scenario-3-integration" class="mb-8">
                    <h3 class="text-xl font-medium mb-4 text-gray-800">5.3. Integrating the <code>AnthropicAdapter</code></h3>
                    <p class="mb-4 text-gray-700 leading-relaxed">As noted before, the default <code>AgentFactory</code> doesn't directly support custom provider classes. You would likely need to:</p>
                    <ol class="list-decimal list-inside space-y-2 mb-4 text-gray-700 leading-relaxed">
                        <li><strong>Manually Instantiate:** Create instances of <code>AnthropicAdapter</code>, <code>StorageAdapter</code>, Repositories, Managers, Systems, and your chosen <code>IAgentCore</code> implementation, injecting dependencies manually.</li>
                        <li><strong>Extend/Modify <code>AgentFactory</code>:** If you control the framework code, modify the factory to recognize a custom provider option (e.g., <code>providerInstance</code> or <code>providerClass</code> in the config) and use it instead of the built-in ones.</li>
                    </ol>
                    <p class="mb-4 text-gray-700 leading-relaxed"><strong>Example Manual Instantiation Snippet (Conceptual):**</p>
<pre><code class="language-typescript">// --- Manual Instantiation Example ---
import { AnthropicAdapter } from './adapters/AnthropicAdapter';
import { IndexedDBStorageAdapter } from 'art-framework';
import { ConversationRepository, StateRepository, ObservationRepository } from 'art-framework'; // Assuming concrete repo exports
import { ConversationManagerImpl, StateManagerImpl, ObservationManagerImpl } from 'art-framework'; // Assuming concrete manager exports
import { ToolRegistryImpl, ToolSystemImpl, UISystemImpl } from 'art-framework'; // Assuming concrete system exports
import { PromptManagerImpl, OutputParserImpl, ReasoningEngineImpl } from 'art-framework'; // Assuming concrete reasoning exports
import { PESAgent } from 'art-framework';
import { CalculatorTool } from 'art-framework';
// ... other necessary imports

async function setupManually(): Promise&lt;ArtInstance&gt; {
    // 1. Init Adapters
    const storageAdapter = new IndexedDBStorageAdapter({ dbName: 'manualArtDb', objectStores: ['conversations', 'state', 'observations'] });
    await storageAdapter.init?.(); // If adapter has init

    // Instantiate the custom provider adapter
    const providerAdapter = new AnthropicAdapter({ apiKey: 'YOUR_ANTHROPIC_KEY' });

    // 2. Init Repositories
    const conversationRepository = new ConversationRepository(storageAdapter);
    const stateRepository = new StateRepository(storageAdapter);
    const observationRepository = new ObservationRepository(storageAdapter);

    // 3. Init UI System & Sockets (crucial dependency)
    const uiSystem = new UISystemImpl(); // Assuming a concrete UISystem implementation

    // 4. Init Managers (Inject Repositories and Sockets)
    const observationManager = new ObservationManagerImpl(observationRepository, uiSystem.getObservationSocket());
    const conversationManager = new ConversationManagerImpl(conversationRepository, uiSystem.getConversationSocket());
    const stateManager = new StateManagerImpl(stateRepository);

    // 5. Init Tooling
    const toolRegistry = new ToolRegistryImpl();
    await toolRegistry.registerTool(new CalculatorTool());
    const toolSystem = new ToolSystemImpl(toolRegistry, stateManager, observationManager);

    // 6. Init Reasoning Components
    const reasoningEngine = new ReasoningEngineImpl(providerAdapter); // Use the custom adapter instance
    const promptManager = new PromptManagerImpl(); // Use default stateless prompt manager
    const outputParser = new OutputParserImpl(); // Use default parser

    // 7. Init Agent Core (Inject all dependencies)
    const agentCore = new PESAgent({
        stateManager,
        conversationManager,
        toolRegistry,
        promptManager,
        reasoningEngine,
        outputParser,
        observationManager,
        toolSystem,
        uiSystem // Inject UISystem for stream handling
    });

    // 8. Construct ArtInstance object
    const artInstance: ArtInstance = {
        process: agentCore.process.bind(agentCore),
        conversationManager,
        stateManager,
        toolRegistry,
        observationManager,
        uiSystem // Expose the UI System instance
    };

    return artInstance;
}
</code></pre>
                    <p class="mt-4 text-gray-600 italic text-sm">Note: The manual setup requires careful dependency injection, especially for the <code>UISystem</code> which provides the sockets needed by Managers and the Agent Core. Using the factory is highly recommended if possible, or consider extending it for custom adapter support.</p>
                 </div>
            </section>

             <section id="scenario-4" class="mb-16 p-6 bg-white rounded-lg shadow-lg fade-in-section">
                <h2 class="text-2xl md:text-3xl font-semibold mb-6 border-b pb-3 text-sky-700">6. Scenario 4: Adding a Custom Storage Adapter (DuckDB WASM Example)</h2>

                 <div class="mb-6 p-4 bg-blue-50 border border-blue-200 text-blue-800 rounded-md text-sm leading-relaxed">
                     <h4 class="font-semibold mb-2">Simplified Explanation for Developers:</h4>
                     <p>Imagine ART, our smart assistant, needs a place to keep its notes and memories (like conversation history or agent state). By default, it might use a simple notebook (IndexedDB) or just remember things short-term (in-memory). But you want it to use a more robust, cloud-based filing cabinet (like Supabase) for long-term storage, while still using a quick notepad (in-memory) for temporary notes to speed things up.</p>
                     <ol class="list-decimal list-inside mt-2 space-y-1">
                         <li><strong>Creating Your Filing Cabinet Connector (Your Custom Supabase Adapter):</strong> You need to build a special connector that knows how to talk to your Supabase filing cabinet. This connector is your custom **Storage Adapter**. You'll write code that implements ART's standard `StorageAdapter` blueprint. This blueprint requires your connector to have methods for basic filing operations: `get`, `set`, `delete`, `query`, `init` (optional), `clearCollection`/`clearAll` (optional). Your code for the Supabase adapter will use the Supabase client library to perform these operations against your Supabase database.</li>
                         <li><strong>Setting Up the Quick Notepad (Using InMemoryStorageAdapter):** ART already comes with a simple in-memory notepad (`InMemoryStorageAdapter`). This adapter is very fast but notes are lost when the browser tab is closed.</li>
                         <li><strong>Connecting the Notepad and the Filing Cabinet (Creating a Caching Adapter):** You create *another* custom adapter, `CachingStorageAdapter`. This adapter uses both the `InMemoryStorageAdapter` (notepad) and your `SupabaseAdapter` (filing cabinet connector).
                             <ul class="list-disc list-inside ml-6 space-y-1 text-xs mt-1">
                                 <li>When asked to `get` a note, it checks the notepad first. If not found, it asks the filing cabinet connector, saves a copy in the notepad, and returns the note.</li>
                                 <li>When asked to `set` or `delete`, it performs the operation on *both* the notepad and the filing cabinet connector.</li>
                             </ul>
                         </li>
                         <li><strong>Giving the Combined Setup to the Assistant:** When you set up ART using `createArtInstance`, you provide your `CachingStorageAdapter` as the main `storage` component. Your `CachingStorageAdapter` instance will be created with instances of the `InMemoryStorageAdapter` and your `SupabaseAdapter` inside it.</li>
                     </ol>
                     <p class="mt-2">You don't need to change any of ART's core files. You build custom components that adhere to ART's standard interfaces and wire them together during your application's initialization.</p>
                 </div>

                 <p class="mb-5 text-gray-700 leading-relaxed">
                    Let's explore using DuckDB WASM as a storage backend. DuckDB is an in-process analytical data management system, and its WASM version allows running it directly in the browser. This could enable more powerful local data storage and querying, including potential vector similarity search for RAG-like capabilities, compared to basic <code>localStorage</code> or <code>IndexedDB</code>.
                </p>
                 <p class="mb-5 text-gray-700 leading-relaxed"><strong>Goal:</strong> Implement a skeleton <code>DuckDBWasmAdapter</code> demonstrating basic CRUD and conceptual vector storage/search.</p>
                 <p class="mb-8 p-4 bg-yellow-50 border border-yellow-200 text-yellow-800 rounded-md text-sm leading-relaxed">
                    <strong>Disclaimer:</strong> Integrating DuckDB WASM is significantly more complex than <code>localStorage</code> or <code>IndexedDB</code>. It involves asynchronous initialization, managing WASM bundles, understanding SQL, and potentially handling vector embeddings and similarity calculations. This example provides a conceptual structure.
                </p>

                 <div id="scenario-4-imports" class="mb-10">
                    <h3 class="text-xl font-medium mb-4 text-gray-800">6.1. Necessary Imports & Explanations</h3>
<pre><code class="language-typescript">// --- ART Storage Adapter Creation Imports ---
import {
  // The interface that a custom storage adapter must implement
  StorageAdapter,
  // Type defining options for querying data (filtering, sorting, limits)
  FilterOptions
} from 'art-framework';

// --- DuckDB WASM Imports ---
// You would typically install @duckdb/duckdb-wasm
import * as duckdb from '@duckdb/duckdb-wasm';
// Import specific types if needed, e.g., from duckdb-wasm
// import { AsyncDuckDB, AsyncDuckDBConnection } from '@duckdb/duckdb-wasm';

// --- Vector Embedding Imports (Conceptual) ---
// You would need a library or function to generate embeddings
// e.g., using Transformers.js or calling an embedding API
// import { pipeline } from '@xenova/transformers'; // Example
// const extractor = await pipeline('feature-extraction', 'Xenova/all-MiniLM-L6-v2');
// async function getEmbedding(text: string): Promise<number[]> {
//   const output = await extractor(text, { pooling: 'mean', normalize: true });
//   return Array.from(output.data as Float32Array);
// }
</code></pre>
                    <h4 class="text-lg font-medium mt-6 mb-4 text-gray-800">Explanation of Storage Adapter Imports:</h4>
                    <div class="space-y-4 text-gray-700 leading-relaxed">
                        <div>
                            <p><strong><code>StorageAdapter</code></strong></p>
                            <p class="mb-2">The blueprint for creating a custom way for ART to save and load its data (like chat history or agent memory) using DuckDB WASM.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                The interface your custom storage class must implement. Requires implementing methods for basic CRUD operations:
                                <ul class="list-disc list-inside ml-4">
                                    <li><code>async get&lt;T&gt;(collection: string, id: string): Promise&lt;T | null&gt;</code>: Retrieve a single item by ID.</li>
                                    <li><code>async set&lt;T&gt;(collection: string, id: string, data: T): Promise&lt;void&gt;</code>: Save (create or update) an item.</li>
                                    <li><code>async delete(collection: string, id: string): Promise&lt;void&gt;</code>: Delete an item by ID.</li>
                                    <li><code>async query&lt;T&gt;(collection: string, filterOptions: FilterOptions): Promise&lt;T[]&gt;</code>: Retrieve multiple items based on filter criteria.</li>
                                    <li>Optional: <code>async init?(config?: any): Promise&lt;void&gt;</code> (for async setup), <code>async clearCollection?(collection: string): Promise&lt;void&gt;</code>, <code>async clearAll?(): Promise&lt;void&gt;</code>. The implementation will translate these generic operations into DuckDB SQL commands executed via the WASM instance.</li>
                                </ul>
                            </details>
                        </div>
                         <div>
                            <p><strong><code>FilterOptions</code></strong></p>
                            <p class="mb-2">Describes how to filter, sort, or limit the data when asking the storage adapter for multiple items. Mapping this to SQL, especially for vector similarity, is the main challenge.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                Interface used as input for the <code>StorageAdapter.query</code> method. May include properties like:
                                <ul class="list-disc list-inside ml-4">
                                    <li><code>filters?: Array&lt;{ field: string; operator: string; value: any }&gt;</code>: Criteria to match items (e.g., <code>{ field: 'role', operator: '==', value: 'USER' }</code>).</li>
                                    <li><code>sortBy?: string</code>: Field name to sort by.</li>
                                    <li><code>sortDirection?: 'asc' | 'desc'</code>: Sorting order.</li>
                                    <li><code>limit?: number</code>: Maximum number of items to return.</li>
                                    <li><code>offset?: number</code>: Number of items to skip (for pagination).</li>
                                    <li>The <code>query</code> implementation in the DuckDB adapter will need to parse these options and construct appropriate <code>WHERE</code>, <code>ORDER BY</code>, and <code>LIMIT</code>/<code>OFFSET</code> clauses in SQL. Supporting complex filters or vector similarity searches (e.g., using a custom operator like <code>&lt;=&gt;</code> if using an extension, or calculating distance manually) requires specific logic.</li>
                                </ul>
                            </details>
                        </div>
                        <div>
                            <p><strong><code>@duckdb/duckdb-wasm</code></strong></p>
                            <p class="mb-2">The library providing the DuckDB WASM engine and browser integration.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                Used for initializing the database (<code>duckdb.selectBundle</code>, <code>db.instantiate</code>, <code>db.open</code>), establishing connections (<code>db.connect()</code>), and executing SQL queries (<code>connection.query()</code>, <code>connection.send()</code>, <code>connection.prepare()</code>, etc.). Requires careful handling of asynchronous initialization and potentially large WASM bundles. Consider using specific backends like OPFS (<code>db.registerFileURL</code>) for better persistence.
                            </details>
                        </div>
                        <div>
                            <p><strong>Vector Embedding Library (Conceptual)</strong></p>
                            <p class="mb-2">A library or function to convert text data (like conversation messages or state content) into numerical vectors (embeddings) for similarity search.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                Needed if implementing vector search capabilities. This is separate from DuckDB itself but crucial for the RAG use case. Embeddings would be generated before <code>set</code>ting data and used during <code>query</code> for similarity calculations. Libraries like <code>Transformers.js</code> can run embedding models client-side.
                            </details>
                        </div>
                    </div>
                 </div>

                 <div id="scenario-4-implementation" class="mb-10">
                    <h3 class="text-xl font-medium mb-4 text-gray-800">6.2. Implementing <code>DuckDBWasmAdapter</code> (Skeleton)</h3>
<pre><code class="language-typescript">// src/adapters/DuckDBWasmAdapter.ts
import { StorageAdapter, FilterOptions } from 'art-framework';
import * as duckdb from '@duckdb/duckdb-wasm';

// --- Vector Embedding Placeholder ---
async function getEmbedding(text: string): Promise&lt;number[]&gt; {
  // Placeholder: Replace with actual embedding generation
  console.warn("Using placeholder embedding function!");
  // Simple hash-based placeholder vector (NOT suitable for real use)
  let hash = 0;
  for (let i = 0; i &lt; text.length; i++) {
    hash = (hash &lt;&lt; 5) - hash + text.charCodeAt(i);
    hash |= 0; // Convert to 32bit integer
  }
  // Create a dummy vector based on the hash
  const vec = Array(16).fill(0); // Small dimension for example
  for(let i=0; i&lt;vec.length; i++) {
      vec[i] = (hash >> (i*2)) & 3; // Simple mapping
  }
  return vec;
}
// --- End Placeholder ---


// Define table schemas conceptually
const TABLE_SCHEMAS: Record&lt;string, string&gt; = {
  conversations: `(id VARCHAR PRIMARY KEY, threadId VARCHAR, role VARCHAR, content TEXT, timestamp BIGINT, embedding FLOAT[16])`, // Added embedding
  state: `(id VARCHAR PRIMARY KEY, threadId VARCHAR, config JSON, agentState JSON)`, // Store JSON directly
  observations: `(id VARCHAR PRIMARY KEY, threadId VARCHAR, traceId VARCHAR, type VARCHAR, timestamp BIGINT, content JSON, metadata JSON)`,
  // Add other collections as needed
};
const EMBEDDING_DIMENSION = 16; // Match the schema

export class DuckDBWasmAdapter implements StorageAdapter {
  private db: duckdb.AsyncDuckDB | null = null;
  private connection: duckdb.AsyncDuckDBConnection | null = null;
  private dbPath: string; // Path for storing DB file if using specific backend
  private initPromise: Promise&lt;void&gt; | null = null; // Prevent race conditions

  constructor(options: { dbPath?: string } = {}) {
      // dbPath might be used with specific backends like OPFS
      this.dbPath = options.dbPath || 'art_duckdb.db';
  }

  // Modified init to handle concurrent calls
  async init(): Promise&lt;void&gt; {
    if (!this.initPromise) {
        this.initPromise = this._initialize();
    }
    return this.initPromise;
  }

  private async _initialize(): Promise&lt;void&gt; {
    if (this.db) return; // Already initialized

    console.log("Initializing DuckDB WASM...");
    try {
      const JSDELIVR_BUNDLES = duckdb.getJsDelivrBundles();
      const bundle = await duckdb.selectBundle(JSDELIVR_BUNDLES);
      const worker_url = URL.createObjectURL(
        new Blob([`importScripts("${bundle.mainWorker!}");`], { type: 'text/javascript' })
      );
      const worker = new Worker(worker_url);
      const logger = new duckdb.ConsoleLogger(); // Or implement custom logger
      this.db = new duckdb.AsyncDuckDB(logger, worker);
      await this.db.instantiate(bundle.mainModule, bundle.pthreadWorker);
      URL.revokeObjectURL(worker_url);

      // Optional: Register a specific file persistence backend if needed (e.g., OPFS)
      // Requires specific browser support and setup
      // await this.db.registerFileURL(this.dbPath, `/${this.dbPath}`, duckdb.DuckDBDataProtocol.BROWSER_FSACCESS, false);

      await this.db.open({
          // path: this.dbPath, // Use if registered above
          query: {
              // Configure WASM specifics if needed, e.g., memory limits
              // initialMemory: '...',
          }
      });
      this.connection = await this.db.connect();
      console.log("DuckDB WASM Initialized and Connected.");

      // Ensure tables exist
      await this.ensureTables();

    } catch (error) {
      console.error("DuckDB WASM Initialization failed:", error);
      this.initPromise = null; // Reset promise on failure
      throw error;
    }
  }

  private async ensureTables(): Promise&lt;void&gt; {
      if (!this.connection) throw new Error("DuckDB connection not available.");
      console.log("Ensuring tables exist...");
      // Consider installing extensions like 'json' if not bundled
      // await this.connection.query(`INSTALL json; LOAD json;`);
      for (const [tableName, schema] of Object.entries(TABLE_SCHEMAS)) {
          try {
              await this.connection.query(`CREATE TABLE IF NOT EXISTS ${tableName} ${schema};`);
              console.log(`Table ${tableName} ensured.`);
          } catch(e) {
              console.error(`Failed to ensure table ${tableName}:`, e);
              throw e;
          }
      }
  }

  private async ensureConnection(): Promise&lt;duckdb.AsyncDuckDBConnection&gt; {
      await this.init(); // Ensure initialization is complete
      if (!this.connection) {
          throw new Error("Failed to establish DuckDB connection after init.");
      }
      return this.connection;
  }

  async get&lt;T&gt;(collection: string, id: string): Promise&lt;T | null&gt; {
    const conn = await this.ensureConnection();
    try {
      // Use prepared statements for safety
      const stmt = await conn.prepare(`SELECT * FROM ${collection} WHERE id = $1`);
      // Use arrow format for potentially better type handling with JSON
      const results = await stmt.query(id);
      await stmt.close(); // Close statement
      if (results.numRows > 0) {
        const row = results.get(0)?.toJSON();
        // DuckDB might return JSON columns as strings, parse them
        return this.parseJsonColumns(collection, row) as T;
      }
      return null;
    } catch (error) {
      console.error(`DuckDB get error in ${collection}:`, error);
      return null; // Or throw? Depends on desired error handling
    }
  }

  async set&lt;T extends { id: string, content?: string }&gt;(collection: string, id: string, data: T): Promise&lt;void&gt; {
    const conn = await this.ensureConnection();
    const schema = TABLE_SCHEMAS[collection];
    if (!schema) throw new Error(`Unknown collection: ${collection}`);

    // Prepare data for insertion (handle JSON, generate embedding)
    const values: any[] = [];
    const placeholders: string[] = [];
    const columns: string[] = [];

    let embedding: number[] | null = null;
    if (collection === 'conversations' && data.content && schema.includes('embedding')) {
        embedding = await getEmbedding(data.content); // Generate embedding
    }

    // Dynamically build based on schema and data properties
    // This is simplified; a real implementation needs robust mapping & type handling
    const columnDefs = schema.substring(1, schema.length - 1).split(',').map(s => s.trim().split(' ')[0]);

    for (const col of columnDefs) {
        if (col === 'embedding') {
            if (embedding) {
                columns.push(col);
                values.push(embedding); // DuckDB WASM might handle array types directly or need list_value syntax
                placeholders.push(`$${values.length}`);
            }
        } else if (col in data) {
            columns.push(col);
            let value = (data as any)[col];
            // Stringify JSON fields
            if (schema.includes(`${col} JSON`) && typeof value === 'object') {
                value = JSON.stringify(value);
            }
            values.push(value);
            placeholders.push(`$${values.length}`);
        } else if (col === 'id') { // Ensure ID is always included if not in data explicitly
             columns.push('id');
             values.push(id);
             placeholders.push(`$${values.length}`);
        }
    }


    const sql = `INSERT OR REPLACE INTO ${collection} (${columns.join(', ')}) VALUES (${placeholders.join(', ')})`;

    try {
      // Use prepared statements for insertion/replacement
      const stmt = await conn.prepare(sql);
      await stmt.send(...values); // Use send for operations not returning rows
      await stmt.close();
    } catch (error) {
      console.error(`DuckDB set error in ${collection}:`, error);
      throw error; // Re-throw to signal failure
    }
  }

  async delete(collection: string, id: string): Promise&lt;void&gt; {
    const conn = await this.ensureConnection();
    try {
      const stmt = await conn.prepare(`DELETE FROM ${collection} WHERE id = $1`);
      await stmt.send(id);
      await stmt.close();
    } catch (error) {
      console.error(`DuckDB delete error in ${collection}:`, error);
      throw error;
    }
  }

  async query&lt;T&gt;(collection: string, filterOptions: FilterOptions): Promise&lt;T[]&gt; {
    const conn = await this.ensureConnection();
    let sql = `SELECT * FROM ${collection}`;
    const params: any[] = [];
    let paramIndex = 1;

    // --- Basic Filtering ---
    if (filterOptions.filters && filterOptions.filters.length > 0) {
      const whereClauses = filterOptions.filters
        .map((filter) => {
            // Basic equality check - needs expansion for other operators
            if (filter.operator === '==') {
                params.push(filter.value);
                return `${filter.field} = $${paramIndex++}`;
            }
            // TODO: Add support for other operators like '!=', '>', '&lt;', 'in', etc.
            console.warn(`Unsupported filter operator: ${filter.operator}`);
            return null; // Ignore unsupported filters
        })
        .filter(clause => clause !== null); // Remove nulls from ignored filters

      if (whereClauses.length > 0) {
          sql += ` WHERE ${whereClauses.join(' AND ')}`;
      }
    }

    // --- Vector Similarity Search (Conceptual) ---
    // This requires a specific setup in DuckDB (e.g., vss extension)
    // or manual calculation. Let's assume a filter operator 'vector_similarity'.
    const vectorFilter = filterOptions.filters?.find(f => f.operator === 'vector_similarity');
    if (vectorFilter && collection === 'conversations' && TABLE_SCHEMAS[collection].includes('embedding')) {
        // Assuming vectorFilter.value is the query embedding (number[])
        // Assuming vectorFilter.field is 'embedding'
        const queryEmbedding = vectorFilter.value as number[];
        // Example using hypothetical list_dot_product (needs extension or UDF)
        // Or calculate distance manually if needed.
        // This SQL is conceptual and depends heavily on DuckDB setup.
        // sql = `SELECT *, list_dot_product(embedding, list_value(${queryEmbedding.join(',')})) AS similarity FROM ${collection}`;
        // sql += ` ORDER BY similarity DESC`; // Order by similarity
        console.warn("Vector similarity search requested but not fully implemented in this skeleton.");
        // Add placeholder WHERE clause if needed based on filtering logic
    } else {
        // --- Basic Sorting ---
        if (filterOptions.sortBy) {
            sql += ` ORDER BY ${filterOptions.sortBy} ${filterOptions.sortDirection === 'desc' ? 'DESC' : 'ASC'}`;
        }
    }


    // --- Pagination ---
    if (filterOptions.limit !== undefined) {
      sql += ` LIMIT $${paramIndex++}`;
      params.push(filterOptions.limit);
    }
    if (filterOptions.offset !== undefined) {
      sql += ` OFFSET $${paramIndex++}`;
      params.push(filterOptions.offset);
    }

    try {
      console.log("Executing DuckDB Query:", sql, params);
      const stmt = await conn.prepare(sql);
      const results = await stmt.query(...params);
      await stmt.close();
      // Parse JSON columns for all results
      return results.toArray().map(arrowRecord => this.parseJsonColumns(collection, arrowRecord.toJSON()) as T);
    } catch (error) {
      console.error(`DuckDB query error in ${collection}:`, error);
      return []; // Return empty on error, or re-throw
    }
  }

   async clearCollection(collection: string): Promise&lt;void&gt; {
       const conn = await this.ensureConnection();
       try {
           const stmt = await conn.prepare(`DELETE FROM ${collection}`);
           await stmt.send();
           await stmt.close();
       } catch (error) {
           console.error(`DuckDB clearCollection error for ${collection}:`, error);
           throw error;
       }
   }

   async clearAll(): Promise&lt;void&gt; {
       const conn = await this.ensureConnection();
       try {
           for (const tableName of Object.keys(TABLE_SCHEMAS)) {
               const stmt = await conn.prepare(`DELETE FROM ${tableName}`);
               await stmt.send();
               await stmt.close();
           }
       } catch (error) {
           console.error(`DuckDB clearAll error:`, error);
           throw error;
       }
   }

   // Helper to parse columns that should be JSON
   private parseJsonColumns(collection: string, row: any): any {
       if (!row) return null;
       const schema = TABLE_SCHEMAS[collection];
       if (!schema) return row;

       const jsonFields = ['config', 'agentState', 'content', 'metadata']; // Fields potentially stored as JSON strings
       for (const field of jsonFields) {
           // Check if schema defines field as JSON and if current value is string
           if (schema.includes(`${field} JSON`) && typeof row[field] === 'string') {
               try {
                   row[field] = JSON.parse(row[field]);
               } catch (e) {
                   console.warn(`Failed to parse JSON field ${field} in collection ${collection}`, e);
                   // Keep as string if parsing fails
               }
           }
       }
       return row;
   }

   async close(): Promise&lt;void&gt; {
       if (this.initPromise) {
           await this.initPromise; // Ensure init is done before closing
       }
       if (this.connection) {
           console.log("Closing DuckDB connection...");
           await this.connection.close();
           this.connection = null;
       }
       if (this.db) {
           console.log("Terminating DuckDB instance...");
           await this.db.terminate();
           this.db = null;
       }
       this.initPromise = null; // Reset init promise
       console.log("DuckDB terminated.");
   }
}
</code></pre>
                    <h4 class="text-lg font-medium mt-6 mb-4 text-gray-800">Explanation:</h4>
                    <ol class="list-decimal list-inside space-y-3 text-gray-700 leading-relaxed">
                        <li><strong>Implement <code>StorageAdapter</code>:** Fulfills the contract.</li>
                        <li><strong>DuckDB WASM Setup:**
                            <ul class="list-disc list-inside ml-6 space-y-2 mt-2">
                                <li>Imports <code>@duckdb/duckdb-wasm</code>.</li>
                                <li>The <code>init</code> method handles the complex asynchronous loading of the WASM bundle, worker instantiation, database opening, and connection establishment. Uses <code>initPromise</code> to prevent race conditions on concurrent calls.</li>
                                <li><code>ensureTables</code> creates the necessary tables (including conceptual <code>embedding</code> column) if they don't exist.</li>
                                <li><code>ensureConnection</code> is a helper to guarantee initialization.</li>
                            </ul>
                        </li>
                         <li><strong>CRUD Methods:**
                            <ul class="list-disc list-inside ml-6 space-y-2 mt-2">
                                <li>Translate operations into SQL using **prepared statements** (`prepare`, `query`, `send`) for security and efficiency.</li>
                                <li>Handles JSON stringification/parsing for relevant columns.</li>
                                <li>Includes conceptual embedding generation during <code>set</code> for the <code>conversations</code> table.</li>
                            </ul>
                        </li>
                        <li><strong><code>query</code> Method:**
                             <ul class="list-disc list-inside ml-6 space-y-2 mt-2">
                                <li>Constructs SQL <code>SELECT</code> query.</li>
                                <li>Maps simple equality filters from <code>FilterOptions</code> to <code>WHERE</code> clauses using parameterized queries.</li>
                                <li>Includes a *conceptual placeholder* for vector similarity search, noting its complexity and dependency on potential DuckDB extensions (like <code>vss</code>) or manual calculations.</li>
                                <li>Adds basic <code>ORDER BY</code>, <code>LIMIT</code>, and <code>OFFSET</code>.</li>
                                <li><strong>Limitation:</strong> Explicitly notes that complex filtering and efficient vector search are advanced topics.</li>
                            </ul>
                        </li>
                        <li><strong>Cleanup:** Includes an async <code>close</code> method to properly terminate the DB connection and worker, ensuring it waits for initialization if pending.</li>
                    </ol>
                 </div>

                 <div id="scenario-4-integration" class="mb-8">
                    <h3 class="text-xl font-medium mb-4 text-gray-800">6.3. Integrating the <code>DuckDBWasmAdapter</code></h3>
                    <p class="mb-4 text-gray-700 leading-relaxed">As explained in the State Management section, you can provide a pre-instantiated custom adapter to `createArtInstance`.</p>
                    <p class="mb-4 text-gray-700 leading-relaxed"><strong>Example Integration Snippet:**</p>
<pre><code class="language-typescript">// --- Integration Example ---
import { createArtInstance } from 'art-framework';
import { DuckDBWasmAdapter } from './adapters/DuckDBWasmAdapter'; // Your custom adapter
import { OpenAIAdapter } from 'art-framework'; // Or your custom provider
import { PESAgent } from 'art-framework';
import { CalculatorTool } from 'art-framework';
// ... other necessary imports

async function setupArtWithDuckDB(): Promise&lt;ArtInstance&gt; {
    // 1. Instantiate your custom storage adapter
    const duckDbAdapter = new DuckDBWasmAdapter(/* options */);
    // IMPORTANT: Initialize DuckDB WASM before passing to createArtInstance
    await duckDbAdapter.init();

    // 2. Define the rest of the configuration
    const config = {
        storage: duckDbAdapter, // Pass the initialized instance
        reasoning: {
            provider: 'openai',
            apiKey: 'YOUR_OPENAI_KEY',
            model: 'gpt-4o'
        },
        agentCore: PESAgent,
        tools: [new CalculatorTool()]
    };

    // 3. Create the ART instance
    const artInstance = await createArtInstance(config);

    // Add cleanup hook for DuckDB if your app lifecycle allows
    // window.addEventListener('beforeunload', () => duckDbAdapter.close());

    return artInstance;
}
</code></pre>
                    <h4 class="text-lg font-medium mt-6 mb-4 text-gray-800">How it Works Now:</h4>
                    <ul class="list-disc list-inside space-y-3 text-gray-700 leading-relaxed">
                        <li><strong>Node 1 (Developer Interface):** You define <code>DuckDBWasmAdapter</code>, instantiate it, and ensure its <code>init()</code> method is called. You pass this *instance* directly in the <code>storage</code> field of the configuration object given to <code>createArtInstance</code>.</li>
                        <li><strong>Node 2 (Core Orchestration):** The internal <code>StateRepository</code> and other repositories (like <code>ConversationRepository</code>) are configured by the factory to use your provided <code>DuckDBWasmAdapter</code> instance for all data operations. Calls like <code>ConversationManager.getMessages</code> or <code>StateManager.loadThreadContext</code> trigger the adapter's <code>query</code> or <code>get</code> methods, executing SQL against the WASM database.</li>
                        <li><strong>Node 3 (External Dependencies & Interactions):** Your adapter interacts with the DuckDB WASM engine, executing SQL commands to manage data stored potentially in the browser's file system (e.g., OPFS) or memory. If vector search is implemented, it also handles embedding generation and similarity calculations.</li>
                    </ul>
                 </div>
            </section>

            <section id="scenario-5" class="mb-16 p-6 bg-white rounded-lg shadow-lg fade-in-section">
                <h2 class="text-2xl md:text-3xl font-semibold mb-6 border-b pb-3 text-sky-700">7. Scenario 5: Adding a Custom Agent Pattern (Advanced Usage)</h2>
                <p class="mb-5 text-gray-700 leading-relaxed">
                    Let's implement the ReAct (Reason -> Act -> Observe) agent pattern and allow the user to switch between PES and ReAct in the chatbot UI.
                </p>
                <p class="mb-8 text-gray-700 leading-relaxed"><strong>Goal:</strong> Create a <code>ReActAgent</code> class, integrate it, and add UI controls for switching.</p>

                <div id="scenario-5-imports" class="mb-10">
                    <h3 class="text-xl font-medium mb-4 text-gray-800">7.1. Necessary Imports & Explanations</h3>
                    <p class="mb-4 text-gray-700 leading-relaxed">In addition to imports from previous scenarios, you need these for creating a custom agent core:</p>
<pre><code class="language-typescript">// --- ART Agent Core Creation Imports ---
import {
  // The interface that a custom agent core must implement
  IAgentCore,
  // Input properties for the process method
  AgentProps,
  // Output type for the process method
  AgentFinalResponse,
  // --- Interfaces for Dependencies Injected into the Agent Core ---
  // These define the components your custom agent will use internally
  StateManager,
  ConversationManager,
  ToolRegistry,
  PromptManager, // Handles prompt assembly using blueprints
  ReasoningEngine, // Returns AsyncIterable<StreamEvent>
  OutputParser, // Parses aggregated LLM output
  ObservationManager,
  ToolSystem,
  UISystem, // Needed for broadcasting stream events
  // Other needed types
  ToolSchema, ParsedToolCall, ToolResult, ArtStandardPrompt, StreamEvent, LLMMetadata, ExecutionMetadata, PromptContext
} from 'art-framework';
</code></pre>
                     <h4 class="text-lg font-medium mt-6 mb-4 text-gray-800">Explanation of Agent Core Imports:</h4>
                     <div class="space-y-4 text-gray-700 leading-relaxed">
                         <div>
                            <p><strong><code>IAgentCore</code></strong></p>
                            <p class="mb-2">The main blueprint for creating a new "thinking style" or reasoning process for the agent. If you want the agent to think differently than the default "Plan -> Use Tools -> Answer" style, you implement this.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                The core interface for custom agent logic. Your class must implement <code>IAgentCore</code>. Key requirements:
                                <ul class="list-disc list-inside ml-4">
                                    <li>Implement <code>async process(props: AgentProps): Promise&lt;AgentFinalResponse&gt;</code>: This method *is* your agent's brain. It receives the <code>AgentProps</code> (query, threadId, etc.) and must orchestrate all steps (loading data, assembling prompts via `PromptManager`, calling LLMs via `ReasoningEngine`, handling the `AsyncIterable<StreamEvent>` response, parsing output via `OutputParser`, calling tools via `ToolSystem`, saving data, logging observations, pushing stream events via `UISystem`) according to your custom logic (e.g., a ReAct loop) and return the final response including aggregated metadata.</li>
                                    <li>Define a <code>constructor</code> that accepts a single argument: an object containing instances of the necessary ART subsystems (dependencies) defined by the interfaces below (e.g., `constructor(private deps: { stateManager: StateManager, reasoningEngine: ReasoningEngine, uiSystem: UISystem, ... })`). The `AgentFactory` will automatically provide (inject) these dependencies when it instantiates your custom agent core based on the `config.agentCore` setting.</li>
                                </ul>
                            </details>
                        </div>
                         <div>
                            <p><strong>Dependency Interfaces (`StateManager`, `ConversationManager`, etc.)</strong></p>
                            <p class="mb-2">These are the built-in helpers and managers that ART gives to your custom agent brain so it doesn't have to reinvent everything (like how to talk to the LLM, use tools, remember history, log events, or broadcast UI updates). Your custom `process` method will use these helpers.</p>
                            <details>
                                <summary>Developer Notes</summary>
                                These interfaces define the contracts for the core ART subsystems injected into your <code>IAgentCore</code> constructor. You'll use their methods within your <code>process</code> implementation:
                                <ul class="list-disc list-inside ml-4">
                                    <li><code>StateManager</code>: Use `.loadThreadContext(threadId)` to get <code>ThreadConfig</code> and <code>AgentState</code>. Use `.saveStateIfModified(threadId)` to persist state changes. Use `.isToolEnabled(threadId, toolName)` for checks.</li>
                                    <li><code>ConversationManager</code>: Use `.getMessages(threadId, options)` to retrieve history. Use `.addMessages(threadId, messages)` to save new user/assistant messages.</li>
                                    <li><code>ToolRegistry</code>: Use `.getAvailableTools({ enabledForThreadId })` to get <code>ToolSchema[]</code> for prompt context. Use `.getToolExecutor(toolName)` if needed (though <code>ToolSystem</code> is usually preferred).</li>
                                    <li><code>PromptManager</code>: Now a stateless assembler. Use `.assemblePrompt(blueprint, context)` with your custom agent's blueprints and gathered <code>PromptContext</code> to create `ArtStandardPrompt` objects for the LLM.</li>
                                    <li><code>ReasoningEngine</code>: Use `.call(prompt, options)` to send an `ArtStandardPrompt` to the configured LLM. This method now returns a <code>Promise&lt;AsyncIterable&lt;StreamEvent&gt;&gt;</code>, which your agent must consume to process streaming responses. Remember to set `options.stream = true` and provide `options.callContext`.</li>
                                    <li><code>OutputParser</code>: Use `.parsePlanningOutput(...)`, `.parseSynthesisOutput(...)` (for PES-like flows) or potentially define/use custom methods to extract structured data (like thoughts, actions, final answers) from the LLM's raw response content (which your agent needs to buffer/aggregate from the stream).</li>
                                    <li><code>ObservationManager</code>: Use `.record(observationData)` frequently to log key steps (start/end, LLM calls, tool calls, custom steps like 'thought' or 'action', and new <code>LLM_STREAM_...</code> events) for debugging and UI feedback via sockets.</li>
                                    <li><code>ToolSystem</code>: Use `.executeTools(parsedToolCalls, threadId, traceId)` to run one or more tools identified by your agent's logic.</li>
                                    <li><code>UISystem</code>: Use `.getLLMStreamSocket()` to access the socket for broadcasting real-time <code>StreamEvent</code>s (received from the `ReasoningEngine`'s stream) to the UI. Also use `.getConversationSocket()` and `.getObservationSocket()`.</li>
                                </ul>
                            </details>
                        </div>
                    </div>
                </div>

                <div id="scenario-5-implementation" class="mb-10">
                    <h3 class="text-xl font-medium mb-4 text-gray-800">7.2. Implementing the <code>ReActAgent</code> (with Streaming)</h3>
                    <p class="mb-4 text-gray-700 leading-relaxed">This requires defining the ReAct loop logic within the <code>process</code> method, including handling the asynchronous stream from the `ReasoningEngine`.</p>
                    <pre><code class="language-typescript">// src/components/ArtChatbot.tsx
                        import React, { useState, useEffect, useRef, useCallback } from 'react';
                        // ... other imports from Scenario 1 ...
                        import { PESAgent, StreamEvent } from 'art-framework'; // Default agent & StreamEvent
                        import { ReActAgent } from '../agents/ReActAgent'; // Import custom agent (adjust path)
                        import { CurrentInfoTool } from '../tools/CurrentInfoTool'; // Import custom tool
                        
                        // --- Add Agent Type State ---
                        type AgentType = 'pes' | 'react';
                        
                        // Helper to generate temporary IDs
                        const tempId = () => `temp-${Date.now()}-${Math.random().toString(16).slice(2)}`;
                        
                        const ArtChatbot: React.FC = () => {
                          const [messages, setMessages] = useState&lt;ConversationMessage[]&gt;([]);
                          const [input, setInput] = useState('');
                          const [isLoading, setIsLoading] = useState(false);
                          const [status, setStatus] = useState&lt;string&gt;('Initializing...');
                          const [selectedAgent, setSelectedAgent] = useState&lt;AgentType&gt;('pes'); // State for agent choice
                          const artInstanceRef = useRef&lt;ArtInstance | null&gt;(null);
                          const messageListRef = useRef&lt;HTMLDivElement&gt;(null);
                          const threadId = 'web-chatbot-thread-1';
                          const [isInitializing, setIsInitializing] = useState(true); // Track initialization state
                          const streamingMessageIdRef = useRef&lt;string | null&gt;(null); // Track the ID of the message being streamed
                        
                          // --- Auto-scrolling ---
                          useEffect(() => {
                            if (messageListRef.current) {
                              messageListRef.current.scrollTop = messageListRef.current.scrollHeight;
                            }
                          }, [messages]);
                        
                          // --- ART Initialization (Modified for Agent Switching) ---
                          useEffect(() => {
                            let isMounted = true;
                            let unsubObservation: (() => void) | null = null;
                            let unsubConversation: (() => void) | null = null;
                            let unsubStream: (() => void) | null = null;
                        
                            const initializeArt = async () => {
                              // Clear previous instance and subscriptions
                              if (unsubObservation) unsubObservation();
                              if (unsubConversation) unsubConversation();
                              if (unsubStream) unsubStream();
                              artInstanceRef.current = null; // Helps ensure we don't use stale instance
                        
                              if (!isMounted) return;
                              setIsInitializing(true);
                              setStatus(`Initializing ${selectedAgent.toUpperCase()} Agent...`);
                        
                              try {
                                const AgentCoreClass = selectedAgent === 'react' ? ReActAgent : PESAgent;
                        
                                const config = {
                                  // Use different DB for different agents, or manage state carefully if shared
                                  storage: { type: 'indexedDB', dbName: `artWebChatHistory-${selectedAgent}` },
                                  reasoning: {
                                    provider: 'openai', // Or Anthropic, Gemini etc.
                                    apiKey: import.meta.env.VITE_OPENAI_API_KEY || 'YOUR_API_KEY',
                                    model: 'gpt-4o' // Ensure model supports chosen agent pattern well
                                  },
                                  agentCore: AgentCoreClass, // Dynamically set the agent core
                                  tools: [
                                      new CalculatorTool(),
                                      new CurrentInfoTool() // Include custom tool
                                  ]
                                  // logger: { level: 'debug' } // Optional: Enable detailed logging
                                };
                        
                                const instance = await createArtInstance(config);
                                if (!isMounted) return;
                        
                                artInstanceRef.current = instance;
                                setStatus('Loading history...');
                                await loadMessages(); // Reload messages for the new instance/config
                        
                                // --- Re-subscribe to Observations ---
                                setStatus('Connecting observers...');
                                const observationSocket = instance.uiSystem.getObservationSocket();
                                unsubObservation = observationSocket.subscribe(
                                    (observation: Observation) => {
                                       if (observation.threadId === threadId && isMounted && !isLoading) { // Avoid status flicker during processing
                                            let newStatus = status;
                                            switch (observation.type) {
                                                // ... (observation handling from Scenario 1, potentially add ReAct steps)
                                                case ObservationType.PROCESS_START: newStatus = 'Processing...'; break;
                                                case ObservationType.LLM_REQUEST: newStatus = 'Thinking...'; break;
                                                case ObservationType.TOOL_START: newStatus = `Using ${observation.metadata?.toolName}...`; break;
                                                case ObservationType.TOOL_END: newStatus = 'Tool finished.'; break;
                                                case ObservationType.PROCESS_END: newStatus = 'Ready.'; streamingMessageIdRef.current = null; break;
                                                case ObservationType.LLM_STREAM_END: newStatus = 'Receiving final response...'; break; // Update on stream end
                                                case ObservationType.LLM_STREAM_ERROR: newStatus = 'Stream error.'; streamingMessageIdRef.current = null; break;
                                                case 'thought' as ObservationType: newStatus = 'Thinking (ReAct)...'; break;
                                                case 'observation' as ObservationType: newStatus = 'Observing (ReAct)...'; break;
                                            }
                                            setStatus(newStatus);
                                       }
                                    },
                                    undefined, // Subscribe to all types for simplicity here
                                    { threadId: threadId }
                                );
                        
                                // --- Re-subscribe to Conversation (Final Messages) ---
                                 const conversationSocket = instance.uiSystem.getConversationSocket();
                                 unsubConversation = conversationSocket.subscribe(
                                     (message: ConversationMessage) => {
                                         if (message.threadId === threadId && isMounted) {
                                             console.log("Received final message via socket:", message);
                                             setMessages(prev => {
                                                 const existingIndex = prev.findIndex(m => m.id === streamingMessageIdRef.current && m.role === MessageRole.ASSISTANT);
                                                 if (existingIndex > -1) {
                                                     const updatedMessages = [...prev];
                                                     updatedMessages[existingIndex] = message; // Replace temp with final
                                                     return updatedMessages;
                                                 } else if (!prev.some(m => m.id === message.id)) {
                                                     return [...prev, message].sort((a, b) => a.timestamp - b.timestamp); // Add if new
                                                 }
                                                 return prev;
                                             });
                                             streamingMessageIdRef.current = null; // Clear streaming ID
                                         }
                                     },
                                     undefined, { threadId: threadId }
                                 );
                        
                                // --- Re-subscribe to LLM Stream ---
                                const streamSocket = instance.uiSystem.getLLMStreamSocket();
                                unsubStream = streamSocket.subscribe(
                                    (event: StreamEvent) => {
                                         if (event.threadId === threadId && isMounted) {
                                            if (event.type === 'TOKEN' &&
                                               (event.tokenType === 'FINAL_SYNTHESIS_LLM_RESPONSE' || // PES final answer
                                                event.tokenType?.startsWith('FINAL_SYNTHESIS')) // Catch potential thinking/response in final stage
                                               ) {
                                                 setMessages(prev => {
                                                     const currentStreamingId = streamingMessageIdRef.current;
                                                     if (!currentStreamingId) {
                                                         const newStreamingMessage: ConversationMessage = {
                                                             id: tempId(), role: MessageRole.ASSISTANT, content: event.data,
                                                             timestamp: Date.now(), threadId: threadId, metadata: { streaming: true }
                                                         };
                                                         streamingMessageIdRef.current = newStreamingMessage.id;
                                                         return [...prev, newStreamingMessage];
                                                     } else {
                                                         return prev.map(msg => msg.id === currentStreamingId ? { ...msg, content: msg.content + event.data } : msg);
                                                     }
                                                 });
                                            } else if (event.type === 'ERROR') {
                                                console.error("Stream Error:", event.data);
                                                setStatus('Stream Error');
                                                setMessages(prev => [...prev, { id: tempId(), role: MessageRole.SYSTEM, content: `Stream Error: ${event.data.message || event.data}`, timestamp: Date.now(), threadId: threadId }]);
                                                streamingMessageIdRef.current = null;
                                            }
                                            // END event is handled by PROCESS_END observation or ConversationSocket
                                         }
                                    },
                                    { threadId: threadId }
                                );
                        
                        
                                if (isMounted) setStatus('Ready.');
                        
                              } catch (error) {
                                console.error(`Failed to initialize ${selectedAgent.toUpperCase()} ART:`, error);
                                if (isMounted) setStatus(`Initialization Error: ${error instanceof Error ? error.message : 'Unknown error'}`);
                              } finally {
                                 if (isMounted) setIsInitializing(false);
                              }
                            };
                        
                            initializeArt(); // Initialize on mount and when selectedAgent changes
                        
                            // Cleanup function
                            return () => {
                              isMounted = false;
                              console.log("Cleaning up ART subscriptions...");
                              if (unsubObservation) unsubObservation();
                              if (unsubConversation) unsubConversation();
                              if (unsubStream) unsubStream();
                            };
                          }, [selectedAgent, threadId]); // Re-run useEffect when selectedAgent changes!
                        
                          // --- Load Messages ---
                          const loadMessages = useCallback(async () => {
                            setMessages([]); // Clear messages before loading
                            if (!artInstanceRef.current) return;
                             try {
                               // Use a local loading flag for history fetch to avoid interfering with process loading
                               // setIsLoading(true); // Removed this, rely on isInitializing
                               const history = await artInstanceRef.current.conversationManager.getMessages(threadId, { limit: 100 });
                               setMessages(history.sort((a, b) => a.timestamp - b.timestamp));
                             } catch (error) {
                               console.error("Failed to load messages:", error);
                               setStatus('Error loading history.');
                             } finally {
                               // setIsLoading(false); // Removed this
                             }
                          }, [threadId]); // Depends only on threadId now, called by useEffect
                        
                          // --- Handle Sending ---
                          const handleSend = useCallback(async () => {
                             if (!input.trim() || !artInstanceRef.current || isLoading || isInitializing) return;
                        
                             const userMessage: ConversationMessage = {
                               id: `user-${Date.now()}`, role: MessageRole.USER, content: input,
                               timestamp: Date.now(), threadId: threadId,
                             };
                             setMessages(prev => [...prev, userMessage]);
                             streamingMessageIdRef.current = null; // Reset streaming ID before sending
                        
                             const currentInput = input;
                             setInput('');
                             setIsLoading(true); // Indicate processing started
                             setStatus('Processing request...'); // Initial status
                        
                             try {
                               const props: AgentProps = { query: currentInput, threadId: threadId };
                               // Process call is now async but UI updates via sockets
                               const response: AgentFinalResponse = await artInstanceRef.current.process(props);
                               console.log("ART process completed. Final Response:", response);
                               // Final message display is handled by ConversationSocket subscription
                        
                             } catch (error) {
                               console.error("Error processing message:", error);
                               const errorMsg = error instanceof Error ? error.message : 'Failed to get response';
                               setMessages(prev => [...prev, { id: `error-${Date.now()}`, role: MessageRole.SYSTEM, content: `Error: ${errorMsg}`, timestamp: Date.now(), threadId: threadId }]);
                               setStatus('Error occurred.');
                                streamingMessageIdRef.current = null;
                             } finally {
                               setIsLoading(false); // Indicate processing finished
                               // Status should be updated to 'Ready' via PROCESS_END observation if successful
                             }
                          }, [input, isLoading, threadId, isInitializing, selectedAgent]); // Add selectedAgent dependency
                        
                          // --- Render Component ---
                          return (
                            &lt;div className="chatbot-container"&gt;
                              {/* Agent Switcher UI */}
                              &lt;div style={{ padding: '5px 10px', borderBottom: '1px solid #eee', textAlign: 'center' }}&gt;
                                &lt;label&gt;Agent Mode: &lt;/label&gt;
                                &lt;select value={selectedAgent} onChange={(e) => setSelectedAgent(e.target.value as AgentType)} disabled={isInitializing || isLoading}&gt;
                                  &lt;option value="pes"&gt;Plan-Execute-Synthesize&lt;/option&gt;
                                  &lt;option value="react"&gt;ReAct&lt;/option&gt;
                                &lt;/select&gt;
                                {(isInitializing || isLoading) && &lt;span style={{ marginLeft: '10px', fontSize: '0.8em' }}&gt;{status}...&lt;/span&gt;}
                              &lt;/div&gt;
                        
                              &lt;div className="message-list" ref={messageListRef}&gt;
                                 {messages.map((msg) => (
                                   &lt;div key={msg.id} className={`message ${msg.role} ${msg.metadata?.streaming ? 'streaming' : ''}`}&gt;
                                     &lt;pre style={{ whiteSpace: 'pre-wrap', margin: 0, fontFamily: 'inherit' }}&gt;{msg.content}&lt;/pre&gt;
                                   &lt;/div&gt;
                                 ))}
                              &lt;/div&gt;
                              &lt;div className="status-indicator"&gt;{isInitializing ? 'Initializing...' : (isLoading ? status : 'Ready.')}&lt;/div&gt;
                              &lt;div className="input-area"&gt;
                                &lt;input
                                  type="text"
                                  value={input}
                                  onChange={(e) => setInput(e.target.value)}
                                  onKeyPress={(e) => e.key === 'Enter' && !isLoading && !isInitializing && handleSend()}
                                  disabled={isLoading || isInitializing || !artInstanceRef.current}
                                  placeholder={isInitializing ? 'Initializing...' : (artInstanceRef.current ? "Ask something..." : "Error - Init failed")}
                                /&gt;
                                &lt;button
                                  onClick={handleSend}
                                  disabled={isLoading || isInitializing || !artInstanceRef.current || !input.trim()}
                                &gt;
                                  {isLoading ? '...' : 'Send'}
                                &lt;/button&gt;
                              &lt;/div&gt;
                            &lt;/div&gt;
                          );
                        };
                        
                        export default ArtChatbot;
                        </code></pre>
                    <h4 class="text-lg font-medium mt-6 mb-4 text-gray-800">Explanation of Changes:</h4>
                     <ol class="list-decimal list-inside space-y-3 text-gray-700 leading-relaxed">
                         <li><strong>State for Agent Type:** Added <code>selectedAgent</code> state (<code>'pes'</code> or <code>'react'</code>).</li>
                         <li><strong>Agent Switcher UI:** Added a <code>&lt;select&gt;</code> dropdown to allow the user to change the <code>selectedAgent</code> state.</li>
                         <li><strong>Dynamic Initialization:** The main <code>useEffect</code> hook now has <code>selectedAgent</code> in its dependency array. This means whenever <code>selectedAgent</code> changes, the effect re-runs:
                             <ul class="list-disc list-inside ml-6 space-y-2 mt-2">
                                 <li>It determines the correct <code>AgentCoreClass</code> (<code>PESAgent</code> or <code>ReActAgent</code>) based on the state.</li>
                                 <li>It calls <code>createArtInstance</code> with the chosen <code>agentCore</code>.</li>
                                 <li>It reloads messages (potentially from a different DB if configured, or clears the list for the new agent context).</li>
                                 <li>It re-subscribes to the `ObservationSocket`, `ConversationSocket`, and `LLMStreamSocket` for the new instance.</li>
                             </ul>
                         </li>
                         <li><strong>Loading/Disabled States:** Added <code>isInitializing</code> state and updated `disabled` conditions on input/button/select to prevent interaction while ART is initializing or processing.</li>
                         <li><strong>Stream Handling in UI:**
                             <ul class="list-disc list-inside ml-6 space-y-2 mt-2">
                                 <li>Subscribes to `LLMStreamSocket`.</li>
                                 <li>Uses `streamingMessageIdRef` to track the temporary ID of the message being built from tokens.</li>
                                 <li>On receiving a `TOKEN` event (of the correct `tokenType`), it either creates a new temporary assistant message or appends the token to the existing one.</li>
                                 <li>On receiving the final message via `ConversationSocket`, it replaces the temporary message with the final, persisted one using the ID.</li>
                             </ul>
                          </li>
                     </ol>
                    <h4 class="text-lg font-medium mt-6 mb-4 text-gray-800">How it Works Now:</h4>
                    <ul class="list-disc list-inside space-y-3 text-gray-700 leading-relaxed">
                        <li><strong>Node 1 (Developer Interface):** You've defined the <code>ReActAgent</code> and provided UI controls (<code>&lt;select&gt;</code>) to change the <code>selectedAgent</code> state. The <code>useEffect</code> hook dynamically sets the <code>agentCore</code> in the <code>config</code> based on this state before calling <code>createArtInstance</code>.</li>
                        <li><strong>Node 2 (Core Orchestration):** When the user selects an agent type, <code>createArtInstance</code> builds the internal engine using the *specified* <code>IAgentCore</code> implementation. When <code>art.process()</code> is called, the framework routes the call to the currently active agent core's `process` method. This method now handles the streaming response from the `ReasoningEngine`, pushes `StreamEvent`s to the `UISystem`'s `LLMStreamSocket`, buffers the final response, saves it, and returns the `AgentFinalResponse`.</li>
                        <li><strong>Node 3 (External Dependencies & Interactions):** The `ProviderAdapter` handles the streaming communication with the LLM. Tools are invoked based on the logic within the active `IAgentCore` implementation (PES or ReAct).</li>
                    </ul>
                </div>
            </section>

            <section id="conclusion" class="mb-16 p-6 bg-white rounded-lg shadow-lg fade-in-section">
                <h2 class="text-2xl md:text-3xl font-semibold mb-6 border-b pb-3 text-sky-700">8. Conclusion</h2>
                <p class="text-gray-700 leading-relaxed">
                    ART offers a layered approach to building browser-based AI agents. Developers can start simply by configuring built-in components, progress to extending capabilities with custom tools and adapters, and finally achieve deep customization by implementing entirely new agent reasoning patterns. Recent advancements, such as the flexible prompt management system using blueprints and the implementation of real-time LLM response streaming, further enhance ART's capabilities. By understanding the 3-node architecture and the different usage levels, you can effectively leverage ART to create powerful and flexible client-side AI applications.
                </p>
            </section>

        </main>
    </div>

    <script>
        // Initialize Mermaid
        mermaid.initialize({ startOnLoad: true });

        // Initialize Highlight.js
        hljs.highlightAll();

        // Sidebar Active Link Highlighting
        const sections = document.querySelectorAll('main section[id], main div[id]'); // Include divs with IDs for subsections
        const navLinks = document.querySelectorAll('.sidebar-link');
        const subNavLinks = document.querySelectorAll('.sidebar ul ul a'); // Select sub-links too

        function changeActiveLink() {
            let index = sections.length;
            let currentId = '';

            // Find the topmost section currently visible
             while(--index >= 0 && window.scrollY + 100 < sections[index].offsetTop) {}

             if (index >= 0) {
                currentId = sections[index].id;
             }

            // Remove active class from all links first
            navLinks.forEach((link) => link.classList.remove('active'));
            subNavLinks.forEach((link) => link.classList.remove('active'));

            if (currentId) {
                let activeLink = document.querySelector(`.sidebar-link[href="#${currentId}"], .sidebar ul ul a[href="#${currentId}"]`);

                 // If the active element is a subsection (e.g., scenario-1-imports), highlight its parent too
                 if (activeLink && activeLink.closest('ul ul')) { // Check if it's a sub-link
                     let parentLi = activeLink.closest('ul.ml-4')?.closest('li');
                     let parentLink = parentLi?.querySelector('a.sidebar-link');
                     if (parentLink) {
                         parentLink.classList.add('active');
                     }
                 } else if (activeLink && activeLink.classList.contains('sidebar-link')) {
                      // If it's a main section link, ensure no sublinks under it are active unless it's the specific one
                 }

                 // Add active class to the specific link found
                 if (activeLink) {
                     activeLink.classList.add('active');
                 }
            }
        }


        // Initial check and add listener
        changeActiveLink();
        window.addEventListener('scroll', changeActiveLink);

         // Smooth scroll for sidebar links (already handled by CSS `scroll-behavior: smooth;`)

         // Fade-in sections on scroll (Simple version)
         const observer = new IntersectionObserver((entries) => {
             entries.forEach(entry => {
                 if (entry.isIntersecting) {
                     entry.target.classList.add('visible'); // You might need to adjust CSS for .visible
                 }
             });
         }, { threshold: 0.1 }); // Trigger when 10% visible

         document.querySelectorAll('.fade-in-section').forEach(section => {
             observer.observe(section);
         });

    </script>
</body>
</html>