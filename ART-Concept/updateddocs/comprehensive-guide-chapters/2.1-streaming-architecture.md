## 2.1. Core Concept: Real-time Streaming Architecture

To provide a more responsive and interactive user experience, ART incorporates a real-time streaming architecture for handling LLM responses. Instead of waiting for the entire response, the UI can receive and display tokens as soon as the LLM generates them.

**Key Components:**

*   **`ReasoningEngine.call` returning `AsyncIterable<StreamEvent>`:**
    *   **Non-Developer Explanation:** Instead of the agent's "brain" waiting for the LLM's complete answer, it now gets a "conveyor belt" (`AsyncIterable`). Pieces of the answer (`StreamEvent` objects) arrive on the belt one by one as the LLM thinks and writes.
    *   **Developer Notes:** The core `ReasoningEngine` interface's `call` method now returns a `Promise` resolving to an `AsyncIterable`. This iterable yields `StreamEvent` objects, allowing the consuming code (typically the `AgentCore`) to process tokens, metadata, errors, and end signals asynchronously as they arrive from the `ProviderAdapter`.

*   **`StreamEvent` Interface:**
    *   **Non-Developer Explanation:** Each item on the conveyor belt has a label (`StreamEvent`) saying what it is: a piece of text (`TOKEN`), statistics (`METADATA`), an error (`ERROR`), or the end signal (`END`). Text tokens also have a sub-label (`tokenType`) indicating if it's part of the LLM's internal "thinking" process or the final "response".
    *   **Developer Notes:** This interface (detailed in Section 3.1) standardizes the data flowing from the LLM stream. The `type` field is crucial for routing, and the `tokenType` field enables differentiating intermediate reasoning steps from the final output meant for the user. Adapters are responsible for correctly populating these fields based on provider-specific stream formats and the `callContext` option.

*   **`LLMStreamSocket` (`UISystem`):**
    *   **Non-Developer Explanation:** ART uses an announcement system (sockets) for different parts to communicate, especially with the UI. A new, dedicated channel (`LLMStreamSocket`) was added specifically for broadcasting the live stream events (tokens, metadata, errors, end signals) from the LLM conveyor belt to any UI components listening.
    *   **Developer Notes:** Accessed via `artInstance.uiSystem.getLLMStreamSocket()`. The `AgentCore` consumes the `AsyncIterable` from the `ReasoningEngine` and pushes each `StreamEvent` to this socket. UI components subscribe to this socket to receive real-time updates, decoupling the UI from the stream source and providing a consistent subscription pattern (`socket.subscribe(...)`).

**Flow Overview:**

1.  **UI/App:** Calls `artInstance.process(props)`.
2.  **Agent Core (`PESAgent`, etc.):** Calls `reasoningEngine.call(prompt, { stream: true, callContext: '...' })`.
3.  **Reasoning Engine (Adapter - e.g., `OpenAIAdapter`):** Makes a streaming request to the LLM provider API.
4.  **Adapter:** Receives stream chunks, parses them, determines `tokenType`, and `yield`s `StreamEvent` objects via the `AsyncIterable`.
5.  **Agent Core:** Consumes the `AsyncIterable` using `for await...of`.
6.  **Agent Core:** For each `StreamEvent`:
    *   Pushes the event to `uiSystem.getLLMStreamSocket().notify(event)`.
    *   If it's a final response `TOKEN`, appends it to an internal buffer.
    *   If it's `METADATA`, `ERROR`, or `END`, records it via `ObservationManager`.
    *   Aggregates `METADATA`.
7.  **UI:** Receives `StreamEvent`s via its `LLMStreamSocket` subscription and updates the display in real-time.
8.  **Agent Core (After Stream Ends):** Constructs the final `ConversationMessage` from the buffer, saves it via `ConversationManager`, and returns the `AgentFinalResponse` containing the final message and aggregated `ExecutionMetadata` (including `llmMetadata`).
9.  **UI:** (Optional but recommended) Receives the final `ConversationMessage` via `ConversationSocket` subscription and replaces the temporary streamed message with the final, persisted one to ensure consistency.

**Thinking vs. Response Tokens (`tokenType`):**

*   The `StreamEvent.tokenType` field allows distinguishing between tokens generated during intermediate reasoning steps (e.g., `AGENT_THOUGHT_LLM_RESPONSE`) and tokens forming the final user-facing answer (e.g., `FINAL_SYNTHESIS_LLM_RESPONSE`).
*   Adapters determine the `LLM_THINKING` vs `LLM_RESPONSE` part based on provider-specific markers (if available).
*   The Agent Core provides the `AGENT_THOUGHT` vs `FINAL_SYNTHESIS` context via `CallOptions.callContext`.
*   The UI can use `tokenType` to visually differentiate these tokens (e.g., showing thinking steps faded or in a separate area).

**Metadata Delivery (`LLMMetadata`):**

*   Detailed LLM statistics (token counts, timing) are packaged into `LLMMetadata` objects.
*   Adapters yield these as `METADATA` `StreamEvent`s (either during the stream if the provider supports it, or after the stream ends based on the final response/usage info).
*   These events are broadcast via `LLMStreamSocket` for potential real-time display.
*   They are also logged as discrete observations (`LLM_STREAM_METADATA`) via `ObservationManager`.
*   Finally, the metadata from all relevant LLM calls within an execution cycle is aggregated and included in the `AgentFinalResponse.metadata.llmMetadata` field.