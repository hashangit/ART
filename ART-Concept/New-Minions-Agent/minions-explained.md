# Minions Protocol: How it Works

The Minions protocol is designed to make large language model (LLM) tasks involving long documents more cost-effective by leveraging collaboration between a small, local LLM (running on your device) and a large, powerful remote LLM (in the cloud). The key idea is to minimize the expensive processing done by the remote model by having the local model handle the bulk of the document reading.

Here's a breakdown of the steps involved, based on the paper:
1. **Problem Setup:** You have a query (e.g., "Find the depreciation margin in this financial report") and a long context document (the report itself) stored locally. The goal is to get an answer without sending the entire large document to the expensive remote LLM.
2. **Motivation:** A simple chat (like the "Minion" protocol mentioned) between the local and remote models isn't always efficient. Small local models often struggle with complex, multi-step instructions or finding specific information in very long texts when directed by the remote model.
3. **Minions Approach (Divide and Conquer):** The Minions protocol addresses these issues with a structured, iterative process:
    - **Step 1: Job Preparation (Remote LM)**
        - The remote LLM receives the query but not the full document.
        - Instead of reading the document, it writes Python code. This code defines how to break the main query down into smaller, simpler sub-tasks ("jobs").
        - Each job typically involves a simple instruction (e.g., "Extract total revenue for FY2015") to be applied to a specific chunk of the document.
        - Crucially, the code specifies how to chunk the document (e.g., by page, by paragraph) and what instruction to run on each chunk. This code generation step is cheap for the remote model as it doesn't process the large document content.
        - If this isn't the first round, the remote model can use results from the previous round to generate more targeted jobs (e.g., focusing on chunks identified as relevant earlier).
    - **Step 2: Job Execution & Filtering (Local LM)**
        - The Python code generated by the remote LLM is sent to the local device.
        - The local LLM executes this code. This involves:
          - Loading the full document (which it has access to locally).
          - Chunking the document according to the received code's instructions.
          - Running the specified simple instruction (job) on each chunk in parallel.
        - For each chunk, the local LLM attempts the job. If a chunk is irrelevant to the job's instruction, the local LLM "abstains" (provides no result for that chunk).
        - The results from relevant chunks (often including the extracted text/answer and maybe a citation/explanation) are collected. The results from abstained jobs are discarded (filtered out). This filtering is key to reducing the amount of data sent back to the remote model.
    - **Step 3: Job Aggregation (Remote LM)**
        - The filtered, relevant results from the local LLM are sent back to the remote LLM. This payload is much smaller than the original document.
        - The remote LLM analyzes these aggregated results.
        - It decides if it has enough information to answer the original query.
        - **If Yes:** It synthesizes the final answer and the process ends.
        - **If No:** It determines what additional information is needed and loops back to Step 1 to generate new, potentially more refined, jobs for the local LLM to execute. This loop continues until an answer is found or a maximum number of rounds is reached.

In essence, Minions uses the powerful remote LLM for strategic planning (decomposing the task and synthesizing results) and the efficient local LLM for the heavy lifting (parallel processing of document chunks), significantly reducing the data sent to and processed by the costly remote model.
