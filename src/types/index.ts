// src/types/index.ts

/**
 * Represents the role of a message sender in a conversation.
 */
export enum MessageRole {
  USER = 'USER',
  AI = 'AI',
  SYSTEM = 'SYSTEM', // Added for system prompts, though not explicitly in checklist message interface
  TOOL = 'TOOL',     // Added for tool results, though not explicitly in checklist message interface
}

/**
 * Represents a single message within a conversation thread.
 */
export interface ConversationMessage {
  /** A unique identifier for this specific message. */
  messageId: string;
  /** The identifier of the conversation thread this message belongs to. */
  threadId: string;
  /** The role of the sender (User, AI, System, or Tool). */
  role: MessageRole;
  /** The textual content of the message. */
  content: string;
  /** A Unix timestamp (in milliseconds) indicating when the message was created. */
  timestamp: number;
  /** Optional metadata associated with the message (e.g., related observation IDs, tool call info, UI state). */
  metadata?: Record<string, any>;
}

/**
 * Represents the type of an observation record.
 */
export enum ObservationType {
  INTENT = 'INTENT',
  PLAN = 'PLAN',
  THOUGHTS = 'THOUGHTS',
  /** Records the LLM's decision to call one or more tools (part of the plan). */
  TOOL_CALL = 'TOOL_CALL',
  /** Records the actual execution attempt and result of a specific tool call. */
  TOOL_EXECUTION = 'TOOL_EXECUTION',
  /** Records events specifically related to the synthesis phase (e.g., the LLM call). */
  SYNTHESIS = 'SYNTHESIS',
  /** Records an error encountered during any phase of execution. */
  ERROR = 'ERROR',
  /** Records the final AI response message generated by the agent. */
  FINAL_RESPONSE = 'FINAL_RESPONSE',
  /** Records changes made to the agent's persistent state. */
  STATE_UPDATE = 'STATE_UPDATE',
}

/**
 * Represents a recorded event during the agent's execution.
 */
export interface Observation {
  /** A unique identifier for this specific observation record. */
  id: string;
  /** The identifier of the conversation thread this observation relates to. */
  threadId: string;
  /** An optional identifier for tracing a request across multiple systems or components. */
  traceId?: string;
  /** A Unix timestamp (in milliseconds) indicating when the observation was recorded. */
  timestamp: number;
  /** The category of the event being observed (e.g., PLAN, THOUGHTS, TOOL_EXECUTION). */
  type: ObservationType;
  /** A concise, human-readable title summarizing the observation (often generated based on type/metadata). */
  title: string;
  /** The main data payload of the observation, structure depends on the `type`. */
  content: any;
  /** Optional metadata providing additional context (e.g., source phase, related IDs, status). */
  metadata?: Record<string, any>;
}

/**
 * Represents a basic JSON Schema definition, focusing on object types commonly used for tool inputs/outputs.
 * This is a simplified representation and doesn't cover all JSON Schema features.
 */
export interface JsonObjectSchema {
  type: 'object';
  properties: {
    [key: string]: {
      type: string; // e.g., 'string', 'number', 'boolean', 'object', 'array'
      description?: string;
      default?: any;
      items?: JsonObjectSchema | { type: string }; // For array type
      properties?: JsonObjectSchema['properties']; // For nested object type
      required?: string[]; // For nested object type
      additionalProperties?: boolean | { type: string };
      [key: string]: any; // Allow other JSON schema properties
    };
  };
  required?: string[];
  additionalProperties?: boolean;
}

// Allow for other schema types (string, number, etc.) although object is most common for tools
export type JsonSchema = JsonObjectSchema | { type: 'string' | 'number' | 'boolean' | 'array', [key: string]: any };


/**
 * Defines the schema for a tool, including its input parameters.
 * Uses JSON Schema format for inputSchema.
 */
export interface ToolSchema {
  /** A unique name identifying the tool (used in LLM prompts and registry lookups). Must be unique. */
  name: string;
  /** A clear description of what the tool does, intended for the LLM to understand its purpose and usage. */
  description: string;
  /** A JSON Schema object defining the structure, types, and requirements of the input arguments the tool expects. */
  inputSchema: JsonSchema;
  /** An optional JSON Schema object defining the expected structure of the data returned in the `output` field of a successful `ToolResult`. */
  outputSchema?: JsonSchema;
  /** Optional array of examples demonstrating how to use the tool, useful for few-shot prompting of the LLM. */
  examples?: Array<{ input: any; output?: any; description?: string }>;
}

/**
 * Represents the structured result of a tool execution.
 */
export interface ToolResult {
  /** The unique identifier of the corresponding `ParsedToolCall` that initiated this execution attempt. */
  callId: string;
  /** The name of the tool that was executed. */
  toolName: string;
  /** Indicates whether the tool execution succeeded or failed. */
  status: 'success' | 'error';
  /** The data returned by the tool upon successful execution. Structure may be validated against `outputSchema`. */
  output?: any;
  /** A descriptive error message if the execution failed (`status` is 'error'). */
  error?: string;
  /** Optional metadata about the execution (e.g., duration, cost, logs). */
  metadata?: Record<string, any>;
}

/**
 * Represents a parsed request from the LLM to call a specific tool.
 */
export interface ParsedToolCall {
  /** A unique identifier generated by the OutputParser for this specific tool call request within a plan. */
  callId: string;
  /** The name of the tool the LLM intends to call. Must match a registered tool's schema name. */
  toolName: string;
  /** The arguments object, parsed from the LLM response, intended to be passed to the tool's `execute` method after validation. */
  arguments: any;
}

/**
 * Configuration specific to a conversation thread.
 */
export interface ThreadConfig {
  /** Configuration for the Reasoning System for this thread. */
  reasoning: {
    /** Identifier for the primary LLM provider adapter to use (e.g., 'openai', 'anthropic'). */
    provider: string;
    /** The specific model identifier to use with the provider (e.g., 'gpt-4o', 'claude-3-5-sonnet-20240620'). */
    model: string;
    /** Optional provider-specific parameters (e.g., temperature, max_tokens, top_p). */
    parameters?: Record<string, any>;
    // TODO: Potentially add prompt template overrides here
  };
  /** An array of tool names (matching `ToolSchema.name`) that are permitted for use within this thread. */
  enabledTools: string[];
  /** The maximum number of past messages (`ConversationMessage` objects) to retrieve for context. */
  historyLimit: number;
  /** An optional system prompt string that overrides any default system prompt for this thread. */
  systemPrompt?: string;
  // TODO: Add other potential thread-specific settings (e.g., RAG configuration, default timeouts)
}

/**
 * Represents non-configuration state associated with an agent or thread.
 * Could include user preferences, accumulated knowledge, etc. (Less defined for v1.0)
 */
export interface AgentState {
  /** A flexible object to store persistent, non-configuration data associated with a thread or user (e.g., preferences, summaries, intermediate results). Structure is application-defined. */
  [key: string]: any;
}

/**
 * Encapsulates the configuration and state for a specific thread.
 */
export interface ThreadContext {
  /** The configuration settings (`ThreadConfig`) currently active for the thread. */
  config: ThreadConfig;
  /** The persistent state (`AgentState`) associated with the thread, or `null` if no state exists. */
  state: AgentState | null;
}

/**
 * Properties required to initiate an agent processing cycle.
 */
export interface AgentProps {
  /** The user's input query or request to the agent. */
  query: string;
  /** The mandatory identifier for the conversation thread. All context is scoped to this ID. */
  threadId: string;
  /** An optional identifier for the specific UI session, useful for targeting UI updates. */
  sessionId?: string;
  /** An optional identifier for the user interacting with the agent. */
  userId?: string;
  /** An optional identifier used for tracing a request across multiple systems or services. */
  traceId?: string;
  /** Optional runtime options that can override default behaviors for this specific `process` call. */
  options?: AgentOptions;
  // Note: Core dependencies (StateManager, ConversationManager, etc.) are typically injected
  // during `createArtInstance` and are accessed internally by the Agent Core, not passed in AgentProps.
}

/**
 * Options to override agent behavior at runtime.
 */
export interface AgentOptions {
  /** Override specific LLM parameters (e.g., temperature, max_tokens) for this call only. */
  llmParams?: Record<string, any>;
  /** Force the use of specific tools, potentially overriding the thread's `enabledTools` for this call (use with caution). */
  forceTools?: string[];
  /** Specify a particular reasoning model to use for this call, overriding the thread's default. */
  overrideModel?: { provider: string; model: string };
  // TODO: Add other potential runtime overrides (e.g., specific system prompt, history length).
}

/**
 * The final structured response returned by the agent core after processing.
 */
export interface AgentFinalResponse {
  /** The final `ConversationMessage` generated by the AI, which has also been persisted. */
  response: ConversationMessage;
  /** Metadata summarizing the execution cycle that produced this response. */
  metadata: ExecutionMetadata;
}

/**
 * Metadata summarizing an agent execution cycle.
 */
export interface ExecutionMetadata {
  /** The thread ID associated with this execution cycle. */
  threadId: string;
  /** The trace ID used during this execution, if provided. */
  traceId?: string;
  /** The user ID associated with this execution, if provided. */
  userId?: string;
  /** The overall status of the execution ('success', 'error', or 'partial' if some steps failed but a response was generated). */
  status: 'success' | 'error' | 'partial';
  /** The total duration of the `agent.process()` call in milliseconds. */
  totalDurationMs: number;
  /** The number of calls made to the `ReasoningEngine`. */
  llmCalls: number;
  /** The number of tool execution attempts made by the `ToolSystem`. */
  toolCalls: number;
  /** An optional estimated cost for the LLM calls made during this execution. */
  llmCost?: number;
  /** A top-level error message if the overall status is 'error' or 'partial'. */
  error?: string;
}

/**
 * Context provided to a tool during its execution.
 */
export interface ExecutionContext {
  /** The ID of the thread in which the tool is being executed. */
  threadId: string;
  /** The trace ID for this execution cycle, if available. */
  traceId?: string;
  /** The user ID associated with the execution, if available. */
  userId?: string;
  // TODO: Potentially include access tokens or credentials scoped to this execution, if needed securely.
  // TODO: Consider providing limited access to StateManager or other relevant context if required by complex tools.
}

/**
 * Options for configuring an LLM call.
 */
export interface CallOptions {
  /** The mandatory thread ID, used by the ReasoningEngine to fetch thread-specific configuration (e.g., model, params) via StateManager. */
  threadId: string;
  /** Optional trace ID for correlation. */
  traceId?: string;
  /** Optional user ID. */
  userId?: string;
  /** Optional session ID. */
  sessionId?: string; // Added sessionId
  /** An optional callback function invoked when the LLM streams intermediate 'thoughts' or reasoning steps. */
  onThought?: (thought: string) => void;
  /** Additional key-value pairs representing provider-specific parameters (e.g., `temperature`, `max_tokens`, `model`). These often override defaults set in `ThreadConfig`. */
  [key: string]: any;
}

/**
 * Represents the prompt data formatted for a specific LLM provider.
 * Can be a simple string or a complex object (e.g., for OpenAI Chat Completion API).
 */
export type FormattedPrompt = string | object | Array<object>;

/**
 * Options for filtering data retrieved from storage.
 * Structure depends heavily on the underlying adapter's capabilities.
 */
export interface FilterOptions {
  /** An object defining filter criteria (e.g., `{ threadId: 'abc', type: 'TOOL_EXECUTION' }`). Structure may depend on adapter capabilities. */
  filter?: Record<string, any>;
  /** An object defining sorting criteria (e.g., `{ timestamp: 'desc' }`). */
  sort?: Record<string, 'asc' | 'desc'>;
  /** The maximum number of records to return. */
  limit?: number;
  /** The number of records to skip (for pagination). */
  skip?: number;
  // TODO: Consider adding projection options to retrieve only specific fields.
}

/**
 * Options for retrieving conversation messages.
 */
export interface MessageOptions {
  /** The maximum number of messages to retrieve. */
  limit?: number;
  /** Retrieve messages created before this Unix timestamp (milliseconds). */
  beforeTimestamp?: number;
  /** Retrieve messages created after this Unix timestamp (milliseconds). */
  afterTimestamp?: number;
  /** Optionally filter messages by role (e.g., retrieve only 'AI' messages). */
  roles?: MessageRole[];
}

/**
 * Options for filtering observations.
 */
export interface ObservationFilter {
  /** An array of `ObservationType` enums to filter by. If provided, only observations matching these types are returned. */
  types?: ObservationType[];
  /** Retrieve observations recorded before this Unix timestamp (milliseconds). */
  beforeTimestamp?: number;
  /** Retrieve observations recorded after this Unix timestamp (milliseconds). */
  afterTimestamp?: number;
  // TODO: Add other potential criteria like filtering by metadata content if needed.
}