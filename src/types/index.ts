// src/types/index.ts
import type { RuntimeProviderConfig } from './providers'; // Import for use within this file
import type { IToolExecutor, IAgentCore } from '../core/interfaces'; // For ArtInstanceConfig
import type { LogLevel } from '../utils/logger'; // For ArtInstanceConfig
import type { StorageAdapter } from '../core/interfaces'; // For ArtInstanceConfig (storage property)
// --- ART Error Types ---
export {
    ErrorCode,
    ARTError,
    UnknownProviderError,
    LocalProviderConflictError,
    LocalInstanceBusyError,
    ApiQueueTimeoutError,
    AdapterInstantiationError
} from '../errors';

// --- UI Socket Related Types ---
export { LLMStreamSocket } from '../systems/ui/llm-stream-socket';
export { TypedSocket } from '../systems/ui/typed-socket';
export type { UnsubscribeFunction } from '../systems/ui/typed-socket';

// --- Zod Schemas for Validation ---
export { ArtStandardPromptSchema, ArtStandardMessageSchema } from './schemas';


// Re-export necessary types from submodules
export type {
    ProviderManagerConfig,
    AvailableProviderEntry,
    RuntimeProviderConfig,
    ManagedAdapterAccessor,
    IProviderManager
} from './providers';

/**
 * Represents the role of a message sender in a conversation.
 */
export enum MessageRole {
  USER = 'USER',
  AI = 'AI',
  SYSTEM = 'SYSTEM', // Added for system prompts, though not explicitly in checklist message interface
  TOOL = 'TOOL',     // Added for tool results, though not explicitly in checklist message interface
}

/**
 * Represents a single message within a conversation thread.
 */
export interface ConversationMessage {
  /** A unique identifier for this specific message. */
  messageId: string;
  /** The identifier of the conversation thread this message belongs to. */
  threadId: string;
  /** The role of the sender (User, AI, System, or Tool). */
  role: MessageRole;
  /** The textual content of the message. */
  content: string;
  /** A Unix timestamp (in milliseconds) indicating when the message was created. */
  timestamp: number;
  /** Optional metadata associated with the message (e.g., related observation IDs, tool call info, UI state). */
  metadata?: Record<string, any>;
}

/**
 * Represents the type of an observation record, capturing significant events during agent execution.
 */
export enum ObservationType {
  INTENT = 'INTENT',
  PLAN = 'PLAN',
  THOUGHTS = 'THOUGHTS',
  /** Records the LLM's decision to call one or more tools (part of the plan). */
  TOOL_CALL = 'TOOL_CALL',
  /** Records the actual execution attempt and result of a specific tool call. */
  TOOL_EXECUTION = 'TOOL_EXECUTION',
  /** Records events specifically related to the synthesis phase (e.g., the LLM call). */
  SYNTHESIS = 'SYNTHESIS',
  /** Records an error encountered during any phase of execution. */
  ERROR = 'ERROR',
  /** Records the final AI response message generated by the agent. */
  FINAL_RESPONSE = 'FINAL_RESPONSE',
  /** Records changes made to the agent's persistent state. */
  STATE_UPDATE = 'STATE_UPDATE',

  // New types for streaming events
  /** Logged by Agent Core when LLM stream consumption begins. */
  LLM_STREAM_START = 'LLM_STREAM_START',
  /** Logged by Agent Core upon receiving a METADATA stream event. Content should be LLMMetadata. */
  LLM_STREAM_METADATA = 'LLM_STREAM_METADATA',
  /** Logged by Agent Core upon receiving an END stream event. */
  LLM_STREAM_END = 'LLM_STREAM_END',
  /** Logged by Agent Core upon receiving an ERROR stream event. Content should be Error object or message. */
  LLM_STREAM_ERROR = 'LLM_STREAM_ERROR',
}

// --- NEW ENUM DEFINITION ---
/**
 * Represents the different capabilities a model might possess.
 * Used for model selection and validation.
 */
export enum ModelCapability {
  TEXT = 'text',         // Basic text generation/understanding
  VISION = 'vision',       // Ability to process and understand images
  STREAMING = 'streaming',   // Supports streaming responses chunk by chunk
  TOOL_USE = 'tool_use',    // Capable of using tools/function calling
  RAG = 'rag',           // Built-in or optimized for Retrieval-Augmented Generation
  CODE = 'code',         // Specialized in understanding or generating code
  REASONING = 'reasoning'  // Advanced reasoning, planning, complex instruction following
}
// --- END NEW ENUM DEFINITION ---


/**
 * Represents a recorded event during the agent's execution.
 */
export interface Observation {
  /** A unique identifier for this specific observation record. */
  id: string;
  /** The identifier of the conversation thread this observation relates to. */
  threadId: string;
  /** An optional identifier for tracing a request across multiple systems or components. */
  traceId?: string;
  /** A Unix timestamp (in milliseconds) indicating when the observation was recorded. */
  timestamp: number;
  /** The category of the event being observed (e.g., PLAN, THOUGHTS, TOOL_EXECUTION). */
  type: ObservationType;
  /** A concise, human-readable title summarizing the observation (often generated based on type/metadata). */
  title: string;
  /** The main data payload of the observation, structure depends on the `type`. */
  content: any;
  /** Optional metadata providing additional context (e.g., source phase, related IDs, status). */
  metadata?: Record<string, any>;
}

/**
 * Represents a single event emitted from an asynchronous LLM stream (`ReasoningEngine.call`).
 * Allows for real-time delivery of tokens, metadata, errors, and lifecycle signals.
 * Adapters are responsible for translating provider-specific stream chunks into these standard events.
 */
export interface StreamEvent {
  /**
   * The type of the stream event:
   * - `TOKEN`: A chunk of text generated by the LLM.
   * - `METADATA`: Information about the LLM call (e.g., token counts, stop reason), typically sent once at the end.
   * - `ERROR`: An error occurred during the LLM call or stream processing. `data` will contain the Error object.
   * - `END`: Signals the successful completion of the stream. `data` is typically null.
   */
  type: 'TOKEN' | 'METADATA' | 'ERROR' | 'END';
  /**
   * The actual content of the event.
   * - For `TOKEN`: string (the text chunk).
   * - For `METADATA`: `LLMMetadata` object.
   * - For `ERROR`: `Error` object or error details.
   * - For `END`: null.
   */
  data: any;
  /**
   * Optional: Provides a more specific classification for `TOKEN` events,
   * combining LLM-level detection (thinking/response, if available from adapter)
   * and agent-level context (`callContext` from `CallOptions`).
   * Used by consumers (like UI) to differentiate between intermediate thoughts and the final response.
   *
   * - `LLM_THINKING`: Token identified by the adapter as part of the LLM's internal reasoning/thought process.
   * - `LLM_RESPONSE`: Token identified by the adapter as part of the LLM's final response content.
   * - `AGENT_THOUGHT_LLM_THINKING`: Token from an LLM call made in the 'AGENT_THOUGHT' context, identified as thinking.
   * - `AGENT_THOUGHT_LLM_RESPONSE`: Token from an LLM call made in the 'AGENT_THOUGHT' context, identified as response (e.g., the raw planning output).
   * - `FINAL_SYNTHESIS_LLM_THINKING`: Token from an LLM call made in the 'FINAL_SYNTHESIS' context, identified as thinking.
   * - `FINAL_SYNTHESIS_LLM_RESPONSE`: Token from an LLM call made in the 'FINAL_SYNTHESIS' context, identified as response (part of the final answer to the user).
   *
   * Note: Not all adapters can reliably distinguish 'LLM_THINKING' vs 'LLM_RESPONSE'.
   * Adapters should prioritize setting the agent context part (`AGENT_THOUGHT_...` or `FINAL_SYNTHESIS_...`) based on `CallOptions.callContext`.
   * If thinking detection is unavailable, adapters should default to `AGENT_THOUGHT_LLM_RESPONSE` or `FINAL_SYNTHESIS_LLM_RESPONSE`.
   */
  tokenType?: 'LLM_THINKING' | 'LLM_RESPONSE' | 'AGENT_THOUGHT_LLM_THINKING' | 'AGENT_THOUGHT_LLM_RESPONSE' | 'FINAL_SYNTHESIS_LLM_THINKING' | 'FINAL_SYNTHESIS_LLM_RESPONSE';
  /** The identifier of the conversation thread this event belongs to. */
  threadId: string;
  /** The identifier tracing the specific agent execution cycle this event is part of. */
  traceId: string;
  /** Optional identifier linking the event to a specific UI tab/window. */
  sessionId?: string;
}

/**
 * Represents a basic JSON Schema definition, focusing on object types commonly used for tool inputs/outputs.
 * This is a simplified representation and doesn't cover all JSON Schema features.
 */
export interface JsonObjectSchema {
  type: 'object';
  properties: {
    [key: string]: {
      type: string; // e.g., 'string', 'number', 'boolean', 'object', 'array'
      description?: string;
      default?: any;
      items?: JsonObjectSchema | { type: string }; // For array type
      properties?: JsonObjectSchema['properties']; // For nested object type
      required?: string[]; // For nested object type
      additionalProperties?: boolean | { type: string };
      [key: string]: any; // Allow other JSON schema properties
    };
  };
  required?: string[];
  additionalProperties?: boolean;
}

// Allow for other schema types (string, number, etc.) although object is most common for tools
export type JsonSchema = JsonObjectSchema | { type: 'string' | 'number' | 'boolean' | 'array', [key: string]: any };

/**
 * Structure for holding metadata about an LLM call, typically received via a `METADATA` `StreamEvent`
 * or parsed from a non-streaming response. Fields are optional as availability varies by provider and stream state.
 */
export interface LLMMetadata {
  /** The number of tokens in the input prompt, if available. */
  inputTokens?: number;
  /** The number of tokens generated in the output response, if available. */
  outputTokens?: number;
  /** The number of tokens identified as part of the LLM's internal thinking process (if available from provider). */
  thinkingTokens?: number;
  /** The time elapsed (in milliseconds) until the first token was generated in a streaming response, if applicable and available. */
  timeToFirstTokenMs?: number;
  /** The total time elapsed (in milliseconds) for the entire generation process, if available. */
  totalGenerationTimeMs?: number;
  /** The reason the LLM stopped generating tokens (e.g., 'stop_sequence', 'max_tokens', 'tool_calls'), if available. */
  stopReason?: string;
  /** Optional raw usage data provided directly by the LLM provider for extensibility (structure depends on provider). */
  providerRawUsage?: any;
  /** The trace ID associated with the LLM call, useful for correlating metadata with the specific request. */
  traceId?: string; // Include traceId if this object might be stored or passed independently.
}

/**
 * Defines the schema for a tool, including its input parameters.
 * Uses JSON Schema format for inputSchema.
 */
export interface ToolSchema {
  /** A unique name identifying the tool (used in LLM prompts and registry lookups). Must be unique. */
  name: string;
  /** A clear description of what the tool does, intended for the LLM to understand its purpose and usage. */
  description: string;
  /** A JSON Schema object defining the structure, types, and requirements of the input arguments the tool expects. */
  inputSchema: JsonSchema;
  /** An optional JSON Schema object defining the expected structure of the data returned in the `output` field of a successful `ToolResult`. */
  outputSchema?: JsonSchema;
  /** Optional array of examples demonstrating how to use the tool, useful for few-shot prompting of the LLM. */
  examples?: Array<{ input: any; output?: any; description?: string }>;
}
/**
 * Represents the structured result of a tool execution.
 */
export interface ToolResult {
  /** The unique identifier of the corresponding `ParsedToolCall` that initiated this execution attempt. */
  callId: string;
  /** The name of the tool that was executed. */
  toolName: string;
  /** Indicates whether the tool execution succeeded or failed. */
  status: 'success' | 'error';
  /** The data returned by the tool upon successful execution. Structure may be validated against `outputSchema`. */
  output?: any;
  /** A descriptive error message if the execution failed (`status` is 'error'). */
  error?: string;
  /** Optional metadata about the execution (e.g., duration, cost, logs). */
  metadata?: Record<string, any>;
}

/**
 * Represents a parsed request from the LLM to call a specific tool.
 */
export interface ParsedToolCall {
  /** A unique identifier generated by the OutputParser for this specific tool call request within a plan. */
  callId: string;
  /** The name of the tool the LLM intends to call. Must match a registered tool's schema name. */
  toolName: string;
  /** The arguments object, parsed from the LLM response, intended to be passed to the tool's `execute` method after validation. */
  arguments: any;
}

/**
 * Configuration specific to a conversation thread.
 */
 export interface ThreadConfig {
   /** Default provider configuration for this thread. */
   providerConfig: RuntimeProviderConfig;
   /** An array of tool names (matching `ToolSchema.name`) that are permitted for use within this thread. */
   enabledTools: string[];
   /** The maximum number of past messages (`ConversationMessage` objects) to retrieve for context. */
   historyLimit: number;
   /** Optional system prompt string to be used for this thread, overriding instance or agent defaults. */
   systemPrompt?: string;
   // TODO: Add other potential thread-specific settings (e.g., RAG configuration, default timeouts)
 }

/**
 * Represents non-configuration state associated with an agent or thread.
 * Could include user preferences, accumulated knowledge, etc. (Less defined for v1.0)
 */
export interface AgentState {
  /** The primary data payload of the agent's state. Structure is application-defined. */
  data: any;
  /** An optional version number for the agent's state, useful for migrations or tracking changes. */
  version?: number;
  /** Allows for other arbitrary properties to be stored in the agent's state. */
  [key: string]: any;
}

/**
 * Encapsulates the configuration and state for a specific thread.
 */
export interface ThreadContext {
  /** The configuration settings (`ThreadConfig`) currently active for the thread. */
  config: ThreadConfig;
  /** The persistent state (`AgentState`) associated with the thread, or `null` if no state exists. */
  state: AgentState | null;
}

/**
 * Properties required to initiate an agent processing cycle.
 */
export interface AgentProps {
  /** The user's input query or request to the agent. */
  query: string;
  /** The mandatory identifier for the conversation thread. All context is scoped to this ID. */
  threadId: string;
  /** An optional identifier for the specific UI session, useful for targeting UI updates. */
  sessionId?: string;
  /** An optional identifier for the user interacting with the agent. */
  userId?: string;
  /** An optional identifier used for tracing a request across multiple systems or services. */
  traceId?: string;
  /** Optional runtime options that can override default behaviors for this specific `process` call. */
  options?: AgentOptions;
  // Note: Core dependencies (StateManager, ConversationManager, etc.) are typically injected
  // during `createArtInstance` and are accessed internally by the Agent Core, not passed in AgentProps.
}

/**
 * Options to override agent behavior at runtime.
 */
 export interface AgentOptions {
   /** Override specific LLM parameters (e.g., temperature, max_tokens) for this call only. */
   llmParams?: Record<string, any>;
   /** Override provider configuration for this specific call. */
   providerConfig?: RuntimeProviderConfig; // Add this line
   /** Force the use of specific tools, potentially overriding the thread's `enabledTools` for this call (use with caution). */
   forceTools?: string[];
   /** Specify a particular reasoning model to use for this call, overriding the thread's default. */
   overrideModel?: { provider: string; model: string };
   /** Request a streaming response for this specific agent process call. */
   stream?: boolean;
   /** Override the prompt template used for this specific call. */
   promptTemplateId?: string;
   /** Optional system prompt string to override thread, instance, or agent defaults for this specific call. */
   systemPrompt?: string;
   // TODO: Add other potential runtime overrides (e.g., history length).
 }

/**
 * The final structured response returned by the agent core after processing.
 */
export interface AgentFinalResponse {
  /** The final `ConversationMessage` generated by the AI, which has also been persisted. */
  response: ConversationMessage;
  /** Metadata summarizing the execution cycle that produced this response. */
  metadata: ExecutionMetadata;
}

/**
 * Metadata summarizing an agent execution cycle, including performance metrics and outcomes.
 */
export interface ExecutionMetadata {
  /** The thread ID associated with this execution cycle. */
  threadId: string;
  /** The trace ID used during this execution, if provided. */
  traceId?: string;
  /** The user ID associated with the execution, if provided. */
  userId?: string;
  /** The overall status of the execution ('success', 'error', or 'partial' if some steps failed but a response was generated). */
  status: 'success' | 'error' | 'partial';
  /** The total duration of the `agent.process()` call in milliseconds. */
  totalDurationMs: number;
  /** The number of calls made to the `ReasoningEngine`. */
  llmCalls: number;
  /** The number of tool execution attempts made by the `ToolSystem`. */
  toolCalls: number;
  /** An optional estimated cost for the LLM calls made during this execution. */
  llmCost?: number;
  /** A top-level error message if the overall status is 'error' or 'partial'. */
  error?: string;
  /** Aggregated metadata from LLM calls made during the execution. */
  llmMetadata?: LLMMetadata;
}

/**
 * Context provided to a tool during its execution.
 */
export interface ExecutionContext {
  /** The ID of the thread in which the tool is being executed. */
  threadId: string;
  /** The trace ID for this execution cycle, if available. */
  traceId?: string;
  /** The user ID associated with the execution, if available. */
  userId?: string;
  // TODO: Potentially include access tokens or credentials scoped to this execution, if needed securely.
  // TODO: Consider providing limited access to StateManager or other relevant context if required by complex tools.
}

/**
 * Options for configuring an LLM call, including streaming and context information.
 */
export interface CallOptions {
  /** The mandatory thread ID, used by the ReasoningEngine to fetch thread-specific configuration (e.g., model, params) via StateManager. */
  threadId: string;
  /** Optional trace ID for correlation. */
  traceId?: string;
  /** Optional user ID. */
  userId?: string;
  /** Optional session ID. */
  sessionId?: string; // Added sessionId
  /**
   * Request a streaming response from the LLM provider.
   * Adapters MUST check this flag.
   */
  stream?: boolean;
  /**
   * Provides context for the LLM call, allowing adapters to differentiate
   * between agent-level thoughts and final synthesis calls for token typing.
   * Agent Core MUST provide this.
   */
  callContext?: 'AGENT_THOUGHT' | 'FINAL_SYNTHESIS' | string;
  /** An optional callback function invoked when the LLM streams intermediate 'thoughts' or reasoning steps.
   * @deprecated Prefer using StreamEvent with appropriate tokenType for thoughts. Kept for potential transitional compatibility.
   */
  // onThought?: (thought: string) => void; // Commented out as per implementation plan decision (Ref: 7.2, Checklist Phase 1)
  /** Carries the specific target provider and configuration for this call. */
  providerConfig: RuntimeProviderConfig;
  /** Additional key-value pairs representing provider-specific parameters (e.g., `temperature`, `max_tokens`, `top_p`). These often override defaults set in `ThreadConfig`. */
  [key: string]: any;
}

// --- ART STANDARD PROMPT TYPES (Refactor Phase 1) ---

/**
 * Defines the standard roles for messages within the `ArtStandardPrompt` format.
 * These roles are chosen for broad compatibility across major LLM providers (like OpenAI, Anthropic, Gemini).
 * Provider Adapters are responsible for translating these standard roles into the specific formats
 * required by their respective APIs (e.g., 'assistant' might become 'model' for Gemini).
 *
 * - `system`: Instructions or context provided to the AI, typically at the beginning.
 * - `user`: Input or queries from the end-user. Also used to wrap `tool_result` content for some providers (e.g., Gemini).
 * - `assistant`: Responses generated by the AI model. Can contain text content and/or `tool_calls`.
 * - `tool_request`: Represents the LLM's request to use tools (often implicitly part of an `assistant` message with `tool_calls`). Included for potential future explicit use.
 * - `tool_result`: The outcome (output or error) of executing a requested tool call.
 */
 export type ArtStandardMessageRole = 'system' | 'user' | 'assistant' | 'tool_request' | 'tool_result' | 'tool'; // Added 'tool' role

/**
 * Represents a single message in the standardized, provider-agnostic `ArtStandardPrompt` format.
 * This structure aims to capture common message elements used by various LLM APIs.
 */
export interface ArtStandardMessage {
  /** The role indicating the source or type of the message. */
  role: ArtStandardMessageRole;
  /**
   * The primary content of the message. The type and interpretation depend on the `role`:
   * - `system`: string (The system instruction).
   * - `user`: string (The user's text input).
   * - `assistant`: string | null (The AI's text response, or null/empty if only making `tool_calls`).
   * - `tool_request`: object | null (Structured representation of the tool call, often implicitly handled via `assistant` message's `tool_calls`).
   * - `tool_result`: string (Stringified JSON output or error message from the tool execution).
   */
  content: string | object | null;
  /** Optional name associated with the message. Primarily used for `tool_result` role to specify the name of the tool that was executed. */
  name?: string;
  /**
   * Optional array of tool calls requested by the assistant.
   * Only relevant for 'assistant' role messages that trigger tool usage.
   * Structure mirrors common provider formats (e.g., OpenAI).
   */
  tool_calls?: Array<{
    /** A unique identifier for this specific tool call request. */
    id: string;
    /** The type of the tool call, typically 'function'. */
    type: 'function'; // Assuming 'function' is the standard type
    /** Details of the function to be called. */
    function: {
      /** The name of the function/tool to call. */
      name: string;
      /** A stringified JSON object representing the arguments for the function. */
      arguments: string;
    };
  }>;
  /**
   * Optional identifier linking a 'tool_result' message back to the specific 'tool_calls' entry
   * in the preceding 'assistant' message that requested it.
   * Required for 'tool_result' role.
   */
  tool_call_id?: string;
}

/**
 * Represents the entire prompt as an array of standardized messages (`ArtStandardMessage`).
 * This is the standard format produced by `PromptManager.assemblePrompt` and consumed
 * by `ProviderAdapter.call` for translation into provider-specific API formats.
 */
export type ArtStandardPrompt = ArtStandardMessage[];

/**
 * Represents the contextual data gathered by Agent Logic (e.g., `PESAgent`) to be injected
 * into a Mustache blueprint/template by the `PromptManager.assemblePrompt` method.
 *
 * Contains standard fields commonly needed for prompts, plus allows for arbitrary
 * additional properties required by specific agent blueprints. Agent logic is responsible
 * for populating this context appropriately before calling `assemblePrompt`.
 */
export interface PromptContext {
  /** The user's current query or input relevant to this prompt generation step. */
  query?: string;
  /**
   * The conversation history, typically formatted as an array suitable for the blueprint
   * (e.g., array of objects with `role` and `content`). Agent logic should pre-format this.
   * Note: While `ArtStandardPrompt` could be used, simpler structures might be preferred for blueprints.
   */
  history?: Array<{ role: string; content: string; [key: string]: any }>; // Flexible history format for blueprints
  /**
   * The schemas of the tools available for use, potentially pre-formatted for the blueprint
   * (e.g., with `inputSchemaJson` pre-stringified).
   */
  availableTools?: Array<ToolSchema & { inputSchemaJson?: string }>;
  /**
   * The results from any tools executed in a previous step, potentially pre-formatted for the blueprint
   * (e.g., with `outputJson` pre-stringified).
   */
  toolResults?: Array<ToolResult & { outputJson?: string }>;
  /** The system prompt string to be used (resolved by agent logic from config or defaults). */
  systemPrompt?: string;
  /** Allows agent patterns (like PES) to pass any other custom data needed by their specific blueprints (e.g., `intent`, `plan`). */
  [key: string]: any;
}

// --- END ART STANDARD PROMPT TYPES ---


/**
 * Represents the prompt data formatted for a specific LLM provider.
 * Can be a simple string or a complex object (e.g., for OpenAI Chat Completion API).
 * @deprecated Use `ArtStandardPrompt` as the standard intermediate format. ProviderAdapters handle final formatting.
 */
export type FormattedPrompt = ArtStandardPrompt; // Point deprecated type to the new standard

/**
 * Options for filtering data retrieved from storage.
 * Structure depends heavily on the underlying adapter's capabilities.
 */
export interface FilterOptions {
  /** An object defining filter criteria (e.g., `{ threadId: 'abc', type: 'TOOL_EXECUTION' }`). Structure may depend on adapter capabilities. */
  filter?: Record<string, any>;
  /** An object defining sorting criteria (e.g., `{ timestamp: 'desc' }`). */
  sort?: Record<string, 'asc' | 'desc'>;
  /** The maximum number of records to return. */
  limit?: number;
  /** The number of records to skip (for pagination). */
  skip?: number;
  // TODO: Consider adding projection options to retrieve only specific fields.
}

/**
 * Options for retrieving conversation messages.
 */
export interface MessageOptions {
  /** The maximum number of messages to retrieve. */
  limit?: number;
  /** Retrieve messages created before this Unix timestamp (milliseconds). */
  beforeTimestamp?: number;
  /** Retrieve messages created after this Unix timestamp (milliseconds). */
  afterTimestamp?: number;
  /** Optionally filter messages by role (e.g., retrieve only 'AI' messages). */
  roles?: MessageRole[];
}

/**
 * Options for filtering observations.
 */
export interface ObservationFilter {
  /** An array of `ObservationType` enums to filter by. If provided, only observations matching these types are returned. */
  types?: ObservationType[];
  /** Retrieve observations recorded before this Unix timestamp (milliseconds). */
  beforeTimestamp?: number;
  /** Retrieve observations recorded after this Unix timestamp (milliseconds). */
  afterTimestamp?: number;
  // TODO: Add other potential criteria like filtering by metadata content if needed.
}

// Removed duplicate TypedSocket interface definition.
// The primary definition is in src/core/interfaces.ts

/**
 * Defines the strategy for saving AgentState.
 * - 'explicit': AgentState is only saved when `StateManager.setAgentState()` is explicitly called by the agent.
 *               `StateManager.saveStateIfModified()` will be a no-op for AgentState persistence.
 * - 'implicit': AgentState is loaded by `StateManager.loadThreadContext()`, and if modified by the agent,
 *               `StateManager.saveStateIfModified()` will attempt to automatically persist these changes
 *               by comparing the current state with a snapshot taken at load time.
 *               `StateManager.setAgentState()` will still work for explicit saves.
 */
export type StateSavingStrategy = 'explicit' | 'implicit';

// Explicitly import ProviderManagerConfig here for ArtInstanceConfig
import type { ProviderManagerConfig as PMConfig } from './providers';

/**
 * Configuration for creating an ART instance.
 */
export interface ArtInstanceConfig {
  /**
   * Configuration for the storage adapter.
   * Can be a pre-configured `StorageAdapter` instance,
   * or an object specifying the type and options for a built-in adapter.
   * Example: `{ type: 'indexedDB', dbName: 'MyArtDB' }`
   */
  storage: StorageAdapter | { type: 'memory' | 'indexedDB', dbName?: string, version?: number, objectStores?: any[] };
  /** Configuration for the ProviderManager, defining available LLM provider adapters. */
  providers: PMConfig; // Use the aliased import
  /**
   * The agent core implementation class to use.
   * Defaults to `PESAgent` if not provided.
   * Example: `MyCustomAgentClass`
   */
  agentCore?: new (dependencies: any) => IAgentCore; // Constructor type for an IAgentCore implementation
  /** An optional array of tool executor instances to register at initialization. */
  tools?: IToolExecutor[];
  /**
   * Defines the strategy for saving `AgentState`. Defaults to 'explicit'.
   * - 'explicit': `AgentState` is only saved when `StateManager.setAgentState()` is explicitly called by the agent.
   *               `StateManager.saveStateIfModified()` will be a no-op for `AgentState` persistence.
   * - 'implicit': `AgentState` is loaded by `StateManager.loadThreadContext()`. If modified by the agent,
   *               `StateManager.saveStateIfModified()` will attempt to automatically persist these changes.
   *               `StateManager.setAgentState()` will still work for explicit saves in this mode.
   */
  stateSavingStrategy?: StateSavingStrategy;
  /** Optional configuration for the framework's logger. */
  logger?: {
    /** Minimum log level to output. Defaults to 'info'. */
    level?: LogLevel;
  };
  /**
   * Optional default system prompt string to be used for the entire ART instance.
   * This can be overridden at the thread level or at the individual call level.
   */
  defaultSystemPrompt?: string;
  // Add other top-level configuration properties as needed, e.g.:
  // defaultThreadConfig?: Partial<ThreadConfig>;
}

/**
 * Represents the possible states of an A2A (Agent-to-Agent) task.
 */
export enum A2ATaskStatus {
  /** Task has been created but not yet assigned to an agent. */
  PENDING = 'PENDING',
  /** Task has been assigned to an agent and is being processed. */
  IN_PROGRESS = 'IN_PROGRESS',
  /** Task has been completed successfully. */
  COMPLETED = 'COMPLETED',
  /** Task has failed during execution. */
  FAILED = 'FAILED',
  /** Task has been cancelled before completion. */
  CANCELLED = 'CANCELLED',
  /** Task is waiting for external dependencies or manual intervention. */
  WAITING = 'WAITING',
  /** Task is being reviewed for quality assurance. */
  REVIEW = 'REVIEW'
}

/**
 * Represents the priority level of an A2A task.
 */
export enum A2ATaskPriority {
  LOW = 'LOW',
  MEDIUM = 'MEDIUM',
  HIGH = 'HIGH',
  URGENT = 'URGENT'
}

/**
 * Represents agent information for A2A task assignment.
 */
export interface A2AAgentInfo {
  /** Unique identifier for the agent. */
  agentId: string;
  /** Human-readable name for the agent. */
  agentName: string;
  /** The type or role of the agent (e.g., 'reasoning', 'data-processing', 'synthesis'). */
  agentType: string;
  /** Base URL or endpoint for communicating with the agent. */
  endpoint?: string;
  /** Agent capabilities or specializations. */
  capabilities?: string[];
  /** Current load or availability status of the agent. */
  status?: 'available' | 'busy' | 'offline';
  /** Authentication configuration for communicating with the agent. */
  authentication?: {
    /** Type of authentication required. */
    type: 'bearer' | 'api_key' | 'none';
    /** Bearer token for authorization (if type is 'bearer'). */
    token?: string;
    /** API key for authorization (if type is 'api_key'). */
    apiKey?: string;
  };
}

/**
 * Represents metadata about A2A task execution.
 */
export interface A2ATaskMetadata {
  /** Timestamp when the task was created (Unix timestamp in milliseconds). */
  createdAt: number;
  /** Timestamp when the task was last updated (Unix timestamp in milliseconds). */
  updatedAt: number;
  /** Timestamp when the task was started (if applicable). */
  startedAt?: number;
  /** Timestamp when the task was completed/failed (if applicable). */
  completedAt?: number;
  /** Timestamp when the task was delegated to a remote agent (if applicable). */
  delegatedAt?: number;
  /** Timestamp when the task was last updated (for compatibility). */
  lastUpdated?: number;
  /** The user or system that initiated this task. */
  initiatedBy?: string;
  /** Correlation ID for tracking related tasks across the system. */
  correlationId?: string;
  /** Number of retry attempts made for this task. */
  retryCount?: number;
  /** Maximum number of retry attempts allowed. */
  maxRetries?: number;
  /** Timeout duration in milliseconds. */
  timeoutMs?: number;
  /** Estimated completion time in milliseconds (if provided by remote agent). */
  estimatedCompletionMs?: number;
  /** Tags or labels for categorizing tasks. */
  tags?: string[];
}

/**
 * Represents the result of an A2A task execution.
 */
export interface A2ATaskResult {
  /** Whether the task execution was successful. */
  success: boolean;
  /** The data returned by the task execution. */
  data?: any;
  /** Error message if the task failed. */
  error?: string;
  /** Additional metadata about the execution. */
  metadata?: Record<string, any>;
  /** Execution duration in milliseconds. */
  durationMs?: number;
}

/**
 * Represents a task for Agent-to-Agent (A2A) communication and delegation.
 * Used for asynchronous task delegation between AI agents in distributed systems.
 */
export interface A2ATask {
  /** Unique identifier for the task. */
  taskId: string;
  
  /** Current status of the task. */
  status: A2ATaskStatus;
  
  /** The data payload containing task parameters and context. */
  payload: {
    /** The type of task to be executed (e.g., 'analyze', 'synthesize', 'transform'). */
    taskType: string;
    /** Input data required for task execution. */
    input: any;
    /** Instructions or configuration for the task. */
    instructions?: string;
    /** Additional parameters specific to the task type. */
    parameters?: Record<string, any>;
  };
  
  /** Information about the agent that created/requested this task. */
  sourceAgent: A2AAgentInfo;
  
  /** Information about the agent assigned to execute this task (if assigned). */
  targetAgent?: A2AAgentInfo;
  
  /** Task priority level. */
  priority: A2ATaskPriority;
  
  /** Task execution metadata. */
  metadata: A2ATaskMetadata;
  
  /** The result of task execution (if completed). */
  result?: A2ATaskResult;
  
  /** Callback URL or identifier for task completion notifications. */
  callbackUrl?: string;
  
  /** Dependencies that must be completed before this task can start. */
  dependencies?: string[];
}

/**
 * Represents a request to create a new A2A task.
 */
export interface CreateA2ATaskRequest {
  /** The type of task to be executed. */
  taskType: string;
  /** Input data for the task. */
  input: any;
  /** Instructions for task execution. */
  instructions?: string;
  /** Task parameters. */
  parameters?: Record<string, any>;
  /** Task priority. */
  priority?: A2ATaskPriority;
  /** Source agent information. */
  sourceAgent: A2AAgentInfo;
  /** Preferred target agent (if any). */
  preferredTargetAgent?: A2AAgentInfo;
  /** Task dependencies. */
  dependencies?: string[];
  /** Callback URL for notifications. */
  callbackUrl?: string;
  /** Task timeout in milliseconds. */
  timeoutMs?: number;
  /** Maximum retry attempts. */
  maxRetries?: number;
  /** Task tags. */
  tags?: string[];
}

/**
 * Represents an update to an existing A2A task.
 */
export interface UpdateA2ATaskRequest {
  /** Task ID to update. */
  taskId: string;
  /** New task status (if changing). */
  status?: A2ATaskStatus;
  /** Target agent assignment (if assigning/reassigning). */
  targetAgent?: A2AAgentInfo;
  /** Task result (if completing). */
  result?: A2ATaskResult;
  /** Additional metadata updates. */
  metadata?: Partial<A2ATaskMetadata>;
}