[**ART Framework Component Reference**](../README.md)

***

[ART Framework Component Reference](../README.md) / ProviderAdapter

# Interface: ProviderAdapter

Defined in: [src/core/interfaces.ts:178](https://github.com/hashangit/ART/blob/fe46dfaaacd3f198d9540925c3184fcab0f9c813/src/core/interfaces.ts#L178)

Base interface for LLM Provider Adapters, extending the core ReasoningEngine.
Implementations will handle provider-specific API calls, authentication, etc.

## Extends

- [`ReasoningEngine`](ReasoningEngine.md)

## Properties

### providerName

> `readonly` **providerName**: `string`

Defined in: [src/core/interfaces.ts:182](https://github.com/hashangit/ART/blob/fe46dfaaacd3f198d9540925c3184fcab0f9c813/src/core/interfaces.ts#L182)

The unique identifier name for this provider (e.g., 'openai', 'anthropic').

## Methods

### call()

> **call**(`prompt`, `options`): `Promise`\<`AsyncIterable`\<[`StreamEvent`](StreamEvent.md), `any`, `any`\>\>

Defined in: [src/core/interfaces.ts:83](https://github.com/hashangit/ART/blob/fe46dfaaacd3f198d9540925c3184fcab0f9c813/src/core/interfaces.ts#L83)

Executes a call to the configured Large Language Model (LLM).
This method is typically implemented by a specific `ProviderAdapter`.
When streaming is requested via `options.stream`, it returns an AsyncIterable
that yields `StreamEvent` objects as they are generated by the LLM provider.
When streaming is not requested, it should still return an AsyncIterable
that yields a minimal sequence of events (e.g., a single TOKEN event with the full response,
a METADATA event if available, and an END event).

#### Parameters

##### prompt

[`ArtStandardPrompt`](../type-aliases/ArtStandardPrompt.md)

The prompt to send to the LLM, potentially formatted specifically for the provider.

##### options

[`CallOptions`](CallOptions.md)

Options controlling the LLM call, including mandatory `threadId`, tracing IDs, model parameters (like temperature), streaming preference, and call context.

#### Returns

`Promise`\<`AsyncIterable`\<[`StreamEvent`](StreamEvent.md), `any`, `any`\>\>

A promise resolving to an AsyncIterable of `StreamEvent` objects.

#### Throws

If a critical error occurs during the initial call setup or if the stream itself errors out (typically code `LLM_PROVIDER_ERROR`).

#### Inherited from

[`ReasoningEngine`](ReasoningEngine.md).[`call`](ReasoningEngine.md#call)

***

### shutdown()?

> `optional` **shutdown**(): `Promise`\<`void`\>

Defined in: [src/core/interfaces.ts:185](https://github.com/hashangit/ART/blob/fe46dfaaacd3f198d9540925c3184fcab0f9c813/src/core/interfaces.ts#L185)

Optional: Method for graceful shutdown

#### Returns

`Promise`\<`void`\>
