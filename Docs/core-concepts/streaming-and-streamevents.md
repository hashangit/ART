# Streaming and StreamEvents in ART

Modern Large Language Models (LLMs) often support streaming, where they generate responses token by token (or chunk by chunk) rather than returning the entire response at once. The ART Framework embraces streaming to enhance user experience by providing real-time feedback and improving perceived performance. This is primarily achieved through the `StreamEvent` interface and its handling by the `ReasoningEngine` and `ProviderAdapter`s.

## What is `StreamEvent`?

`StreamEvent` is a standardized interface within ART (`src/types/index.ts`) that represents a single piece of data emitted from an LLM stream. Adapters are responsible for translating provider-specific stream chunks into these standard events.

```typescript
export interface StreamEvent {
  type: 'TOKEN' | 'METADATA' | 'ERROR' | 'END';
  data: any; // Content depends on the 'type'
  tokenType?: 'LLM_THINKING' | 'LLM_RESPONSE' | /* ...other combined types */;
  threadId: string;
  traceId: string;
  sessionId?: string;
}
```

**Key `StreamEvent` Types:**

*   **`TOKEN`**:
    *   `data`: A `string` representing a chunk of text generated by the LLM.
    *   `tokenType` (Optional): Provides more specific classification for `TOKEN` events, helping to distinguish between intermediate thoughts and the final response. It combines LLM-level detection (if available from the adapter, e.g., `<think>` tags) and agent-level context (`callContext` from `CallOptions`).
        *   Examples: `AGENT_THOUGHT_LLM_RESPONSE` (for planning output), `FINAL_SYNTHESIS_LLM_RESPONSE` (for the user-facing answer).
*   **`METADATA`**:
    *   `data`: An `LLMMetadata` object containing information like token counts (`inputTokens`, `outputTokens`), `stopReason`, `timeToFirstTokenMs`, and `totalGenerationTimeMs`. This event is typically sent once, often towards or at the end of a stream.
*   **`ERROR`**:
    *   `data`: An `Error` object or error details if an issue occurred during the LLM call or while processing the stream.
*   **`END`**:
    *   `data`: Usually `null`. Signals that the LLM has finished generating its response for the current call and the stream is complete.

Each `StreamEvent` also includes `threadId`, `traceId`, and an optional `sessionId` for context and correlation.

## How Streaming Works in ART

1.  **Requesting a Stream:**
    When an agent (like `PESAgent`) calls the `ReasoningEngine`, it can request a streaming response by setting `stream: true` in the `CallOptions`.

    ```typescript
    // In PESAgent, during a call to the ReasoningEngine:
    const callOptions: CallOptions = {
        threadId: props.threadId,
        traceId: traceId,
        stream: true, // Request a streaming response
        callContext: 'AGENT_THOUGHT', // e.g., for planning phase
        providerConfig: { /* ... */ },
        // ... other parameters
    };
    const llmStream = await this.deps.reasoningEngine.call(promptObject, callOptions);
    ```

2.  **`ReasoningEngine` and `ProviderAdapter` Handling:**
    *   The `ReasoningEngine` receives the call. It uses the `ProviderManager` to obtain an appropriate `ProviderAdapter` based on `callOptions.providerConfig`.
    *   The selected `ProviderAdapter` (e.g., `OpenAIAdapter`, `AnthropicAdapter`) is responsible for making the actual API call to the LLM provider with streaming enabled.
    *   As the LLM provider sends back data chunks, the `ProviderAdapter` translates these provider-specific chunks into standard ART `StreamEvent` objects.
    *   The `ProviderAdapter.call()` method returns an `AsyncIterable<StreamEvent>`.

3.  **Consuming the Stream (`PESAgent` Example):**
    The `PESAgent` (or any other component calling `ReasoningEngine`) consumes this `AsyncIterable` to process events as they arrive.

    ```typescript
    // In PESAgent, after calling reasoningEngine.call():
    let accumulatedText = "";
    for await (const event of llmStream) {
        // Notify UI subscribers about every event
        this.deps.uiSystem.getLLMStreamSocket().notify(event, { /* ... */ });

        switch (event.type) {
            case 'TOKEN':
                accumulatedText += event.data;
                // Further processing based on event.tokenType if needed
                break;
            case 'METADATA':
                // Store or log metadata
                console.log("LLM Metadata received:", event.data);
                break;
            case 'ERROR':
                // Handle stream error
                console.error("LLM Stream Error:", event.data);
                // Potentially throw or set an error state
                break;
            case 'END':
                // Stream finished
                console.log("LLM Stream Ended.");
                break;
        }
        if (/* error occurred */) break; // Exit loop on error
    }
    // After the loop, 'accumulatedText' contains the full response from this LLM call.
    ```

## Benefits of Streaming

*   **Real-time Feedback:** Users see parts of the response as they are generated, making the agent feel more responsive, especially for longer generations.
*   **Improved Perceived Performance:** The application doesn't have to wait for the entire LLM response before displaying something.
*   **Handling Intermediate Thoughts:** The `tokenType` field in `StreamEvent` (when `type` is `TOKEN`) allows the system to differentiate between the LLM's internal "thinking" process (e.g., planning steps, reasoning traces often wrapped in `<think>` tags by some models/prompts) and the actual "response" content intended for the next stage or the user. This is crucial for complex agent behavior and debugging.

## UI Integration via `LLMStreamSocket`

The ART Framework provides an `LLMStreamSocket` (part of the `UISystem`) specifically for broadcasting `StreamEvent`s.

*   The `PESAgent` (or other core logic) `notify`s this socket with every `StreamEvent` it receives from the `ReasoningEngine`.
*   UI components can `subscribe` to the `LLMStreamSocket` to receive these events in real-time.
*   Subscribers can filter events based on `StreamEvent.type` (e.g., only listen for `TOKEN` events) or by `threadId` and `sessionId`.

**Conceptual UI Snippet:**

```javascript
// Hypothetical client-side JavaScript using a WebSocket connection to ART's backend sockets

// Assume 'artSockets' is an object that manages WebSocket connections
// and provides interfaces similar to ART's internal sockets.

const llmStreamDisplay = document.getElementById('llm-output');
const currentThreadId = 'some-active-thread-id';

artSockets.llmStream.subscribe(
    (event) => { // Callback function
        switch (event.type) {
            case 'TOKEN':
                // Append text, potentially differentiating based on event.tokenType
                const span = document.createElement('span');
                span.textContent = event.data;
                if (event.tokenType && event.tokenType.includes('THOUGHT')) {
                    span.className = 'thought-token'; // Style thoughts differently
                }
                llmStreamDisplay.appendChild(span);
                break;
            case 'METADATA':
                console.log('LLM Call Metadata:', event.data);
                break;
            case 'ERROR':
                console.error('LLM Stream Error:', event.data);
                llmStreamDisplay.innerHTML += `<p class="error">Error: ${event.data.message || event.data}</p>`;
                break;
            case 'END':
                console.log('LLM Stream finished.');
                llmStreamDisplay.innerHTML += `<p class="stream-end">--- End of Stream ---</p>`;
                break;
        }
    },
    null, // No type filter (receive all event types)
    { threadId: currentThreadId } // Filter by current thread
);
```

By standardizing stream events and providing dedicated sockets, ART enables developers to easily build responsive and informative user interfaces for their AI agents.