# Deep Dive: `LLMStreamSocket`

The `LLMStreamSocket` is a specialized `TypedSocket` within ART's UI System, dedicated to broadcasting `StreamEvent` objects. These events (`TOKEN`, `METADATA`, `ERROR`, `END`) are generated by `ProviderAdapter`s during Large Language Model (LLM) interactions, especially when streaming is enabled. This socket allows UI components or other services to receive real-time updates directly from the LLM's generation process.

*   **Source:** `src/systems/ui/llm-stream-socket.ts`
*   **Extends:** `TypedSocket<StreamEvent, StreamEvent['type'] | Array<StreamEvent['type']>>`
    *   `DataType` is `StreamEvent`.
    *   `FilterType` is a single `StreamEvent['type']` (e.g., `'TOKEN'`, `'METADATA'`) or an array of such types.

## Constructor

```typescript
constructor()
```

*   The `LLMStreamSocket` constructor is simple and currently takes no arguments. It calls `super()` to initialize the base `TypedSocket` functionality and logs an initialization message.
*   It does not require a repository, as `StreamEvent`s are transient and typically not persisted as individual historical records in the same way `ConversationMessage`s or `Observation`s are (though their *content* or effects might be captured in those other records).

## Key Methods

1.  **`subscribe(callback: (data: StreamEvent) => void, filter?: StreamEventTypeFilter, options?: { threadId?: string; sessionId?: string }): UnsubscribeFunction`**
    *   Inherited from `TypedSocket`, but with `FilterType` specialized for `StreamEvent['type']`.
    *   Registers a `callback` function to be invoked when a new `StreamEvent` is notified that matches the optional `filter` and `options`.
    *   **`filter?: StreamEvent['type'] | Array<StreamEvent['type']>`:**
        *   If provided, the callback will only be triggered if the `type` of the notified `StreamEvent` (e.g., `'TOKEN'`, `'METADATA'`) matches the specified type or is one of the types in the array.
        *   If `undefined`, the callback receives all `StreamEvent` types (subject to `options` filtering).
    *   **`options?: { threadId?: string; sessionId?: string }`:**
        *   `threadId`: If provided, the callback is only triggered for events belonging to this `threadId`.
        *   `sessionId`: If provided, the callback is only triggered for events associated with this specific UI session.
        *   The `notifyStreamEvent` method uses `targetThreadId` and `targetSessionId` from the `StreamEvent` itself for this comparison.
    *   Returns an `UnsubscribeFunction` to remove the subscription.

2.  **`notifyStreamEvent(event: StreamEvent): void`**
    *   This is a convenience method specific to `LLMStreamSocket` (internally calls `super.notify()`).
    *   **Purpose:** To broadcast a new `StreamEvent` to all relevant subscribers.
    *   **Process:**
        1.  Logs the notification attempt, including event type, threadId, and traceId.
        2.  Calls `super.notify(event, { targetThreadId: event.threadId, targetSessionId: event.sessionId }, filterCheckFn)`.
            *   `targetThreadId: event.threadId` and `targetSessionId: event.sessionId` ensure that only subscribers interested in this specific thread/session (or all) are considered.
            *   The `filterCheckFn` is an internal function passed to `super.notify` that implements the logic for matching the event's `type` against the subscriber's `StreamEvent['type']` filter.

3.  **`getHistory?` Method:**
    *   The `LLMStreamSocket` **does not** override the `getHistory` method from `TypedSocket`.
    *   `StreamEvent`s are transient and represent real-time chunks of data from an active LLM call. They are generally not stored historically as individual events by the framework.
    *   If `getHistory` were called on an `LLMStreamSocket` instance, it would execute the base `TypedSocket.getHistory` implementation, which logs a warning ("getHistory is not implemented in the base TypedSocket.") and returns an empty array.

## Usage Scenario

The `PESAgent` (or other agent core logic) is the primary component that calls `llmStreamSocket.notifyStreamEvent()`. As the agent consumes the `AsyncIterable<StreamEvent>` returned by `ReasoningEngine.call()`, it forwards each `StreamEvent` to this socket.

**Frontend/UI Integration (Conceptual):**

A UI component aiming to display an LLM's response as it streams in would:

1.  Obtain an instance of `LLMStreamSocket` (e.g., via `artInstance.uiSystem.getLLMStreamSocket()`).
2.  When an LLM call is initiated for a specific operation (e.g., final response synthesis for `currentThreadId` and `currentSessionId`):
    *   Subscribe to receive `TOKEN` events:
        ```javascript
        // const currentThreadId = 'thread-abc';
        // const currentSessionId = 'session-xyz'; // If using session-specific updates
        // const llmOutputElement = document.getElementById('llm-response-area');
        //
        // const unsubscribe = art.uiSystem.getLLMStreamSocket().subscribe(
        //   (event) => {
        //     if (event.type === 'TOKEN') {
        //       // Append event.data (the text chunk) to the UI
        //       // Optionally use event.tokenType for different styling (e.g., thoughts vs. response)
        //       llmOutputElement.textContent += event.data;
        //     } else if (event.type === 'METADATA') {
        //       console.log('LLM Metadata:', event.data);
        //     } else if (event.type === 'END') {
        //       console.log('LLM Stream ended.');
        //       // Perform any UI cleanup for the end of the stream
        //     } else if (event.type === 'ERROR') {
        //       console.error('LLM Stream error:', event.data);
        //       llmOutputElement.innerHTML += `<p class="error">Stream Error!</p>`;
        //     }
        //   },
        //   ['TOKEN', 'METADATA', 'ERROR', 'END'], // Subscribe to all relevant types
        //   { threadId: currentThreadId, sessionId: currentSessionId }
        // );
        //
        // // Later, when the UI component is destroyed or the stream is no longer needed:
        // // unsubscribe();
        ```

The `LLMStreamSocket` is essential for creating responsive user experiences by enabling UIs to display LLM-generated content token by token, providing immediate feedback to the user.